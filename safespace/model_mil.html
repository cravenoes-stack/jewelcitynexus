<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Reference Document</title>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body { font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; line-height: 1.6; color: #333; background: #f8f9fa; }
        .container { max-width: 1200px; margin: 0 auto; padding: 20px; }
        .header { background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 40px; border-radius: 10px; margin-bottom: 30px; text-align: center; }
        .header h1 { font-size: 2.5em; margin-bottom: 10px; }
        .meta { opacity: 0.9; font-size: 1.1em; }
        .toc { background: white; padding: 30px; border-radius: 10px; margin-bottom: 30px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }
        .toc h2 { color: #2c3e50; margin-bottom: 20px; }
        .toc ul { list-style: none; }
        .toc li { margin: 10px 0; padding: 10px; background: #f8f9fa; border-left: 4px solid #3498db; }
        .toc a { text-decoration: none; color: #2c3e50; font-weight: 500; }
        .toc a:hover { color: #3498db; }
        .file-section { background: white; margin-bottom: 30px; border-radius: 10px; overflow: hidden; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }
        .file-header { background: linear-gradient(135deg, #3498db, #2980b9); color: white; padding: 20px; }
        .file-header h2 { font-size: 1.5em; }
        .file-meta { opacity: 0.9; margin-top: 5px; }
        .file-content { padding: 30px; }
        .file-content pre { background: #f8f9fa; padding: 20px; border-radius: 5px; overflow-x: auto; border-left: 4px solid #3498db; white-space: pre-wrap; word-wrap: break-word; }
        .file-content code { font-family: 'Courier New', monospace; font-size: 14px; }
        .back-to-top { position: fixed; bottom: 20px; right: 20px; background: #3498db; color: white; border: none; padding: 15px; border-radius: 50%; cursor: pointer; box-shadow: 0 2px 10px rgba(0,0,0,0.2); }
        .back-to-top:hover { background: #2980b9; }
        @media (max-width: 768px) { .container { padding: 10px; } .header { padding: 20px; } .file-content { padding: 15px; } }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>AI Reference Document</h1>
            <div class="meta">Generated on 11/7/2025 â€¢ 27 files</div>
        </div>
        
        
                        <div class="toc">
                            <h2>ðŸ“‹ Table of Contents</h2>
                            <ul><li><a href="#file-0">model.mil</a></li><li><a href="#file-1">model.mil 27</a></li><li><a href="#file-2">model.mil 16</a></li><li><a href="#file-3">model.mil 25</a></li><li><a href="#file-4">model.mil 10</a></li><li><a href="#file-5">model.mil 20</a></li><li><a href="#file-6">model.mil 4</a></li><li><a href="#file-7">model.mil 9</a></li><li><a href="#file-8">model.mil 21</a></li><li><a href="#file-9">model.mil 26</a></li><li><a href="#file-10">model.mil 12</a></li><li><a href="#file-11">model.mil 14</a></li><li><a href="#file-12">model.mil 13</a></li><li><a href="#file-13">model.mil 19</a></li><li><a href="#file-14">model.mil 17</a></li><li><a href="#file-15">model.mil 24</a></li><li><a href="#file-16">model.mil 23</a></li><li><a href="#file-17">model.mil 15</a></li><li><a href="#file-18">model.mil 11</a></li><li><a href="#file-19">model.mil 6</a></li><li><a href="#file-20">model.mil 22</a></li><li><a href="#file-21">model.mil 18</a></li><li><a href="#file-22">model.mil 8</a></li><li><a href="#file-23">model.mil 5</a></li><li><a href="#file-24">model.mil 2</a></li><li><a href="#file-25">model.mil 3</a></li><li><a href="#file-26">model.mil 7</a></li></ul>
                        </div>
                    
        
        
                <div class="file-section" id="file-0">
                    <div class="file-header">
                        <h2>ðŸ“„ model.mil</h2>
                        <div class="file-meta">Size: 4.27 KB</div>
                    </div>
                    <div class="file-content">
                        <pre><code>program(1.0)
[buildInfo = dict&lt;tensor&lt;string, []&gt;, tensor&lt;string, []&gt;&gt;({{"coremlc-component-MIL", "3400.8.1"}, {"coremlc-version", "3400.7.1"}})]
{
    func main&lt;ios16&gt;(tensor&lt;int32, [?, 100]&gt; input) [FlexibleShapeInformation = tuple&lt;tuple&lt;tensor&lt;string, []&gt;, dict&lt;tensor&lt;string, []&gt;, tensor&lt;int32, [?]&gt;&gt;&gt;, tuple&lt;tensor&lt;string, []&gt;, dict&lt;tensor&lt;string, []&gt;, list&lt;tensor&lt;int32, [2]&gt;, ?&gt;&gt;&gt;&gt;((("DefaultShapes", {{"input", [1, 100]}}), ("RangeDims", {{"input", [[1, -1], [100, 100]]}})))] {
            tensor&lt;int32, []&gt; model_embedding_embedding_lookup_axis_0 = const()[name = tensor&lt;string, []&gt;("model_embedding_embedding_lookup_axis_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, []&gt; model_embedding_embedding_lookup_batch_dims_0 = const()[name = tensor&lt;string, []&gt;("model_embedding_embedding_lookup_batch_dims_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;fp16, [29849, 64]&gt; model_embedding_embedding_lookup_69_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_embedding_embedding_lookup_69_to_fp16"), val = tensor&lt;fp16, [29849, 64]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(64)))];
            tensor&lt;fp16, [?, 100, 64]&gt; model_embedding_embedding_lookup_cast = gather(axis = model_embedding_embedding_lookup_axis_0, batch_dims = model_embedding_embedding_lookup_batch_dims_0, indices = input, x = model_embedding_embedding_lookup_69_to_fp16);
            tensor&lt;int32, [1]&gt; model_tf_math_reduce_sum_Sum_axes_0 = const()[name = tensor&lt;string, []&gt;("model_tf_math_reduce_sum_Sum_axes_0"), val = tensor&lt;int32, [1]&gt;([1])];
            tensor&lt;bool, []&gt; model_tf_math_reduce_sum_Sum_keep_dims_0 = const()[name = tensor&lt;string, []&gt;("model_tf_math_reduce_sum_Sum_keep_dims_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [?, 64]&gt; model_tf_math_reduce_sum_Sum_cast = reduce_sum(axes = model_tf_math_reduce_sum_Sum_axes_0, keep_dims = model_tf_math_reduce_sum_Sum_keep_dims_0, x = model_embedding_embedding_lookup_cast);
            tensor&lt;fp16, [?, 64]&gt; model_tf_math_l2_normalize_l2_normalize_Square_cast = mul(x = model_tf_math_reduce_sum_Sum_cast, y = model_tf_math_reduce_sum_Sum_cast);
            tensor&lt;int32, [1]&gt; model_tf_math_l2_normalize_l2_normalize_Sum_axes_0 = const()[name = tensor&lt;string, []&gt;("model_tf_math_l2_normalize_l2_normalize_Sum_axes_0"), val = tensor&lt;int32, [1]&gt;([1])];
            tensor&lt;bool, []&gt; model_tf_math_l2_normalize_l2_normalize_Sum_keep_dims_0 = const()[name = tensor&lt;string, []&gt;("model_tf_math_l2_normalize_l2_normalize_Sum_keep_dims_0"), val = tensor&lt;bool, []&gt;(true)];
            tensor&lt;fp16, [?, 1]&gt; model_tf_math_l2_normalize_l2_normalize_Sum_cast = reduce_sum(axes = model_tf_math_l2_normalize_l2_normalize_Sum_axes_0, keep_dims = model_tf_math_l2_normalize_l2_normalize_Sum_keep_dims_0, x = model_tf_math_l2_normalize_l2_normalize_Square_cast);
            tensor&lt;fp16, []&gt; model_tf_math_l2_normalize_l2_normalize_Maximum_y_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_tf_math_l2_normalize_l2_normalize_Maximum_y_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1p-24)];
            tensor&lt;fp16, [?, 1]&gt; model_tf_math_l2_normalize_l2_normalize_Maximum_cast = maximum(x = model_tf_math_l2_normalize_l2_normalize_Sum_cast, y = model_tf_math_l2_normalize_l2_normalize_Maximum_y_to_fp16);
            tensor&lt;fp16, []&gt; model_tf_math_l2_normalize_l2_normalize_Rsqrt_epsilon_0_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_tf_math_l2_normalize_l2_normalize_Rsqrt_epsilon_0_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1p-24)];
            tensor&lt;fp16, [?, 1]&gt; model_tf_math_l2_normalize_l2_normalize_Rsqrt_cast = rsqrt(epsilon = model_tf_math_l2_normalize_l2_normalize_Rsqrt_epsilon_0_to_fp16, x = model_tf_math_l2_normalize_l2_normalize_Maximum_cast);
            tensor&lt;fp16, [?, 64]&gt; model_tf_math_l2_normalize_l2_normalize_cast = mul(x = model_tf_math_reduce_sum_Sum_cast, y = model_tf_math_l2_normalize_l2_normalize_Rsqrt_cast);
            tensor&lt;string, []&gt; model_tf_math_l2_normalize_l2_normalize_cast_to_fp32_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_tf_math_l2_normalize_l2_normalize_cast_to_fp32_dtype_0"), val = tensor&lt;string, []&gt;("fp32")];
            tensor&lt;fp32, [?, 64]&gt; Identity = cast(dtype = model_tf_math_l2_normalize_l2_normalize_cast_to_fp32_dtype_0, x = model_tf_math_l2_normalize_l2_normalize_cast);
        } -&gt; (Identity);
}</code></pre>
                    </div>
                </div>
            
                <div class="file-section" id="file-1">
                    <div class="file-header">
                        <h2>ðŸ“„ model.mil 27</h2>
                        <div class="file-meta">Size: 21.78 KB</div>
                    </div>
                    <div class="file-content">
                        <pre><code>program(1.0)
[buildInfo = dict&lt;tensor&lt;string, []&gt;, tensor&lt;string, []&gt;&gt;({{"coremlc-component-MIL", "3405.2.1"}, {"coremlc-version", "3405.2.1"}, {"coremltools-component-tensorflow", "2.13.0"}, {"coremltools-version", "7.0"}, {"mldb_token", "mldb-7mar575y6j"}})]
{
    func main&lt;ios15&gt;(tensor&lt;fp32, [1, 3074]&gt; features) {
            tensor&lt;int32, [4]&gt; model_tf_split_Const = const()[name = tensor&lt;string, []&gt;("model_tf_split_Const"), val = tensor&lt;int32, [4]&gt;([1536, 1536, 1, 1])];
            tensor&lt;int32, []&gt; model_tf_split_split_split_dim = const()[name = tensor&lt;string, []&gt;("model_tf_split_split_split_dim"), val = tensor&lt;int32, []&gt;(1)];
            tensor&lt;fp32, []&gt; model_tf_math_floormod_FloorMod_y = const()[name = tensor&lt;string, []&gt;("model_tf_math_floormod_FloorMod_y"), val = tensor&lt;fp32, []&gt;(0x1.388p+12)];
            tensor&lt;fp32, []&gt; model_tf_math_floormod_1_FloorMod_y = const()[name = tensor&lt;string, []&gt;("model_tf_math_floormod_1_FloorMod_y"), val = tensor&lt;fp32, []&gt;(0x1.d4cp+14)];
            tensor&lt;int32, []&gt; model_concatenate_concat_axis = const()[name = tensor&lt;string, []&gt;("model_concatenate_concat_axis"), val = tensor&lt;int32, []&gt;(1)];
            tensor&lt;int32, [2]&gt; model_flatten_1_Const = const()[name = tensor&lt;string, []&gt;("model_flatten_1_Const"), val = tensor&lt;int32, [2]&gt;([-1, 256])];
            tensor&lt;int32, []&gt; model_concatenate_1_concat_axis = const()[name = tensor&lt;string, []&gt;("model_concatenate_1_concat_axis"), val = tensor&lt;int32, []&gt;(1)];
            tensor&lt;fp32, [1536]&gt; model_batch_normalization_batchnorm_mul = const()[name = tensor&lt;string, []&gt;("model_batch_normalization_batchnorm_mul"), val = tensor&lt;fp32, [1536]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(64)))];
            tensor&lt;fp32, [1536]&gt; model_batch_normalization_batchnorm_sub = const()[name = tensor&lt;string, []&gt;("model_batch_normalization_batchnorm_sub"), val = tensor&lt;fp32, [1536]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(6272)))];
            tensor&lt;fp32, [128]&gt; model_batch_normalization_1_batchnorm_mul = const()[name = tensor&lt;string, []&gt;("model_batch_normalization_1_batchnorm_mul"), val = tensor&lt;fp32, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(12480)))];
            tensor&lt;fp32, [128]&gt; model_batch_normalization_1_batchnorm_sub = const()[name = tensor&lt;string, []&gt;("model_batch_normalization_1_batchnorm_sub"), val = tensor&lt;fp32, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(13056)))];
            tensor&lt;fp32, [128]&gt; model_batch_normalization_2_batchnorm_mul = const()[name = tensor&lt;string, []&gt;("model_batch_normalization_2_batchnorm_mul"), val = tensor&lt;fp32, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(13632)))];
            tensor&lt;fp32, [128]&gt; model_batch_normalization_2_batchnorm_sub = const()[name = tensor&lt;string, []&gt;("model_batch_normalization_2_batchnorm_sub"), val = tensor&lt;fp32, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(14208)))];
            tensor&lt;int32, []&gt; model_tf_split_split_num_splits_0 = const()[name = tensor&lt;string, []&gt;("model_tf_split_split_num_splits_0"), val = tensor&lt;int32, []&gt;(4)];
            tensor&lt;fp32, [1, 1536]&gt; model_tf_split_split_0, tensor&lt;fp32, [1, 1536]&gt; model_tf_split_split_1, tensor&lt;fp32, [1, 1]&gt; model_tf_split_split_2, tensor&lt;fp32, [1, 1]&gt; model_tf_split_split_3 = split(axis = model_tf_split_split_split_dim, num_splits = model_tf_split_split_num_splits_0, split_sizes = model_tf_split_Const, x = features)[name = tensor&lt;string, []&gt;("model_tf_split_split")];
            tensor&lt;fp32, [1, 1536]&gt; model_batch_normalization_batchnorm_mul_1 = mul(x = model_tf_split_split_0, y = model_batch_normalization_batchnorm_mul)[name = tensor&lt;string, []&gt;("model_batch_normalization_batchnorm_mul_1")];
            tensor&lt;string, []&gt; model_embedding_2_Cast_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_embedding_2_Cast_dtype_0"), val = tensor&lt;string, []&gt;("int32")];
            tensor&lt;fp32, [1, 1]&gt; model_tf_math_floormod_FloorMod_floor_div = floor_div(x = model_tf_split_split_3, y = model_tf_math_floormod_FloorMod_y)[name = tensor&lt;string, []&gt;("model_tf_math_floormod_FloorMod_floor_div")];
            tensor&lt;fp32, [1, 1]&gt; model_tf_math_floormod_FloorMod_multiply = mul(x = model_tf_math_floormod_FloorMod_floor_div, y = model_tf_math_floormod_FloorMod_y)[name = tensor&lt;string, []&gt;("model_tf_math_floormod_FloorMod_multiply")];
            tensor&lt;fp32, [1, 1]&gt; model_tf_math_floormod_FloorMod = sub(x = model_tf_split_split_3, y = model_tf_math_floormod_FloorMod_multiply)[name = tensor&lt;string, []&gt;("model_tf_math_floormod_FloorMod")];
            tensor&lt;fp32, [1, 1]&gt; model_tf_math_floormod_1_FloorMod_floor_div = floor_div(x = model_tf_split_split_3, y = model_tf_math_floormod_1_FloorMod_y)[name = tensor&lt;string, []&gt;("model_tf_math_floormod_1_FloorMod_floor_div")];
            tensor&lt;fp32, [1, 1]&gt; model_tf_math_floormod_1_FloorMod_multiply = mul(x = model_tf_math_floormod_1_FloorMod_floor_div, y = model_tf_math_floormod_1_FloorMod_y)[name = tensor&lt;string, []&gt;("model_tf_math_floormod_1_FloorMod_multiply")];
            tensor&lt;fp32, [1, 1]&gt; model_tf_math_floormod_1_FloorMod = sub(x = model_tf_split_split_3, y = model_tf_math_floormod_1_FloorMod_multiply)[name = tensor&lt;string, []&gt;("model_tf_math_floormod_1_FloorMod")];
            tensor&lt;fp32, [1, 1536]&gt; model_batch_normalization_batchnorm_add_1 = add(x = model_batch_normalization_batchnorm_mul_1, y = model_batch_normalization_batchnorm_sub)[name = tensor&lt;string, []&gt;("model_batch_normalization_batchnorm_add_1")];
            tensor&lt;string, []&gt; model_tf_split_split_1_to_fp16_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_tf_split_split_1_to_fp16_dtype_0"), val = tensor&lt;string, []&gt;("fp16")];
            tensor&lt;fp16, [128, 1536]&gt; transpose_0_to_fp16 = const()[name = tensor&lt;string, []&gt;("transpose_0_to_fp16"), val = tensor&lt;fp16, [128, 1536]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(14784)))];
            tensor&lt;fp16, [128]&gt; model_dense_2_BiasAdd_bias_0_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_dense_2_BiasAdd_bias_0_to_fp16"), val = tensor&lt;fp16, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(408064)))];
            tensor&lt;fp16, [1, 1536]&gt; cast_16 = cast(dtype = model_tf_split_split_1_to_fp16_dtype_0, x = model_tf_split_split_1)[name = tensor&lt;string, []&gt;("cast_16")];
            tensor&lt;fp16, [1, 128]&gt; model_dense_2_BiasAdd_cast = linear(bias = model_dense_2_BiasAdd_bias_0_to_fp16, weight = transpose_0_to_fp16, x = cast_16)[name = tensor&lt;string, []&gt;("model_dense_2_BiasAdd_cast")];
            tensor&lt;string, []&gt; model_dense_2_BiasAdd_cast_to_fp32_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_dense_2_BiasAdd_cast_to_fp32_dtype_0"), val = tensor&lt;string, []&gt;("fp32")];
            tensor&lt;int32, []&gt; model_embedding_2_embedding_lookup_axis_0 = const()[name = tensor&lt;string, []&gt;("model_embedding_2_embedding_lookup_axis_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;fp16, [622, 128]&gt; model_embedding_2_embedding_lookup_505_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_embedding_2_embedding_lookup_505_to_fp16"), val = tensor&lt;fp16, [622, 128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(408384)))];
            tensor&lt;int32, [1, 1]&gt; cast_17 = cast(dtype = model_embedding_2_Cast_dtype_0, x = model_tf_split_split_2)[name = tensor&lt;string, []&gt;("cast_17")];
            tensor&lt;fp16, [1, 1, 128]&gt; model_embedding_2_embedding_lookup_cast = gather(axis = model_embedding_2_embedding_lookup_axis_0, indices = cast_17, x = model_embedding_2_embedding_lookup_505_to_fp16)[name = tensor&lt;string, []&gt;("model_embedding_2_embedding_lookup_cast")];
            tensor&lt;string, []&gt; model_embedding_2_embedding_lookup_cast_to_fp32_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_embedding_2_embedding_lookup_cast_to_fp32_dtype_0"), val = tensor&lt;string, []&gt;("fp32")];
            tensor&lt;string, []&gt; model_embedding_Cast_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_embedding_Cast_dtype_0"), val = tensor&lt;string, []&gt;("int32")];
            tensor&lt;string, []&gt; model_embedding_1_Cast_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_embedding_1_Cast_dtype_0"), val = tensor&lt;string, []&gt;("int32")];
            tensor&lt;int32, []&gt; model_embedding_embedding_lookup_axis_0 = const()[name = tensor&lt;string, []&gt;("model_embedding_embedding_lookup_axis_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;fp16, [5000, 128]&gt; model_embedding_embedding_lookup_495_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_embedding_embedding_lookup_495_to_fp16"), val = tensor&lt;fp16, [5000, 128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(567680)))];
            tensor&lt;int32, [1, 1]&gt; cast_13 = cast(dtype = model_embedding_Cast_dtype_0, x = model_tf_math_floormod_FloorMod)[name = tensor&lt;string, []&gt;("cast_13")];
            tensor&lt;fp16, [1, 1, 128]&gt; model_embedding_embedding_lookup_cast = gather(axis = model_embedding_embedding_lookup_axis_0, indices = cast_13, x = model_embedding_embedding_lookup_495_to_fp16)[name = tensor&lt;string, []&gt;("model_embedding_embedding_lookup_cast")];
            tensor&lt;string, []&gt; model_embedding_embedding_lookup_cast_to_fp32_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_embedding_embedding_lookup_cast_to_fp32_dtype_0"), val = tensor&lt;string, []&gt;("fp32")];
            tensor&lt;int32, []&gt; model_embedding_1_embedding_lookup_axis_0 = const()[name = tensor&lt;string, []&gt;("model_embedding_1_embedding_lookup_axis_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;fp16, [30000, 1]&gt; model_embedding_1_embedding_lookup_500_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_embedding_1_embedding_lookup_500_to_fp16"), val = tensor&lt;fp16, [30000, 1]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(1847744)))];
            tensor&lt;int32, [1, 1]&gt; cast_12 = cast(dtype = model_embedding_1_Cast_dtype_0, x = model_tf_math_floormod_1_FloorMod)[name = tensor&lt;string, []&gt;("cast_12")];
            tensor&lt;fp16, [1, 1, 1]&gt; model_embedding_1_embedding_lookup_cast = gather(axis = model_embedding_1_embedding_lookup_axis_0, indices = cast_12, x = model_embedding_1_embedding_lookup_500_to_fp16)[name = tensor&lt;string, []&gt;("model_embedding_1_embedding_lookup_cast")];
            tensor&lt;string, []&gt; model_embedding_1_embedding_lookup_cast_to_fp32_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_embedding_1_embedding_lookup_cast_to_fp32_dtype_0"), val = tensor&lt;string, []&gt;("fp32")];
            tensor&lt;string, []&gt; model_batch_normalization_batchnorm_add_1_to_fp16_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_batch_normalization_batchnorm_add_1_to_fp16_dtype_0"), val = tensor&lt;string, []&gt;("fp16")];
            tensor&lt;fp16, [128, 1536]&gt; transpose_1_to_fp16 = const()[name = tensor&lt;string, []&gt;("transpose_1_to_fp16"), val = tensor&lt;fp16, [128, 1536]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(1907840)))];
            tensor&lt;fp16, [128]&gt; model_dense_BiasAdd_bias_0_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_dense_BiasAdd_bias_0_to_fp16"), val = tensor&lt;fp16, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(2301120)))];
            tensor&lt;fp16, [1, 1536]&gt; cast_9 = cast(dtype = model_batch_normalization_batchnorm_add_1_to_fp16_dtype_0, x = model_batch_normalization_batchnorm_add_1)[name = tensor&lt;string, []&gt;("cast_9")];
            tensor&lt;fp16, [1, 128]&gt; model_dense_BiasAdd_cast = linear(bias = model_dense_BiasAdd_bias_0_to_fp16, weight = transpose_1_to_fp16, x = cast_9)[name = tensor&lt;string, []&gt;("model_dense_BiasAdd_cast")];
            tensor&lt;string, []&gt; model_dense_BiasAdd_cast_to_fp32_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_dense_BiasAdd_cast_to_fp32_dtype_0"), val = tensor&lt;string, []&gt;("fp32")];
            tensor&lt;fp32, [1, 128]&gt; cast_15 = cast(dtype = model_dense_2_BiasAdd_cast_to_fp32_dtype_0, x = model_dense_2_BiasAdd_cast)[name = tensor&lt;string, []&gt;("cast_15")];
            tensor&lt;fp32, [1, 128]&gt; model_dense_2_Relu = relu(x = cast_15)[name = tensor&lt;string, []&gt;("model_dense_2_Relu")];
            tensor&lt;fp32, [1, 1, 1]&gt; cast_10 = cast(dtype = model_embedding_1_embedding_lookup_cast_to_fp32_dtype_0, x = model_embedding_1_embedding_lookup_cast)[name = tensor&lt;string, []&gt;("cast_10")];
            tensor&lt;fp32, [1, 1, 128]&gt; cast_11 = cast(dtype = model_embedding_embedding_lookup_cast_to_fp32_dtype_0, x = model_embedding_embedding_lookup_cast)[name = tensor&lt;string, []&gt;("cast_11")];
            tensor&lt;fp32, [1, 1, 128]&gt; model_multiply_mul = mul(x = cast_11, y = cast_10)[name = tensor&lt;string, []&gt;("model_multiply_mul")];
            tensor&lt;bool, []&gt; model_concatenate_concat_interleave_0 = const()[name = tensor&lt;string, []&gt;("model_concatenate_concat_interleave_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp32, [1, 1, 128]&gt; cast_14 = cast(dtype = model_embedding_2_embedding_lookup_cast_to_fp32_dtype_0, x = model_embedding_2_embedding_lookup_cast)[name = tensor&lt;string, []&gt;("cast_14")];
            tensor&lt;fp32, [1, 2, 128]&gt; model_concatenate_concat = concat(axis = model_concatenate_concat_axis, interleave = model_concatenate_concat_interleave_0, values = (cast_14, model_multiply_mul))[name = tensor&lt;string, []&gt;("model_concatenate_concat")];
            tensor&lt;fp32, [1, 128]&gt; cast_8 = cast(dtype = model_dense_BiasAdd_cast_to_fp32_dtype_0, x = model_dense_BiasAdd_cast)[name = tensor&lt;string, []&gt;("cast_8")];
            tensor&lt;fp32, [1, 128]&gt; model_dense_Relu = relu(x = cast_8)[name = tensor&lt;string, []&gt;("model_dense_Relu")];
            tensor&lt;fp32, [1, 256]&gt; model_flatten_1_Reshape = reshape(shape = model_flatten_1_Const, x = model_concatenate_concat)[name = tensor&lt;string, []&gt;("model_flatten_1_Reshape")];
            tensor&lt;string, []&gt; model_flatten_1_Reshape_to_fp16_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_flatten_1_Reshape_to_fp16_dtype_0"), val = tensor&lt;string, []&gt;("fp16")];
            tensor&lt;fp16, [128, 256]&gt; transpose_2_to_fp16 = const()[name = tensor&lt;string, []&gt;("transpose_2_to_fp16"), val = tensor&lt;fp16, [128, 256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(2301440)))];
            tensor&lt;fp16, [128]&gt; model_dense_1_BiasAdd_bias_0_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_dense_1_BiasAdd_bias_0_to_fp16"), val = tensor&lt;fp16, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(2367040)))];
            tensor&lt;fp16, [1, 256]&gt; cast_7 = cast(dtype = model_flatten_1_Reshape_to_fp16_dtype_0, x = model_flatten_1_Reshape)[name = tensor&lt;string, []&gt;("cast_7")];
            tensor&lt;fp16, [1, 128]&gt; model_dense_1_BiasAdd_cast = linear(bias = model_dense_1_BiasAdd_bias_0_to_fp16, weight = transpose_2_to_fp16, x = cast_7)[name = tensor&lt;string, []&gt;("model_dense_1_BiasAdd_cast")];
            tensor&lt;string, []&gt; model_dense_1_BiasAdd_cast_to_fp32_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_dense_1_BiasAdd_cast_to_fp32_dtype_0"), val = tensor&lt;string, []&gt;("fp32")];
            tensor&lt;fp32, [1, 128]&gt; cast_6 = cast(dtype = model_dense_1_BiasAdd_cast_to_fp32_dtype_0, x = model_dense_1_BiasAdd_cast)[name = tensor&lt;string, []&gt;("cast_6")];
            tensor&lt;fp32, [1, 128]&gt; model_dense_1_Relu = relu(x = cast_6)[name = tensor&lt;string, []&gt;("model_dense_1_Relu")];
            tensor&lt;bool, []&gt; model_concatenate_1_concat_interleave_0 = const()[name = tensor&lt;string, []&gt;("model_concatenate_1_concat_interleave_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp32, [1, 384]&gt; model_concatenate_1_concat = concat(axis = model_concatenate_1_concat_axis, interleave = model_concatenate_1_concat_interleave_0, values = (model_dense_Relu, model_dense_1_Relu, model_dense_2_Relu))[name = tensor&lt;string, []&gt;("model_concatenate_1_concat")];
            tensor&lt;string, []&gt; model_concatenate_1_concat_to_fp16_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_concatenate_1_concat_to_fp16_dtype_0"), val = tensor&lt;string, []&gt;("fp16")];
            tensor&lt;fp16, [128, 384]&gt; transpose_3_to_fp16 = const()[name = tensor&lt;string, []&gt;("transpose_3_to_fp16"), val = tensor&lt;fp16, [128, 384]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(2367360)))];
            tensor&lt;fp16, [128]&gt; model_dense_3_BiasAdd_bias_0_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_dense_3_BiasAdd_bias_0_to_fp16"), val = tensor&lt;fp16, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(2465728)))];
            tensor&lt;fp16, [1, 384]&gt; cast_5 = cast(dtype = model_concatenate_1_concat_to_fp16_dtype_0, x = model_concatenate_1_concat)[name = tensor&lt;string, []&gt;("cast_5")];
            tensor&lt;fp16, [1, 128]&gt; model_dense_3_BiasAdd_cast = linear(bias = model_dense_3_BiasAdd_bias_0_to_fp16, weight = transpose_3_to_fp16, x = cast_5)[name = tensor&lt;string, []&gt;("model_dense_3_BiasAdd_cast")];
            tensor&lt;string, []&gt; model_dense_3_BiasAdd_cast_to_fp32_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_dense_3_BiasAdd_cast_to_fp32_dtype_0"), val = tensor&lt;string, []&gt;("fp32")];
            tensor&lt;fp32, [1, 128]&gt; cast_4 = cast(dtype = model_dense_3_BiasAdd_cast_to_fp32_dtype_0, x = model_dense_3_BiasAdd_cast)[name = tensor&lt;string, []&gt;("cast_4")];
            tensor&lt;fp32, [1, 128]&gt; model_dense_3_Relu = relu(x = cast_4)[name = tensor&lt;string, []&gt;("model_dense_3_Relu")];
            tensor&lt;fp32, [1, 128]&gt; model_batch_normalization_1_batchnorm_mul_1 = mul(x = model_dense_3_Relu, y = model_batch_normalization_1_batchnorm_mul)[name = tensor&lt;string, []&gt;("model_batch_normalization_1_batchnorm_mul_1")];
            tensor&lt;fp32, [1, 128]&gt; model_batch_normalization_1_batchnorm_add_1 = add(x = model_batch_normalization_1_batchnorm_mul_1, y = model_batch_normalization_1_batchnorm_sub)[name = tensor&lt;string, []&gt;("model_batch_normalization_1_batchnorm_add_1")];
            tensor&lt;string, []&gt; model_batch_normalization_1_batchnorm_add_1_to_fp16_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_batch_normalization_1_batchnorm_add_1_to_fp16_dtype_0"), val = tensor&lt;string, []&gt;("fp16")];
            tensor&lt;fp16, [128, 128]&gt; transpose_4_to_fp16 = const()[name = tensor&lt;string, []&gt;("transpose_4_to_fp16"), val = tensor&lt;fp16, [128, 128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(2466048)))];
            tensor&lt;fp16, [128]&gt; model_dense_4_BiasAdd_bias_0_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_dense_4_BiasAdd_bias_0_to_fp16"), val = tensor&lt;fp16, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(2498880)))];
            tensor&lt;fp16, [1, 128]&gt; cast_3 = cast(dtype = model_batch_normalization_1_batchnorm_add_1_to_fp16_dtype_0, x = model_batch_normalization_1_batchnorm_add_1)[name = tensor&lt;string, []&gt;("cast_3")];
            tensor&lt;fp16, [1, 128]&gt; model_dense_4_BiasAdd_cast = linear(bias = model_dense_4_BiasAdd_bias_0_to_fp16, weight = transpose_4_to_fp16, x = cast_3)[name = tensor&lt;string, []&gt;("model_dense_4_BiasAdd_cast")];
            tensor&lt;string, []&gt; model_dense_4_BiasAdd_cast_to_fp32_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_dense_4_BiasAdd_cast_to_fp32_dtype_0"), val = tensor&lt;string, []&gt;("fp32")];
            tensor&lt;fp32, [1, 128]&gt; cast_2 = cast(dtype = model_dense_4_BiasAdd_cast_to_fp32_dtype_0, x = model_dense_4_BiasAdd_cast)[name = tensor&lt;string, []&gt;("cast_2")];
            tensor&lt;fp32, [1, 128]&gt; model_dense_4_Relu = relu(x = cast_2)[name = tensor&lt;string, []&gt;("model_dense_4_Relu")];
            tensor&lt;fp32, [1, 128]&gt; model_batch_normalization_2_batchnorm_mul_1 = mul(x = model_dense_4_Relu, y = model_batch_normalization_2_batchnorm_mul)[name = tensor&lt;string, []&gt;("model_batch_normalization_2_batchnorm_mul_1")];
            tensor&lt;fp32, [1, 128]&gt; model_batch_normalization_2_batchnorm_add_1 = add(x = model_batch_normalization_2_batchnorm_mul_1, y = model_batch_normalization_2_batchnorm_sub)[name = tensor&lt;string, []&gt;("model_batch_normalization_2_batchnorm_add_1")];
            tensor&lt;string, []&gt; model_batch_normalization_2_batchnorm_add_1_to_fp16_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_batch_normalization_2_batchnorm_add_1_to_fp16_dtype_0"), val = tensor&lt;string, []&gt;("fp16")];
            tensor&lt;fp16, [5, 128]&gt; transpose_5_to_fp16 = const()[name = tensor&lt;string, []&gt;("transpose_5_to_fp16"), val = tensor&lt;fp16, [5, 128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(2499200)))];
            tensor&lt;fp16, [5]&gt; model_labels_BiasAdd_bias_0_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_labels_BiasAdd_bias_0_to_fp16"), val = tensor&lt;fp16, [5]&gt;([0x1.1fcp-2, 0x1.4cp-3, -0x1.4c4p-1, -0x1.104p-1, 0x1.344p-3])];
            tensor&lt;fp16, [1, 128]&gt; cast_1 = cast(dtype = model_batch_normalization_2_batchnorm_add_1_to_fp16_dtype_0, x = model_batch_normalization_2_batchnorm_add_1)[name = tensor&lt;string, []&gt;("cast_1")];
            tensor&lt;fp16, [1, 5]&gt; model_labels_BiasAdd_cast = linear(bias = model_labels_BiasAdd_bias_0_to_fp16, weight = transpose_5_to_fp16, x = cast_1)[name = tensor&lt;string, []&gt;("model_labels_BiasAdd_cast")];
            tensor&lt;string, []&gt; model_labels_BiasAdd_cast_to_fp32_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_labels_BiasAdd_cast_to_fp32_dtype_0"), val = tensor&lt;string, []&gt;("fp32")];
            tensor&lt;int32, []&gt; model_labels_Softmax_axis_0 = const()[name = tensor&lt;string, []&gt;("model_labels_Softmax_axis_0"), val = tensor&lt;int32, []&gt;(-1)];
            tensor&lt;fp32, [1, 5]&gt; cast_0 = cast(dtype = model_labels_BiasAdd_cast_to_fp32_dtype_0, x = model_labels_BiasAdd_cast)[name = tensor&lt;string, []&gt;("cast_0")];
            tensor&lt;fp32, [1, 5]&gt; Identity = softmax(axis = model_labels_Softmax_axis_0, x = cast_0)[name = tensor&lt;string, []&gt;("model_labels_Softmax")];
        } -&gt; (Identity);
}</code></pre>
                    </div>
                </div>
            
                <div class="file-section" id="file-2">
                    <div class="file-header">
                        <h2>ðŸ“„ model.mil 16</h2>
                        <div class="file-meta">Size: 15.67 KB</div>
                    </div>
                    <div class="file-content">
                        <pre><code>program(1.0)
[buildInfo = dict&lt;tensor&lt;string, []&gt;, tensor&lt;string, []&gt;&gt;({{"coremlc-component-MIL", "3405.2.1"}, {"coremlc-version", "3405.2.1"}, {"coremltools-component-tensorflow", "2.13.1"}, {"coremltools-version", "8.1"}, {"mldb_token", "mldb-7mar575y6j"}})]
{
    func main&lt;ios15&gt;(tensor&lt;fp32, [1, 3074]&gt; features) {
            tensor&lt;int32, [4]&gt; model_tf_split_Const = const()[name = tensor&lt;string, []&gt;("model_tf_split_Const"), val = tensor&lt;int32, [4]&gt;([1536, 1536, 1, 1])];
            tensor&lt;int32, []&gt; model_tf_split_split_split_dim = const()[name = tensor&lt;string, []&gt;("model_tf_split_split_split_dim"), val = tensor&lt;int32, []&gt;(1)];
            tensor&lt;int32, [2]&gt; model_flatten_1_Const = const()[name = tensor&lt;string, []&gt;("model_flatten_1_Const"), val = tensor&lt;int32, [2]&gt;([-1, 128])];
            tensor&lt;int32, []&gt; model_concatenate_concat_axis = const()[name = tensor&lt;string, []&gt;("model_concatenate_concat_axis"), val = tensor&lt;int32, []&gt;(1)];
            tensor&lt;fp32, [128]&gt; model_batch_normalization_batchnorm_mul = const()[name = tensor&lt;string, []&gt;("model_batch_normalization_batchnorm_mul"), val = tensor&lt;fp32, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(64)))];
            tensor&lt;fp32, [128]&gt; model_batch_normalization_batchnorm_sub = const()[name = tensor&lt;string, []&gt;("model_batch_normalization_batchnorm_sub"), val = tensor&lt;fp32, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(640)))];
            tensor&lt;fp32, [640]&gt; model_batch_normalization_1_batchnorm_mul = const()[name = tensor&lt;string, []&gt;("model_batch_normalization_1_batchnorm_mul"), val = tensor&lt;fp32, [640]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(1216)))];
            tensor&lt;fp32, [640]&gt; model_batch_normalization_1_batchnorm_sub = const()[name = tensor&lt;string, []&gt;("model_batch_normalization_1_batchnorm_sub"), val = tensor&lt;fp32, [640]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(3840)))];
            tensor&lt;fp32, [512]&gt; model_batch_normalization_2_batchnorm_mul = const()[name = tensor&lt;string, []&gt;("model_batch_normalization_2_batchnorm_mul"), val = tensor&lt;fp32, [512]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(6464)))];
            tensor&lt;fp32, [512]&gt; model_batch_normalization_2_batchnorm_sub = const()[name = tensor&lt;string, []&gt;("model_batch_normalization_2_batchnorm_sub"), val = tensor&lt;fp32, [512]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(8576)))];
            tensor&lt;int32, []&gt; model_tf_split_split_num_splits_0 = const()[name = tensor&lt;string, []&gt;("model_tf_split_split_num_splits_0"), val = tensor&lt;int32, []&gt;(4)];
            tensor&lt;fp32, [1, 1536]&gt; model_tf_split_split_0, tensor&lt;fp32, [1, 1536]&gt; model_tf_split_split_1, tensor&lt;fp32, [1, 1]&gt; model_tf_split_split_2, tensor&lt;fp32, [1, 1]&gt; model_tf_split_split_3 = split(axis = model_tf_split_split_split_dim, num_splits = model_tf_split_split_num_splits_0, split_sizes = model_tf_split_Const, x = features)[name = tensor&lt;string, []&gt;("model_tf_split_split")];
            tensor&lt;string, []&gt; model_embedding_Cast_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_embedding_Cast_dtype_0"), val = tensor&lt;string, []&gt;("int32")];
            tensor&lt;string, []&gt; model_tf_split_split_1_to_fp16_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_tf_split_split_1_to_fp16_dtype_0"), val = tensor&lt;string, []&gt;("fp16")];
            tensor&lt;fp16, [512, 1536]&gt; transpose_0_to_fp16 = const()[name = tensor&lt;string, []&gt;("transpose_0_to_fp16"), val = tensor&lt;fp16, [512, 1536]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(10688)))];
            tensor&lt;fp16, [512]&gt; model_dense_BiasAdd_bias_0_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_dense_BiasAdd_bias_0_to_fp16"), val = tensor&lt;fp16, [512]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(1583616)))];
            tensor&lt;fp16, [1, 1536]&gt; model_tf_split_split_1_to_fp16 = cast(dtype = model_tf_split_split_1_to_fp16_dtype_0, x = model_tf_split_split_1)[name = tensor&lt;string, []&gt;("cast_10")];
            tensor&lt;fp16, [1, 512]&gt; model_dense_BiasAdd_cast_fp16 = linear(bias = model_dense_BiasAdd_bias_0_to_fp16, weight = transpose_0_to_fp16, x = model_tf_split_split_1_to_fp16)[name = tensor&lt;string, []&gt;("model_dense_BiasAdd_cast_fp16")];
            tensor&lt;string, []&gt; model_dense_BiasAdd_cast_fp16_to_fp32_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_dense_BiasAdd_cast_fp16_to_fp32_dtype_0"), val = tensor&lt;string, []&gt;("fp32")];
            tensor&lt;int32, []&gt; model_embedding_embedding_lookup_axis_0 = const()[name = tensor&lt;string, []&gt;("model_embedding_embedding_lookup_axis_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;fp16, [622, 128]&gt; model_embedding_embedding_lookup_2182_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_embedding_embedding_lookup_2182_to_fp16"), val = tensor&lt;fp16, [622, 128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(1584704)))];
            tensor&lt;int32, [1, 1]&gt; model_embedding_Cast = cast(dtype = model_embedding_Cast_dtype_0, x = model_tf_split_split_2)[name = tensor&lt;string, []&gt;("cast_11")];
            tensor&lt;fp16, [1, 1, 128]&gt; model_embedding_embedding_lookup_cast_fp16 = gather(axis = model_embedding_embedding_lookup_axis_0, indices = model_embedding_Cast, x = model_embedding_embedding_lookup_2182_to_fp16)[name = tensor&lt;string, []&gt;("model_embedding_embedding_lookup_cast_fp16")];
            tensor&lt;string, []&gt; model_embedding_embedding_lookup_cast_fp16_to_fp32_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_embedding_embedding_lookup_cast_fp16_to_fp32_dtype_0"), val = tensor&lt;string, []&gt;("fp32")];
            tensor&lt;fp32, [1, 1, 128]&gt; model_embedding_embedding_lookup_cast_fp16_to_fp32 = cast(dtype = model_embedding_embedding_lookup_cast_fp16_to_fp32_dtype_0, x = model_embedding_embedding_lookup_cast_fp16)[name = tensor&lt;string, []&gt;("cast_8")];
            tensor&lt;fp32, [1, 128]&gt; model_flatten_1_Reshape = reshape(shape = model_flatten_1_Const, x = model_embedding_embedding_lookup_cast_fp16_to_fp32)[name = tensor&lt;string, []&gt;("model_flatten_1_Reshape")];
            tensor&lt;fp32, [1, 512]&gt; model_dense_BiasAdd_cast_fp16_to_fp32 = cast(dtype = model_dense_BiasAdd_cast_fp16_to_fp32_dtype_0, x = model_dense_BiasAdd_cast_fp16)[name = tensor&lt;string, []&gt;("cast_9")];
            tensor&lt;fp32, [1, 512]&gt; model_dense_Relu = relu(x = model_dense_BiasAdd_cast_fp16_to_fp32)[name = tensor&lt;string, []&gt;("model_dense_Relu")];
            tensor&lt;fp32, [1, 128]&gt; model_batch_normalization_batchnorm_mul_1 = mul(x = model_flatten_1_Reshape, y = model_batch_normalization_batchnorm_mul)[name = tensor&lt;string, []&gt;("model_batch_normalization_batchnorm_mul_1")];
            tensor&lt;fp32, [1, 128]&gt; model_batch_normalization_batchnorm_add_1 = add(x = model_batch_normalization_batchnorm_mul_1, y = model_batch_normalization_batchnorm_sub)[name = tensor&lt;string, []&gt;("model_batch_normalization_batchnorm_add_1")];
            tensor&lt;string, []&gt; model_batch_normalization_batchnorm_add_1_to_fp16_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_batch_normalization_batchnorm_add_1_to_fp16_dtype_0"), val = tensor&lt;string, []&gt;("fp16")];
            tensor&lt;fp16, [128, 128]&gt; transpose_1_to_fp16 = const()[name = tensor&lt;string, []&gt;("transpose_1_to_fp16"), val = tensor&lt;fp16, [128, 128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(1744000)))];
            tensor&lt;fp16, [128]&gt; model_dense_1_BiasAdd_bias_0_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_dense_1_BiasAdd_bias_0_to_fp16"), val = tensor&lt;fp16, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(1776832)))];
            tensor&lt;fp16, [1, 128]&gt; model_batch_normalization_batchnorm_add_1_to_fp16 = cast(dtype = model_batch_normalization_batchnorm_add_1_to_fp16_dtype_0, x = model_batch_normalization_batchnorm_add_1)[name = tensor&lt;string, []&gt;("cast_7")];
            tensor&lt;fp16, [1, 128]&gt; model_dense_1_BiasAdd_cast_fp16 = linear(bias = model_dense_1_BiasAdd_bias_0_to_fp16, weight = transpose_1_to_fp16, x = model_batch_normalization_batchnorm_add_1_to_fp16)[name = tensor&lt;string, []&gt;("model_dense_1_BiasAdd_cast_fp16")];
            tensor&lt;string, []&gt; model_dense_1_BiasAdd_cast_fp16_to_fp32_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_dense_1_BiasAdd_cast_fp16_to_fp32_dtype_0"), val = tensor&lt;string, []&gt;("fp32")];
            tensor&lt;fp32, [1, 128]&gt; model_dense_1_BiasAdd_cast_fp16_to_fp32 = cast(dtype = model_dense_1_BiasAdd_cast_fp16_to_fp32_dtype_0, x = model_dense_1_BiasAdd_cast_fp16)[name = tensor&lt;string, []&gt;("cast_6")];
            tensor&lt;fp32, [1, 128]&gt; model_dense_1_Relu = relu(x = model_dense_1_BiasAdd_cast_fp16_to_fp32)[name = tensor&lt;string, []&gt;("model_dense_1_Relu")];
            tensor&lt;bool, []&gt; model_concatenate_concat_interleave_0 = const()[name = tensor&lt;string, []&gt;("model_concatenate_concat_interleave_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp32, [1, 640]&gt; model_concatenate_concat = concat(axis = model_concatenate_concat_axis, interleave = model_concatenate_concat_interleave_0, values = (model_dense_Relu, model_dense_1_Relu))[name = tensor&lt;string, []&gt;("model_concatenate_concat")];
            tensor&lt;fp32, [1, 640]&gt; model_batch_normalization_1_batchnorm_mul_1 = mul(x = model_concatenate_concat, y = model_batch_normalization_1_batchnorm_mul)[name = tensor&lt;string, []&gt;("model_batch_normalization_1_batchnorm_mul_1")];
            tensor&lt;fp32, [1, 640]&gt; model_batch_normalization_1_batchnorm_add_1 = add(x = model_batch_normalization_1_batchnorm_mul_1, y = model_batch_normalization_1_batchnorm_sub)[name = tensor&lt;string, []&gt;("model_batch_normalization_1_batchnorm_add_1")];
            tensor&lt;string, []&gt; model_batch_normalization_1_batchnorm_add_1_to_fp16_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_batch_normalization_1_batchnorm_add_1_to_fp16_dtype_0"), val = tensor&lt;string, []&gt;("fp16")];
            tensor&lt;fp16, [512, 640]&gt; transpose_2_to_fp16 = const()[name = tensor&lt;string, []&gt;("transpose_2_to_fp16"), val = tensor&lt;fp16, [512, 640]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(1777152)))];
            tensor&lt;fp16, [512]&gt; model_dense_2_BiasAdd_bias_0_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_dense_2_BiasAdd_bias_0_to_fp16"), val = tensor&lt;fp16, [512]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(2432576)))];
            tensor&lt;fp16, [1, 640]&gt; model_batch_normalization_1_batchnorm_add_1_to_fp16 = cast(dtype = model_batch_normalization_1_batchnorm_add_1_to_fp16_dtype_0, x = model_batch_normalization_1_batchnorm_add_1)[name = tensor&lt;string, []&gt;("cast_5")];
            tensor&lt;fp16, [1, 512]&gt; model_dense_2_BiasAdd_cast_fp16 = linear(bias = model_dense_2_BiasAdd_bias_0_to_fp16, weight = transpose_2_to_fp16, x = model_batch_normalization_1_batchnorm_add_1_to_fp16)[name = tensor&lt;string, []&gt;("model_dense_2_BiasAdd_cast_fp16")];
            tensor&lt;string, []&gt; model_dense_2_BiasAdd_cast_fp16_to_fp32_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_dense_2_BiasAdd_cast_fp16_to_fp32_dtype_0"), val = tensor&lt;string, []&gt;("fp32")];
            tensor&lt;fp32, [1, 512]&gt; model_dense_2_BiasAdd_cast_fp16_to_fp32 = cast(dtype = model_dense_2_BiasAdd_cast_fp16_to_fp32_dtype_0, x = model_dense_2_BiasAdd_cast_fp16)[name = tensor&lt;string, []&gt;("cast_4")];
            tensor&lt;fp32, [1, 512]&gt; model_dense_2_Relu = relu(x = model_dense_2_BiasAdd_cast_fp16_to_fp32)[name = tensor&lt;string, []&gt;("model_dense_2_Relu")];
            tensor&lt;fp32, [1, 512]&gt; model_batch_normalization_2_batchnorm_mul_1 = mul(x = model_dense_2_Relu, y = model_batch_normalization_2_batchnorm_mul)[name = tensor&lt;string, []&gt;("model_batch_normalization_2_batchnorm_mul_1")];
            tensor&lt;fp32, [1, 512]&gt; model_batch_normalization_2_batchnorm_add_1 = add(x = model_batch_normalization_2_batchnorm_mul_1, y = model_batch_normalization_2_batchnorm_sub)[name = tensor&lt;string, []&gt;("model_batch_normalization_2_batchnorm_add_1")];
            tensor&lt;string, []&gt; model_batch_normalization_2_batchnorm_add_1_to_fp16_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_batch_normalization_2_batchnorm_add_1_to_fp16_dtype_0"), val = tensor&lt;string, []&gt;("fp16")];
            tensor&lt;fp16, [128, 512]&gt; transpose_3_to_fp16 = const()[name = tensor&lt;string, []&gt;("transpose_3_to_fp16"), val = tensor&lt;fp16, [128, 512]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(2433664)))];
            tensor&lt;fp16, [128]&gt; model_dense_3_BiasAdd_bias_0_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_dense_3_BiasAdd_bias_0_to_fp16"), val = tensor&lt;fp16, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(2564800)))];
            tensor&lt;fp16, [1, 512]&gt; model_batch_normalization_2_batchnorm_add_1_to_fp16 = cast(dtype = model_batch_normalization_2_batchnorm_add_1_to_fp16_dtype_0, x = model_batch_normalization_2_batchnorm_add_1)[name = tensor&lt;string, []&gt;("cast_3")];
            tensor&lt;fp16, [1, 128]&gt; model_dense_3_BiasAdd_cast_fp16 = linear(bias = model_dense_3_BiasAdd_bias_0_to_fp16, weight = transpose_3_to_fp16, x = model_batch_normalization_2_batchnorm_add_1_to_fp16)[name = tensor&lt;string, []&gt;("model_dense_3_BiasAdd_cast_fp16")];
            tensor&lt;string, []&gt; model_dense_3_BiasAdd_cast_fp16_to_fp32_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_dense_3_BiasAdd_cast_fp16_to_fp32_dtype_0"), val = tensor&lt;string, []&gt;("fp32")];
            tensor&lt;fp32, [1, 128]&gt; model_dense_3_BiasAdd_cast_fp16_to_fp32 = cast(dtype = model_dense_3_BiasAdd_cast_fp16_to_fp32_dtype_0, x = model_dense_3_BiasAdd_cast_fp16)[name = tensor&lt;string, []&gt;("cast_2")];
            tensor&lt;fp32, [1, 128]&gt; model_dense_3_Relu = relu(x = model_dense_3_BiasAdd_cast_fp16_to_fp32)[name = tensor&lt;string, []&gt;("model_dense_3_Relu")];
            tensor&lt;string, []&gt; model_dense_3_Relu_to_fp16_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_dense_3_Relu_to_fp16_dtype_0"), val = tensor&lt;string, []&gt;("fp16")];
            tensor&lt;fp16, [2, 128]&gt; transpose_4_to_fp16 = const()[name = tensor&lt;string, []&gt;("transpose_4_to_fp16"), val = tensor&lt;fp16, [2, 128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(2565120)))];
            tensor&lt;fp16, [2]&gt; model_labels_BiasAdd_bias_0_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_labels_BiasAdd_bias_0_to_fp16"), val = tensor&lt;fp16, [2]&gt;([0x1.becp-12, -0x1.becp-12])];
            tensor&lt;fp16, [1, 128]&gt; model_dense_3_Relu_to_fp16 = cast(dtype = model_dense_3_Relu_to_fp16_dtype_0, x = model_dense_3_Relu)[name = tensor&lt;string, []&gt;("cast_1")];
            tensor&lt;fp16, [1, 2]&gt; model_labels_BiasAdd_cast_fp16 = linear(bias = model_labels_BiasAdd_bias_0_to_fp16, weight = transpose_4_to_fp16, x = model_dense_3_Relu_to_fp16)[name = tensor&lt;string, []&gt;("model_labels_BiasAdd_cast_fp16")];
            tensor&lt;string, []&gt; model_labels_BiasAdd_cast_fp16_to_fp32_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_labels_BiasAdd_cast_fp16_to_fp32_dtype_0"), val = tensor&lt;string, []&gt;("fp32")];
            tensor&lt;int32, []&gt; model_labels_Softmax_axis_0 = const()[name = tensor&lt;string, []&gt;("model_labels_Softmax_axis_0"), val = tensor&lt;int32, []&gt;(-1)];
            tensor&lt;fp32, [1, 2]&gt; model_labels_BiasAdd_cast_fp16_to_fp32 = cast(dtype = model_labels_BiasAdd_cast_fp16_to_fp32_dtype_0, x = model_labels_BiasAdd_cast_fp16)[name = tensor&lt;string, []&gt;("cast_0")];
            tensor&lt;fp32, [1, 2]&gt; Identity = softmax(axis = model_labels_Softmax_axis_0, x = model_labels_BiasAdd_cast_fp16_to_fp32)[name = tensor&lt;string, []&gt;("model_labels_Softmax")];
        } -&gt; (Identity);
}</code></pre>
                    </div>
                </div>
            
                <div class="file-section" id="file-3">
                    <div class="file-header">
                        <h2>ðŸ“„ model.mil 25</h2>
                        <div class="file-meta">Size: 26.69 KB</div>
                    </div>
                    <div class="file-content">
                        <pre><code>program(1.0)
[buildInfo = dict&lt;tensor&lt;string, []&gt;, tensor&lt;string, []&gt;&gt;({{"coremlc-component-MIL", "3405.2.1"}, {"coremlc-version", "3405.2.1"}, {"coremltools-component-tensorflow", "2.13.0"}, {"coremltools-version", "7.1"}, {"mldb_token", "mldb-wzxy8wby59"}})]
{
    func main&lt;ios15&gt;(tensor&lt;fp32, [1, 2257]&gt; x) {
            tensor&lt;int32, []&gt; model_lstm_PartitionedCall_time = const()[name = tensor&lt;string, []&gt;("model_lstm_PartitionedCall_time"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, []&gt; model_lstm_PartitionedCall_TensorArrayV2_1_num_elements = const()[name = tensor&lt;string, []&gt;("model_lstm_PartitionedCall_TensorArrayV2_1_num_elements"), val = tensor&lt;int32, []&gt;(1)];
            tensor&lt;int32, [3]&gt; model_tf_split_1_Const = const()[name = tensor&lt;string, []&gt;("model_tf_split_1_Const"), val = tensor&lt;int32, [3]&gt;([256, 2000, 1])];
            tensor&lt;int32, []&gt; model_tf_split_1_split_split_dim = const()[name = tensor&lt;string, []&gt;("model_tf_split_1_split_split_dim"), val = tensor&lt;int32, []&gt;(1)];
            tensor&lt;int32, [3]&gt; model_lstm_PartitionedCall_transpose_perm = const()[name = tensor&lt;string, []&gt;("model_lstm_PartitionedCall_transpose_perm"), val = tensor&lt;int32, [3]&gt;([1, 0, 2])];
            tensor&lt;int32, [2]&gt; model_flatten_1_Const = const()[name = tensor&lt;string, []&gt;("model_flatten_1_Const"), val = tensor&lt;int32, [2]&gt;([-1, 256])];
            tensor&lt;int32, []&gt; model_concatenate_concat_axis = const()[name = tensor&lt;string, []&gt;("model_concatenate_concat_axis"), val = tensor&lt;int32, []&gt;(1)];
            tensor&lt;fp16, [64, 256]&gt; Func_model_lstm_PartitionedCall_input__3 = const()[name = tensor&lt;string, []&gt;("Func_model_lstm_PartitionedCall_input__3"), val = tensor&lt;fp16, [64, 256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(64)))];
            tensor&lt;fp16, [64, 256]&gt; Func_model_lstm_PartitionedCall_input__4 = const()[name = tensor&lt;string, []&gt;("Func_model_lstm_PartitionedCall_input__4"), val = tensor&lt;fp16, [64, 256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(32896)))];
            tensor&lt;fp16, [256]&gt; Func_model_lstm_PartitionedCall_input__5 = const()[name = tensor&lt;string, []&gt;("Func_model_lstm_PartitionedCall_input__5"), val = tensor&lt;fp16, [256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(65728)))];
            tensor&lt;string, []&gt; model_lstm_PartitionedCall_TensorArrayV2_1_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_lstm_PartitionedCall_TensorArrayV2_1_dtype_0"), val = tensor&lt;string, []&gt;("fp16")];
            tensor&lt;bool, []&gt; model_lstm_PartitionedCall_TensorArrayV2_1_dynamic_length_0 = const()[name = tensor&lt;string, []&gt;("model_lstm_PartitionedCall_TensorArrayV2_1_dynamic_length_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;int32, []&gt; model_lstm_PartitionedCall_TensorArrayV2_1_elem_shape0_0 = const()[name = tensor&lt;string, []&gt;("model_lstm_PartitionedCall_TensorArrayV2_1_elem_shape0_0"), val = tensor&lt;int32, []&gt;(1)];
            tensor&lt;int32, []&gt; model_lstm_PartitionedCall_TensorArrayV2_1_elem_shape1_0 = const()[name = tensor&lt;string, []&gt;("model_lstm_PartitionedCall_TensorArrayV2_1_elem_shape1_0"), val = tensor&lt;int32, []&gt;(64)];
            list&lt;tensor&lt;fp16, [1, 64]&gt;, 1&gt; model_lstm_PartitionedCall_TensorArrayV2_1 = make_list(dtype = model_lstm_PartitionedCall_TensorArrayV2_1_dtype_0, dynamic_length = model_lstm_PartitionedCall_TensorArrayV2_1_dynamic_length_0, elem_shape = (model_lstm_PartitionedCall_TensorArrayV2_1_elem_shape0_0, model_lstm_PartitionedCall_TensorArrayV2_1_elem_shape1_0), init_length = model_lstm_PartitionedCall_TensorArrayV2_1_num_elements)[name = tensor&lt;string, []&gt;("model_lstm_PartitionedCall_TensorArrayV2_1")];
            tensor&lt;int32, []&gt; model_tf_split_1_split_num_splits_0 = const()[name = tensor&lt;string, []&gt;("model_tf_split_1_split_num_splits_0"), val = tensor&lt;int32, []&gt;(3)];
            tensor&lt;fp32, [1, 256]&gt; model_tf_split_1_split_0, tensor&lt;fp32, [1, 2000]&gt; model_tf_split_1_split_1, tensor&lt;fp32, [1, 1]&gt; model_tf_split_1_split_2 = split(axis = model_tf_split_1_split_split_dim, num_splits = model_tf_split_1_split_num_splits_0, split_sizes = model_tf_split_1_Const, x = x)[name = tensor&lt;string, []&gt;("model_tf_split_1_split")];
            tensor&lt;string, []&gt; model_embedding_Cast_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_embedding_Cast_dtype_0"), val = tensor&lt;string, []&gt;("int32")];
            tensor&lt;string, []&gt; model_concatenate_Cast_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_concatenate_Cast_dtype_0"), val = tensor&lt;string, []&gt;("fp16")];
            tensor&lt;string, []&gt; model_embedding_1_Cast_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_embedding_1_Cast_dtype_0"), val = tensor&lt;string, []&gt;("int32")];
            tensor&lt;int32, []&gt; model_embedding_embedding_lookup_axis_0 = const()[name = tensor&lt;string, []&gt;("model_embedding_embedding_lookup_axis_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;fp16, [10000, 64]&gt; model_embedding_embedding_lookup_16368_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_embedding_embedding_lookup_16368_to_fp16"), val = tensor&lt;fp16, [10000, 64]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(66304)))];
            tensor&lt;int32, [1, 256]&gt; cast_2 = cast(dtype = model_embedding_Cast_dtype_0, x = model_tf_split_1_split_0)[name = tensor&lt;string, []&gt;("cast_2")];
            tensor&lt;fp16, [1, 256, 64]&gt; model_embedding_embedding_lookup_cast_fp16 = gather(axis = model_embedding_embedding_lookup_axis_0, indices = cast_2, x = model_embedding_embedding_lookup_16368_to_fp16)[name = tensor&lt;string, []&gt;("model_embedding_embedding_lookup_cast_fp16")];
            tensor&lt;int32, []&gt; model_embedding_1_embedding_lookup_axis_0 = const()[name = tensor&lt;string, []&gt;("model_embedding_1_embedding_lookup_axis_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;fp16, [621, 256]&gt; model_embedding_1_embedding_lookup_16374_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_embedding_1_embedding_lookup_16374_to_fp16"), val = tensor&lt;fp16, [621, 256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(1346368)))];
            tensor&lt;int32, [1, 1]&gt; cast_0 = cast(dtype = model_embedding_1_Cast_dtype_0, x = model_tf_split_1_split_2)[name = tensor&lt;string, []&gt;("cast_0")];
            tensor&lt;fp16, [1, 1, 256]&gt; model_embedding_1_embedding_lookup_cast_fp16 = gather(axis = model_embedding_1_embedding_lookup_axis_0, indices = cast_0, x = model_embedding_1_embedding_lookup_16374_to_fp16)[name = tensor&lt;string, []&gt;("model_embedding_1_embedding_lookup_cast_fp16")];
            tensor&lt;fp16, [1, 256]&gt; model_flatten_1_Reshape = reshape(shape = model_flatten_1_Const, x = model_embedding_1_embedding_lookup_cast_fp16)[name = tensor&lt;string, []&gt;("model_flatten_1_Reshape")];
            tensor&lt;fp16, [256, 256]&gt; transpose_0 = const()[name = tensor&lt;string, []&gt;("transpose_0"), val = tensor&lt;fp16, [256, 256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(1664384)))];
            tensor&lt;fp16, [256]&gt; model_dense_1_BiasAdd_bias_0 = const()[name = tensor&lt;string, []&gt;("model_dense_1_BiasAdd_bias_0"), val = tensor&lt;fp16, [256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(1795520)))];
            tensor&lt;fp16, [1, 256]&gt; model_dense_1_BiasAdd = linear(bias = model_dense_1_BiasAdd_bias_0, weight = transpose_0, x = model_flatten_1_Reshape)[name = tensor&lt;string, []&gt;("model_dense_1_BiasAdd")];
            tensor&lt;fp16, [1, 64]&gt; model_lstm_zeros = const()[name = tensor&lt;string, []&gt;("model_lstm_zeros"), val = tensor&lt;fp16, [1, 64]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(1796096)))];
            tensor&lt;fp16, [1, 64]&gt; model_lstm_zeros_1 = const()[name = tensor&lt;string, []&gt;("model_lstm_zeros_1"), val = tensor&lt;fp16, [1, 64]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(1796288)))];
            tensor&lt;fp16, [1, 256]&gt; model_dense_1_Relu = relu(x = model_dense_1_BiasAdd)[name = tensor&lt;string, []&gt;("model_dense_1_Relu")];
            tensor&lt;int32, []&gt; slice_by_index_0 = const()[name = tensor&lt;string, []&gt;("slice_by_index_0"), val = tensor&lt;int32, []&gt;(256)];
            tensor&lt;string, []&gt; tf_make_list_0_dtype_0 = const()[name = tensor&lt;string, []&gt;("tf_make_list_0_dtype_0"), val = tensor&lt;string, []&gt;("fp16")];
            tensor&lt;bool, []&gt; tf_make_list_0_dynamic_length_0 = const()[name = tensor&lt;string, []&gt;("tf_make_list_0_dynamic_length_0"), val = tensor&lt;bool, []&gt;(true)];
            tensor&lt;int32, []&gt; tf_make_list_0_elem_shape0_0 = const()[name = tensor&lt;string, []&gt;("tf_make_list_0_elem_shape0_0"), val = tensor&lt;int32, []&gt;(1)];
            tensor&lt;int32, []&gt; tf_make_list_0_elem_shape1_0 = const()[name = tensor&lt;string, []&gt;("tf_make_list_0_elem_shape1_0"), val = tensor&lt;int32, []&gt;(64)];
            list&lt;tensor&lt;fp16, [1, 64]&gt;, 256&gt; tf_make_list_0 = make_list(dtype = tf_make_list_0_dtype_0, dynamic_length = tf_make_list_0_dynamic_length_0, elem_shape = (tf_make_list_0_elem_shape0_0, tf_make_list_0_elem_shape1_0), init_length = slice_by_index_0)[name = tensor&lt;string, []&gt;("tf_make_list_0")];
            tensor&lt;int32, [256]&gt; range_1d_0 = const()[name = tensor&lt;string, []&gt;("range_1d_0"), val = tensor&lt;int32, [256]&gt;([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255])];
            tensor&lt;fp16, [256, 1, 64]&gt; transpose_4 = transpose(perm = model_lstm_PartitionedCall_transpose_perm, x = model_embedding_embedding_lookup_cast_fp16)[name = tensor&lt;string, []&gt;("transpose_4")];
            list&lt;tensor&lt;fp16, [1, 64]&gt;, 256&gt; model_lstm_PartitionedCall_TensorArrayUnstack_TensorListFromTensor = list_scatter(indices = range_1d_0, ls = tf_make_list_0, value = transpose_4)[name = tensor&lt;string, []&gt;("model_lstm_PartitionedCall_TensorArrayUnstack_TensorListFromTensor")];
            tensor&lt;int32, []&gt; model_lstm_PartitionedCall_strided_slice = const()[name = tensor&lt;string, []&gt;("model_lstm_PartitionedCall_strided_slice"), val = tensor&lt;int32, []&gt;(256)];
            tensor&lt;int32, []&gt; model_lstm_PartitionedCall_while_0, list&lt;tensor&lt;fp16, [1, 64]&gt;, 1&gt; model_lstm_PartitionedCall_while_1, tensor&lt;fp16, [1, 64]&gt; model_lstm_PartitionedCall_while_2, tensor&lt;fp16, [1, 64]&gt; model_lstm_PartitionedCall_while_3 = while_loop(loop_vars = (model_lstm_PartitionedCall_time, model_lstm_PartitionedCall_TensorArrayV2_1, model_lstm_zeros, model_lstm_zeros_1))[name = tensor&lt;string, []&gt;("model_lstm_PartitionedCall_while_renamed")]
                (tensor&lt;int32, []&gt; model_lstm_PartitionedCall_time_x0_1_1_0, list&lt;tensor&lt;fp16, [1, 64]&gt;, 1&gt; model_lstm_PartitionedCall_TensorArrayV2_1_x0, tensor&lt;fp16, [1, 64]&gt; model_lstm_zeros_x0_1_1_0, tensor&lt;fp16, [1, 64]&gt; model_lstm_zeros_1_x0_1_1_0) {
                    tensor&lt;bool, []&gt; model_lstm_PartitionedCall_while_while_cond_16452_while_Less = less(x = model_lstm_PartitionedCall_time_x0_1_1_0, y = model_lstm_PartitionedCall_strided_slice)[name = tensor&lt;string, []&gt;("model_lstm_PartitionedCall_while_while_cond_16452_while_Less")];
                } -&gt; (model_lstm_PartitionedCall_while_while_cond_16452_while_Less)
                (tensor&lt;int32, []&gt; model_lstm_PartitionedCall_time_x0_1_1_1, list&lt;tensor&lt;fp16, [1, 64]&gt;, 1&gt; model_lstm_PartitionedCall_TensorArrayV2_1_x0_1, tensor&lt;fp16, [1, 64]&gt; model_lstm_zeros_x0_1_1_1, tensor&lt;fp16, [1, 64]&gt; model_lstm_zeros_1_x0_1_1_1) {
                    tensor&lt;int32, []&gt; model_lstm_PartitionedCall_while_while_body_16453_while_split_split_dim = const()[name = tensor&lt;string, []&gt;("model_lstm_PartitionedCall_while_while_body_16453_while_split_split_dim"), val = tensor&lt;int32, []&gt;(1)];
                    tensor&lt;int32, []&gt; model_lstm_PartitionedCall_while_while_body_16453_while_TensorArrayV2Write_TensorListSetItem_index = const()[name = tensor&lt;string, []&gt;("model_lstm_PartitionedCall_while_while_body_16453_while_TensorArrayV2Write_TensorListSetItem_index"), val = tensor&lt;int32, []&gt;(0)];
                    tensor&lt;int32, []&gt; model_lstm_PartitionedCall_while_while_body_16453_while_add_2_y = const()[name = tensor&lt;string, []&gt;("model_lstm_PartitionedCall_while_while_body_16453_while_add_2_y"), val = tensor&lt;int32, []&gt;(1)];
                    tensor&lt;int32, []&gt; model_lstm_PartitionedCall_while_while_body_16453_while_add_2 = add(x = model_lstm_PartitionedCall_time_x0_1_1_1, y = model_lstm_PartitionedCall_while_while_body_16453_while_add_2_y)[name = tensor&lt;string, []&gt;("model_lstm_PartitionedCall_while_while_body_16453_while_add_2")];
                    tensor&lt;fp16, [1, 64]&gt; model_lstm_PartitionedCall_while_while_body_16453_while_TensorArrayV2Read_TensorListGetItem = list_read(index = model_lstm_PartitionedCall_time_x0_1_1_1, ls = model_lstm_PartitionedCall_TensorArrayUnstack_TensorListFromTensor)[name = tensor&lt;string, []&gt;("model_lstm_PartitionedCall_while_while_body_16453_while_TensorArrayV2Read_TensorListGetItem")];
                    tensor&lt;bool, []&gt; model_lstm_PartitionedCall_while_while_body_16453_while_MatMul_1_transpose_x_1 = const()[name = tensor&lt;string, []&gt;("model_lstm_PartitionedCall_while_while_body_16453_while_MatMul_1_transpose_x_1"), val = tensor&lt;bool, []&gt;(false)];
                    tensor&lt;bool, []&gt; model_lstm_PartitionedCall_while_while_body_16453_while_MatMul_1_transpose_y_1 = const()[name = tensor&lt;string, []&gt;("model_lstm_PartitionedCall_while_while_body_16453_while_MatMul_1_transpose_y_1"), val = tensor&lt;bool, []&gt;(false)];
                    tensor&lt;fp16, [1, 256]&gt; model_lstm_PartitionedCall_while_while_body_16453_while_MatMul_1 = matmul(transpose_x = model_lstm_PartitionedCall_while_while_body_16453_while_MatMul_1_transpose_x_1, transpose_y = model_lstm_PartitionedCall_while_while_body_16453_while_MatMul_1_transpose_y_1, x = model_lstm_zeros_x0_1_1_1, y = Func_model_lstm_PartitionedCall_input__4)[name = tensor&lt;string, []&gt;("model_lstm_PartitionedCall_while_while_body_16453_while_MatMul_1")];
                    tensor&lt;bool, []&gt; model_lstm_PartitionedCall_while_while_body_16453_while_MatMul_transpose_x_1 = const()[name = tensor&lt;string, []&gt;("model_lstm_PartitionedCall_while_while_body_16453_while_MatMul_transpose_x_1"), val = tensor&lt;bool, []&gt;(false)];
                    tensor&lt;bool, []&gt; model_lstm_PartitionedCall_while_while_body_16453_while_MatMul_transpose_y_1 = const()[name = tensor&lt;string, []&gt;("model_lstm_PartitionedCall_while_while_body_16453_while_MatMul_transpose_y_1"), val = tensor&lt;bool, []&gt;(false)];
                    tensor&lt;fp16, [1, 256]&gt; model_lstm_PartitionedCall_while_while_body_16453_while_MatMul = matmul(transpose_x = model_lstm_PartitionedCall_while_while_body_16453_while_MatMul_transpose_x_1, transpose_y = model_lstm_PartitionedCall_while_while_body_16453_while_MatMul_transpose_y_1, x = model_lstm_PartitionedCall_while_while_body_16453_while_TensorArrayV2Read_TensorListGetItem, y = Func_model_lstm_PartitionedCall_input__3)[name = tensor&lt;string, []&gt;("model_lstm_PartitionedCall_while_while_body_16453_while_MatMul")];
                    tensor&lt;fp16, [1, 256]&gt; model_lstm_PartitionedCall_while_while_body_16453_while_add = add(x = model_lstm_PartitionedCall_while_while_body_16453_while_MatMul, y = model_lstm_PartitionedCall_while_while_body_16453_while_MatMul_1)[name = tensor&lt;string, []&gt;("model_lstm_PartitionedCall_while_while_body_16453_while_add")];
                    tensor&lt;fp16, [1, 256]&gt; model_lstm_PartitionedCall_while_while_body_16453_while_BiasAdd = add(x = model_lstm_PartitionedCall_while_while_body_16453_while_add, y = Func_model_lstm_PartitionedCall_input__5)[name = tensor&lt;string, []&gt;("model_lstm_PartitionedCall_while_while_body_16453_while_BiasAdd")];
                    tensor&lt;int32, []&gt; model_lstm_PartitionedCall_while_while_body_16453_while_split_num_splits_1 = const()[name = tensor&lt;string, []&gt;("model_lstm_PartitionedCall_while_while_body_16453_while_split_num_splits_1"), val = tensor&lt;int32, []&gt;(4)];
                    tensor&lt;fp16, [1, 64]&gt; model_lstm_PartitionedCall_while_while_body_16453_while_split_0, tensor&lt;fp16, [1, 64]&gt; model_lstm_PartitionedCall_while_while_body_16453_while_split_1, tensor&lt;fp16, [1, 64]&gt; model_lstm_PartitionedCall_while_while_body_16453_while_split_2, tensor&lt;fp16, [1, 64]&gt; model_lstm_PartitionedCall_while_while_body_16453_while_split_3 = split(axis = model_lstm_PartitionedCall_while_while_body_16453_while_split_split_dim, num_splits = model_lstm_PartitionedCall_while_while_body_16453_while_split_num_splits_1, x = model_lstm_PartitionedCall_while_while_body_16453_while_BiasAdd)[name = tensor&lt;string, []&gt;("model_lstm_PartitionedCall_while_while_body_16453_while_split")];
                    tensor&lt;fp16, [1, 64]&gt; model_lstm_PartitionedCall_while_while_body_16453_while_Sigmoid = sigmoid(x = model_lstm_PartitionedCall_while_while_body_16453_while_split_0)[name = tensor&lt;string, []&gt;("model_lstm_PartitionedCall_while_while_body_16453_while_Sigmoid")];
                    tensor&lt;fp16, [1, 64]&gt; model_lstm_PartitionedCall_while_while_body_16453_while_Sigmoid_1 = sigmoid(x = model_lstm_PartitionedCall_while_while_body_16453_while_split_1)[name = tensor&lt;string, []&gt;("model_lstm_PartitionedCall_while_while_body_16453_while_Sigmoid_1")];
                    tensor&lt;fp16, [1, 64]&gt; model_lstm_PartitionedCall_while_while_body_16453_while_Tanh = tanh(x = model_lstm_PartitionedCall_while_while_body_16453_while_split_2)[name = tensor&lt;string, []&gt;("model_lstm_PartitionedCall_while_while_body_16453_while_Tanh")];
                    tensor&lt;fp16, [1, 64]&gt; model_lstm_PartitionedCall_while_while_body_16453_while_Sigmoid_2 = sigmoid(x = model_lstm_PartitionedCall_while_while_body_16453_while_split_3)[name = tensor&lt;string, []&gt;("model_lstm_PartitionedCall_while_while_body_16453_while_Sigmoid_2")];
                    tensor&lt;fp16, [1, 64]&gt; model_lstm_PartitionedCall_while_while_body_16453_while_mul = mul(x = model_lstm_PartitionedCall_while_while_body_16453_while_Sigmoid_1, y = model_lstm_zeros_1_x0_1_1_1)[name = tensor&lt;string, []&gt;("model_lstm_PartitionedCall_while_while_body_16453_while_mul")];
                    tensor&lt;fp16, [1, 64]&gt; model_lstm_PartitionedCall_while_while_body_16453_while_mul_1 = mul(x = model_lstm_PartitionedCall_while_while_body_16453_while_Sigmoid, y = model_lstm_PartitionedCall_while_while_body_16453_while_Tanh)[name = tensor&lt;string, []&gt;("model_lstm_PartitionedCall_while_while_body_16453_while_mul_1")];
                    tensor&lt;fp16, [1, 64]&gt; model_lstm_PartitionedCall_while_while_body_16453_while_add_1 = add(x = model_lstm_PartitionedCall_while_while_body_16453_while_mul, y = model_lstm_PartitionedCall_while_while_body_16453_while_mul_1)[name = tensor&lt;string, []&gt;("model_lstm_PartitionedCall_while_while_body_16453_while_add_1")];
                    tensor&lt;fp16, [1, 64]&gt; model_lstm_PartitionedCall_while_while_body_16453_while_Tanh_1 = tanh(x = model_lstm_PartitionedCall_while_while_body_16453_while_add_1)[name = tensor&lt;string, []&gt;("model_lstm_PartitionedCall_while_while_body_16453_while_Tanh_1")];
                    tensor&lt;fp16, [1, 64]&gt; model_lstm_PartitionedCall_while_while_body_16453_while_mul_2 = mul(x = model_lstm_PartitionedCall_while_while_body_16453_while_Sigmoid_2, y = model_lstm_PartitionedCall_while_while_body_16453_while_Tanh_1)[name = tensor&lt;string, []&gt;("model_lstm_PartitionedCall_while_while_body_16453_while_mul_2")];
                    list&lt;tensor&lt;fp16, [1, 64]&gt;, 1&gt; model_lstm_PartitionedCall_while_while_body_16453_while_TensorArrayV2Write_TensorListSetItem = list_write(index = model_lstm_PartitionedCall_while_while_body_16453_while_TensorArrayV2Write_TensorListSetItem_index, ls = model_lstm_PartitionedCall_TensorArrayV2_1_x0_1, value = model_lstm_PartitionedCall_while_while_body_16453_while_mul_2)[name = tensor&lt;string, []&gt;("model_lstm_PartitionedCall_while_while_body_16453_while_TensorArrayV2Write_TensorListSetItem")];
                } -&gt; (model_lstm_PartitionedCall_while_while_body_16453_while_add_2, model_lstm_PartitionedCall_while_while_body_16453_while_TensorArrayV2Write_TensorListSetItem, model_lstm_PartitionedCall_while_while_body_16453_while_mul_2, model_lstm_PartitionedCall_while_while_body_16453_while_add_1);
            tensor&lt;int32, [1]&gt; range_1d_1 = const()[name = tensor&lt;string, []&gt;("range_1d_1"), val = tensor&lt;int32, [1]&gt;([0])];
            tensor&lt;fp16, [1, 1, 64]&gt; model_lstm_PartitionedCall_TensorArrayV2Stack_TensorListStack = list_gather(indices = range_1d_1, ls = model_lstm_PartitionedCall_while_1)[name = tensor&lt;string, []&gt;("model_lstm_PartitionedCall_TensorArrayV2Stack_TensorListStack")];
            tensor&lt;int32, [3]&gt; model_lstm_PartitionedCall_strided_slice_2_begin_0 = const()[name = tensor&lt;string, []&gt;("model_lstm_PartitionedCall_strided_slice_2_begin_0"), val = tensor&lt;int32, [3]&gt;([-1, 0, 0])];
            tensor&lt;int32, [3]&gt; model_lstm_PartitionedCall_strided_slice_2_end_0 = const()[name = tensor&lt;string, []&gt;("model_lstm_PartitionedCall_strided_slice_2_end_0"), val = tensor&lt;int32, [3]&gt;([0, 0, 0])];
            tensor&lt;int32, [3]&gt; model_lstm_PartitionedCall_strided_slice_2_stride_0 = const()[name = tensor&lt;string, []&gt;("model_lstm_PartitionedCall_strided_slice_2_stride_0"), val = tensor&lt;int32, [3]&gt;([1, 1, 1])];
            tensor&lt;bool, [3]&gt; model_lstm_PartitionedCall_strided_slice_2_begin_mask_0 = const()[name = tensor&lt;string, []&gt;("model_lstm_PartitionedCall_strided_slice_2_begin_mask_0"), val = tensor&lt;bool, [3]&gt;([false, true, true])];
            tensor&lt;bool, [3]&gt; model_lstm_PartitionedCall_strided_slice_2_end_mask_0 = const()[name = tensor&lt;string, []&gt;("model_lstm_PartitionedCall_strided_slice_2_end_mask_0"), val = tensor&lt;bool, [3]&gt;([false, true, true])];
            tensor&lt;bool, [3]&gt; model_lstm_PartitionedCall_strided_slice_2_squeeze_mask_0 = const()[name = tensor&lt;string, []&gt;("model_lstm_PartitionedCall_strided_slice_2_squeeze_mask_0"), val = tensor&lt;bool, [3]&gt;([true, false, false])];
            tensor&lt;fp16, [1, 64]&gt; model_lstm_PartitionedCall_strided_slice_2 = slice_by_index(begin = model_lstm_PartitionedCall_strided_slice_2_begin_0, begin_mask = model_lstm_PartitionedCall_strided_slice_2_begin_mask_0, end = model_lstm_PartitionedCall_strided_slice_2_end_0, end_mask = model_lstm_PartitionedCall_strided_slice_2_end_mask_0, squeeze_mask = model_lstm_PartitionedCall_strided_slice_2_squeeze_mask_0, stride = model_lstm_PartitionedCall_strided_slice_2_stride_0, x = model_lstm_PartitionedCall_TensorArrayV2Stack_TensorListStack)[name = tensor&lt;string, []&gt;("model_lstm_PartitionedCall_strided_slice_2")];
            tensor&lt;fp16, [256, 64]&gt; transpose_1 = const()[name = tensor&lt;string, []&gt;("transpose_1"), val = tensor&lt;fp16, [256, 64]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(1796480)))];
            tensor&lt;fp16, [256]&gt; model_dense_BiasAdd_bias_0 = const()[name = tensor&lt;string, []&gt;("model_dense_BiasAdd_bias_0"), val = tensor&lt;fp16, [256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(1829312)))];
            tensor&lt;fp16, [1, 256]&gt; model_dense_BiasAdd = linear(bias = model_dense_BiasAdd_bias_0, weight = transpose_1, x = model_lstm_PartitionedCall_strided_slice_2)[name = tensor&lt;string, []&gt;("model_dense_BiasAdd")];
            tensor&lt;fp16, [1, 256]&gt; model_dense_Relu = relu(x = model_dense_BiasAdd)[name = tensor&lt;string, []&gt;("model_dense_Relu")];
            tensor&lt;bool, []&gt; model_concatenate_concat_interleave_0 = const()[name = tensor&lt;string, []&gt;("model_concatenate_concat_interleave_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [1, 2000]&gt; cast_1 = cast(dtype = model_concatenate_Cast_dtype_0, x = model_tf_split_1_split_1)[name = tensor&lt;string, []&gt;("cast_1")];
            tensor&lt;fp16, [1, 2512]&gt; model_concatenate_concat = concat(axis = model_concatenate_concat_axis, interleave = model_concatenate_concat_interleave_0, values = (model_dense_Relu, cast_1, model_dense_1_Relu))[name = tensor&lt;string, []&gt;("model_concatenate_concat")];
            tensor&lt;fp16, [256, 2512]&gt; transpose_2 = const()[name = tensor&lt;string, []&gt;("transpose_2"), val = tensor&lt;fp16, [256, 2512]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(1829888)))];
            tensor&lt;fp16, [256]&gt; model_dense_2_BiasAdd_bias_0 = const()[name = tensor&lt;string, []&gt;("model_dense_2_BiasAdd_bias_0"), val = tensor&lt;fp16, [256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(3116096)))];
            tensor&lt;fp16, [1, 256]&gt; model_dense_2_BiasAdd = linear(bias = model_dense_2_BiasAdd_bias_0, weight = transpose_2, x = model_concatenate_concat)[name = tensor&lt;string, []&gt;("model_dense_2_BiasAdd")];
            tensor&lt;fp16, [1, 256]&gt; model_dense_2_Relu = relu(x = model_dense_2_BiasAdd)[name = tensor&lt;string, []&gt;("model_dense_2_Relu")];
            tensor&lt;fp16, [5, 256]&gt; transpose_3 = const()[name = tensor&lt;string, []&gt;("transpose_3"), val = tensor&lt;fp16, [5, 256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(3116672)))];
            tensor&lt;fp16, [5]&gt; model_dense_3_BiasAdd_bias_0 = const()[name = tensor&lt;string, []&gt;("model_dense_3_BiasAdd_bias_0"), val = tensor&lt;fp16, [5]&gt;([-0x1.33p-7, -0x1.454p-3, -0x1.988p-7, 0x1.a2cp-3, -0x1.90cp-10])];
            tensor&lt;fp16, [1, 5]&gt; model_dense_3_BiasAdd = linear(bias = model_dense_3_BiasAdd_bias_0, weight = transpose_3, x = model_dense_2_Relu)[name = tensor&lt;string, []&gt;("model_dense_3_BiasAdd")];
            tensor&lt;int32, []&gt; model_dense_3_Softmax_axis_0 = const()[name = tensor&lt;string, []&gt;("model_dense_3_Softmax_axis_0"), val = tensor&lt;int32, []&gt;(-1)];
            tensor&lt;fp16, [1, 5]&gt; Identity__pre__output__fp32__cast = softmax(axis = model_dense_3_Softmax_axis_0, x = model_dense_3_BiasAdd)[name = tensor&lt;string, []&gt;("model_dense_3_Softmax")];
            tensor&lt;string, []&gt; cast_3_dtype_0 = const()[name = tensor&lt;string, []&gt;("cast_3_dtype_0"), val = tensor&lt;string, []&gt;("fp32")];
            tensor&lt;fp32, [1, 5]&gt; Identity = cast(dtype = cast_3_dtype_0, x = Identity__pre__output__fp32__cast)[name = tensor&lt;string, []&gt;("cast_3")];
        } -&gt; (Identity);
}</code></pre>
                    </div>
                </div>
            
                <div class="file-section" id="file-4">
                    <div class="file-header">
                        <h2>ðŸ“„ model.mil 10</h2>
                        <div class="file-meta">Size: 41.08 KB</div>
                    </div>
                    <div class="file-content">
                        <pre><code>program(1.0)
[buildInfo = dict&lt;tensor&lt;string, []&gt;, tensor&lt;string, []&gt;&gt;({{"coremlc-component-MIL", "3405.2.1"}, {"coremlc-version", "3405.2.1"}, {"coremltools-component-tensorflow", "2.8.0"}, {"coremltools-version", "7.1.2"}, {"mldb_token", "mldb-wzxy8wby59"}})]
{
    func main&lt;ios15&gt;(tensor&lt;fp32, [1, 10]&gt; x_sen, tensor&lt;fp32, [1, 50]&gt; x_sub) {
            tensor&lt;int32, []&gt; model_1_lstm_2_PartitionedCall_time = const()[name = tensor&lt;string, []&gt;("model_1_lstm_2_PartitionedCall_time"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, [3]&gt; model_1_lstm_2_PartitionedCall_transpose_perm = const()[name = tensor&lt;string, []&gt;("model_1_lstm_2_PartitionedCall_transpose_perm"), val = tensor&lt;int32, [3]&gt;([1, 0, 2])];
            tensor&lt;fp32, [100, 400]&gt; Func_model_1_lstm_2_PartitionedCall_input__14 = const()[name = tensor&lt;string, []&gt;("Func_model_1_lstm_2_PartitionedCall_input__14"), val = tensor&lt;fp32, [100, 400]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(64)))];
            tensor&lt;fp32, [100, 400]&gt; Func_model_1_lstm_2_PartitionedCall_input__15 = const()[name = tensor&lt;string, []&gt;("Func_model_1_lstm_2_PartitionedCall_input__15"), val = tensor&lt;fp32, [100, 400]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(160128)))];
            tensor&lt;fp32, [400]&gt; Func_model_1_lstm_2_PartitionedCall_input__16 = const()[name = tensor&lt;string, []&gt;("Func_model_1_lstm_2_PartitionedCall_input__16"), val = tensor&lt;fp32, [400]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(320192)))];
            tensor&lt;int32, []&gt; model_1_lstm_3_PartitionedCall_time = const()[name = tensor&lt;string, []&gt;("model_1_lstm_3_PartitionedCall_time"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, [3]&gt; model_1_lstm_3_PartitionedCall_transpose_perm = const()[name = tensor&lt;string, []&gt;("model_1_lstm_3_PartitionedCall_transpose_perm"), val = tensor&lt;int32, [3]&gt;([1, 0, 2])];
            tensor&lt;fp32, [10, 64]&gt; Func_model_1_lstm_3_PartitionedCall_input__3 = const()[name = tensor&lt;string, []&gt;("Func_model_1_lstm_3_PartitionedCall_input__3"), val = tensor&lt;fp32, [10, 64]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(321856)))];
            tensor&lt;fp32, [16, 64]&gt; Func_model_1_lstm_3_PartitionedCall_input__4 = const()[name = tensor&lt;string, []&gt;("Func_model_1_lstm_3_PartitionedCall_input__4"), val = tensor&lt;fp32, [16, 64]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(324480)))];
            tensor&lt;fp32, [64]&gt; Func_model_1_lstm_3_PartitionedCall_input__5 = const()[name = tensor&lt;string, []&gt;("Func_model_1_lstm_3_PartitionedCall_input__5"), val = tensor&lt;fp32, [64]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(328640)))];
            tensor&lt;int32, []&gt; model_1_concatenate_1_concat_axis = const()[name = tensor&lt;string, []&gt;("model_1_concatenate_1_concat_axis"), val = tensor&lt;int32, []&gt;(1)];
            tensor&lt;string, []&gt; model_1_embedding_2_Cast_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_1_embedding_2_Cast_dtype_0"), val = tensor&lt;string, []&gt;("int32")];
            tensor&lt;string, []&gt; model_1_embedding_3_Cast_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_1_embedding_3_Cast_dtype_0"), val = tensor&lt;string, []&gt;("int32")];
            tensor&lt;int32, []&gt; model_1_embedding_2_embedding_lookup_axis_0 = const()[name = tensor&lt;string, []&gt;("model_1_embedding_2_embedding_lookup_axis_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;fp16, [10000, 100]&gt; model_1_embedding_2_embedding_lookup_48808644_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_embedding_2_embedding_lookup_48808644_to_fp16"), val = tensor&lt;fp16, [10000, 100]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(328960)))];
            tensor&lt;int32, [1, 50]&gt; cast_5 = cast(dtype = model_1_embedding_2_Cast_dtype_0, x = x_sub)[name = tensor&lt;string, []&gt;("cast_5")];
            tensor&lt;fp16, [1, 50, 100]&gt; model_1_embedding_2_embedding_lookup_cast_fp16 = gather(axis = model_1_embedding_2_embedding_lookup_axis_0, indices = cast_5, x = model_1_embedding_2_embedding_lookup_48808644_to_fp16)[name = tensor&lt;string, []&gt;("model_1_embedding_2_embedding_lookup_cast_fp16")];
            tensor&lt;string, []&gt; model_1_embedding_2_embedding_lookup_cast_fp16_to_fp32_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_1_embedding_2_embedding_lookup_cast_fp16_to_fp32_dtype_0"), val = tensor&lt;string, []&gt;("fp32")];
            tensor&lt;int32, []&gt; model_1_embedding_3_embedding_lookup_axis_0 = const()[name = tensor&lt;string, []&gt;("model_1_embedding_3_embedding_lookup_axis_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;fp16, [1000, 10]&gt; model_1_embedding_3_embedding_lookup_48808638_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_embedding_3_embedding_lookup_48808638_to_fp16"), val = tensor&lt;fp16, [1000, 10]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(2329024)))];
            tensor&lt;int32, [1, 10]&gt; cast_4 = cast(dtype = model_1_embedding_3_Cast_dtype_0, x = x_sen)[name = tensor&lt;string, []&gt;("cast_4")];
            tensor&lt;fp16, [1, 10, 10]&gt; model_1_embedding_3_embedding_lookup_cast_fp16 = gather(axis = model_1_embedding_3_embedding_lookup_axis_0, indices = cast_4, x = model_1_embedding_3_embedding_lookup_48808638_to_fp16)[name = tensor&lt;string, []&gt;("model_1_embedding_3_embedding_lookup_cast_fp16")];
            tensor&lt;string, []&gt; model_1_embedding_3_embedding_lookup_cast_fp16_to_fp32_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_1_embedding_3_embedding_lookup_cast_fp16_to_fp32_dtype_0"), val = tensor&lt;string, []&gt;("fp32")];
            tensor&lt;int32, []&gt; slice_by_index_0 = const()[name = tensor&lt;string, []&gt;("slice_by_index_0"), val = tensor&lt;int32, []&gt;(50)];
            tensor&lt;string, []&gt; tf_make_list_0_dtype_0 = const()[name = tensor&lt;string, []&gt;("tf_make_list_0_dtype_0"), val = tensor&lt;string, []&gt;("fp32")];
            tensor&lt;bool, []&gt; tf_make_list_0_dynamic_length_0 = const()[name = tensor&lt;string, []&gt;("tf_make_list_0_dynamic_length_0"), val = tensor&lt;bool, []&gt;(true)];
            tensor&lt;int32, []&gt; tf_make_list_0_elem_shape0_0 = const()[name = tensor&lt;string, []&gt;("tf_make_list_0_elem_shape0_0"), val = tensor&lt;int32, []&gt;(1)];
            tensor&lt;int32, []&gt; tf_make_list_0_elem_shape1_0 = const()[name = tensor&lt;string, []&gt;("tf_make_list_0_elem_shape1_0"), val = tensor&lt;int32, []&gt;(100)];
            list&lt;tensor&lt;fp32, [1, 100]&gt;, 50&gt; tf_make_list_0 = make_list(dtype = tf_make_list_0_dtype_0, dynamic_length = tf_make_list_0_dynamic_length_0, elem_shape = (tf_make_list_0_elem_shape0_0, tf_make_list_0_elem_shape1_0), init_length = slice_by_index_0)[name = tensor&lt;string, []&gt;("tf_make_list_0")];
            tensor&lt;int32, [50]&gt; range_1d_0 = const()[name = tensor&lt;string, []&gt;("range_1d_0"), val = tensor&lt;int32, [50]&gt;([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49])];
            tensor&lt;fp32, [1, 50, 100]&gt; cast_3 = cast(dtype = model_1_embedding_2_embedding_lookup_cast_fp16_to_fp32_dtype_0, x = model_1_embedding_2_embedding_lookup_cast_fp16)[name = tensor&lt;string, []&gt;("cast_3")];
            tensor&lt;fp32, [50, 1, 100]&gt; transpose_2 = transpose(perm = model_1_lstm_2_PartitionedCall_transpose_perm, x = cast_3)[name = tensor&lt;string, []&gt;("transpose_2")];
            list&lt;tensor&lt;fp32, [1, 100]&gt;, 50&gt; model_1_lstm_2_PartitionedCall_TensorArrayUnstack_TensorListFromTensor = list_scatter(indices = range_1d_0, ls = tf_make_list_0, value = transpose_2)[name = tensor&lt;string, []&gt;("model_1_lstm_2_PartitionedCall_TensorArrayUnstack_TensorListFromTensor")];
            tensor&lt;int32, []&gt; slice_by_index_1 = const()[name = tensor&lt;string, []&gt;("slice_by_index_1"), val = tensor&lt;int32, []&gt;(10)];
            tensor&lt;string, []&gt; tf_make_list_1_dtype_0 = const()[name = tensor&lt;string, []&gt;("tf_make_list_1_dtype_0"), val = tensor&lt;string, []&gt;("fp32")];
            tensor&lt;bool, []&gt; tf_make_list_1_dynamic_length_0 = const()[name = tensor&lt;string, []&gt;("tf_make_list_1_dynamic_length_0"), val = tensor&lt;bool, []&gt;(true)];
            tensor&lt;int32, []&gt; tf_make_list_1_elem_shape0_0 = const()[name = tensor&lt;string, []&gt;("tf_make_list_1_elem_shape0_0"), val = tensor&lt;int32, []&gt;(1)];
            tensor&lt;int32, []&gt; tf_make_list_1_elem_shape1_0 = const()[name = tensor&lt;string, []&gt;("tf_make_list_1_elem_shape1_0"), val = tensor&lt;int32, []&gt;(10)];
            list&lt;tensor&lt;fp32, [1, 10]&gt;, 10&gt; tf_make_list_1 = make_list(dtype = tf_make_list_1_dtype_0, dynamic_length = tf_make_list_1_dynamic_length_0, elem_shape = (tf_make_list_1_elem_shape0_0, tf_make_list_1_elem_shape1_0), init_length = slice_by_index_1)[name = tensor&lt;string, []&gt;("tf_make_list_1")];
            tensor&lt;int32, [10]&gt; range_1d_1 = const()[name = tensor&lt;string, []&gt;("range_1d_1"), val = tensor&lt;int32, [10]&gt;([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])];
            tensor&lt;fp32, [1, 10, 10]&gt; cast_2 = cast(dtype = model_1_embedding_3_embedding_lookup_cast_fp16_to_fp32_dtype_0, x = model_1_embedding_3_embedding_lookup_cast_fp16)[name = tensor&lt;string, []&gt;("cast_2")];
            tensor&lt;fp32, [10, 1, 10]&gt; transpose_1 = transpose(perm = model_1_lstm_3_PartitionedCall_transpose_perm, x = cast_2)[name = tensor&lt;string, []&gt;("transpose_1")];
            list&lt;tensor&lt;fp32, [1, 10]&gt;, 10&gt; model_1_lstm_3_PartitionedCall_TensorArrayUnstack_TensorListFromTensor = list_scatter(indices = range_1d_1, ls = tf_make_list_1, value = transpose_1)[name = tensor&lt;string, []&gt;("model_1_lstm_3_PartitionedCall_TensorArrayUnstack_TensorListFromTensor")];
            tensor&lt;int32, []&gt; model_1_lstm_2_PartitionedCall_strided_slice = const()[name = tensor&lt;string, []&gt;("model_1_lstm_2_PartitionedCall_strided_slice"), val = tensor&lt;int32, []&gt;(50)];
            tensor&lt;int32, []&gt; model_1_lstm_3_PartitionedCall_strided_slice = const()[name = tensor&lt;string, []&gt;("model_1_lstm_3_PartitionedCall_strided_slice"), val = tensor&lt;int32, []&gt;(10)];
            tensor&lt;fp32, [1, 100]&gt; model_1_lstm_2_zeros = const()[name = tensor&lt;string, []&gt;("model_1_lstm_2_zeros"), val = tensor&lt;fp32, [1, 100]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(2349120)))];
            tensor&lt;string, []&gt; model_1_lstm_2_PartitionedCall_TensorArrayV2_1_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_1_lstm_2_PartitionedCall_TensorArrayV2_1_dtype_0"), val = tensor&lt;string, []&gt;("fp32")];
            tensor&lt;bool, []&gt; model_1_lstm_2_PartitionedCall_TensorArrayV2_1_dynamic_length_0 = const()[name = tensor&lt;string, []&gt;("model_1_lstm_2_PartitionedCall_TensorArrayV2_1_dynamic_length_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;int32, []&gt; model_1_lstm_2_PartitionedCall_TensorArrayV2_1_elem_shape0_0 = const()[name = tensor&lt;string, []&gt;("model_1_lstm_2_PartitionedCall_TensorArrayV2_1_elem_shape0_0"), val = tensor&lt;int32, []&gt;(1)];
            tensor&lt;int32, []&gt; model_1_lstm_2_PartitionedCall_TensorArrayV2_1_elem_shape1_0 = const()[name = tensor&lt;string, []&gt;("model_1_lstm_2_PartitionedCall_TensorArrayV2_1_elem_shape1_0"), val = tensor&lt;int32, []&gt;(100)];
            list&lt;tensor&lt;fp32, [1, 100]&gt;, 50&gt; model_1_lstm_2_PartitionedCall_TensorArrayV2_1 = make_list(dtype = model_1_lstm_2_PartitionedCall_TensorArrayV2_1_dtype_0, dynamic_length = model_1_lstm_2_PartitionedCall_TensorArrayV2_1_dynamic_length_0, elem_shape = (model_1_lstm_2_PartitionedCall_TensorArrayV2_1_elem_shape0_0, model_1_lstm_2_PartitionedCall_TensorArrayV2_1_elem_shape1_0), init_length = model_1_lstm_2_PartitionedCall_strided_slice)[name = tensor&lt;string, []&gt;("model_1_lstm_2_PartitionedCall_TensorArrayV2_1")];
            tensor&lt;fp32, [1, 16]&gt; model_1_lstm_3_zeros = const()[name = tensor&lt;string, []&gt;("model_1_lstm_3_zeros"), val = tensor&lt;fp32, [1, 16]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(2349632)))];
            tensor&lt;fp32, [1, 16]&gt; model_1_lstm_3_zeros_1 = const()[name = tensor&lt;string, []&gt;("model_1_lstm_3_zeros_1"), val = tensor&lt;fp32, [1, 16]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(2349760)))];
            tensor&lt;string, []&gt; model_1_lstm_3_PartitionedCall_TensorArrayV2_1_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_1_lstm_3_PartitionedCall_TensorArrayV2_1_dtype_0"), val = tensor&lt;string, []&gt;("fp32")];
            tensor&lt;bool, []&gt; model_1_lstm_3_PartitionedCall_TensorArrayV2_1_dynamic_length_0 = const()[name = tensor&lt;string, []&gt;("model_1_lstm_3_PartitionedCall_TensorArrayV2_1_dynamic_length_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;int32, []&gt; model_1_lstm_3_PartitionedCall_TensorArrayV2_1_elem_shape0_0 = const()[name = tensor&lt;string, []&gt;("model_1_lstm_3_PartitionedCall_TensorArrayV2_1_elem_shape0_0"), val = tensor&lt;int32, []&gt;(1)];
            tensor&lt;int32, []&gt; model_1_lstm_3_PartitionedCall_TensorArrayV2_1_elem_shape1_0 = const()[name = tensor&lt;string, []&gt;("model_1_lstm_3_PartitionedCall_TensorArrayV2_1_elem_shape1_0"), val = tensor&lt;int32, []&gt;(16)];
            list&lt;tensor&lt;fp32, [1, 16]&gt;, 10&gt; model_1_lstm_3_PartitionedCall_TensorArrayV2_1 = make_list(dtype = model_1_lstm_3_PartitionedCall_TensorArrayV2_1_dtype_0, dynamic_length = model_1_lstm_3_PartitionedCall_TensorArrayV2_1_dynamic_length_0, elem_shape = (model_1_lstm_3_PartitionedCall_TensorArrayV2_1_elem_shape0_0, model_1_lstm_3_PartitionedCall_TensorArrayV2_1_elem_shape1_0), init_length = model_1_lstm_3_PartitionedCall_strided_slice)[name = tensor&lt;string, []&gt;("model_1_lstm_3_PartitionedCall_TensorArrayV2_1")];
            tensor&lt;int32, []&gt; model_1_lstm_2_PartitionedCall_while_0, list&lt;tensor&lt;fp32, [1, 100]&gt;, 50&gt; model_1_lstm_2_PartitionedCall_while_1, tensor&lt;fp32, [1, 100]&gt; model_1_lstm_2_PartitionedCall_while_2, tensor&lt;fp32, [1, 100]&gt; model_1_lstm_2_PartitionedCall_while_3 = while_loop(loop_vars = (model_1_lstm_2_PartitionedCall_time, model_1_lstm_2_PartitionedCall_TensorArrayV2_1, model_1_lstm_2_zeros, model_1_lstm_2_zeros))[name = tensor&lt;string, []&gt;("model_1_lstm_2_PartitionedCall_while_renamed")]
                (tensor&lt;int32, []&gt; model_1_lstm_2_PartitionedCall_time_x0_1_1_0, list&lt;tensor&lt;fp32, [1, 100]&gt;, 50&gt; model_1_lstm_2_PartitionedCall_TensorArrayV2_1_x0, tensor&lt;fp32, [1, 100]&gt; model_1_lstm_2_zeros_x0_1_1_0, tensor&lt;fp32, [1, 100]&gt; model_1_lstm_2_zeros_1_x0_1_1_0) {
                    tensor&lt;bool, []&gt; model_1_lstm_2_PartitionedCall_while_while_cond_48809139_while_Less = less(x = model_1_lstm_2_PartitionedCall_time_x0_1_1_0, y = model_1_lstm_2_PartitionedCall_strided_slice)[name = tensor&lt;string, []&gt;("model_1_lstm_2_PartitionedCall_while_while_cond_48809139_while_Less")];
                } -&gt; (model_1_lstm_2_PartitionedCall_while_while_cond_48809139_while_Less)
                (tensor&lt;int32, []&gt; model_1_lstm_2_PartitionedCall_time_x0_1_1_1, list&lt;tensor&lt;fp32, [1, 100]&gt;, 50&gt; model_1_lstm_2_PartitionedCall_TensorArrayV2_1_x0_1, tensor&lt;fp32, [1, 100]&gt; model_1_lstm_2_zeros_x0_1_1_1, tensor&lt;fp32, [1, 100]&gt; model_1_lstm_2_zeros_1_x0_1_1_1) {
                    tensor&lt;int32, []&gt; model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_split_split_dim = const()[name = tensor&lt;string, []&gt;("model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_split_split_dim"), val = tensor&lt;int32, []&gt;(1)];
                    tensor&lt;int32, []&gt; model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_add_2_y = const()[name = tensor&lt;string, []&gt;("model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_add_2_y"), val = tensor&lt;int32, []&gt;(1)];
                    tensor&lt;int32, []&gt; model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_add_2 = add(x = model_1_lstm_2_PartitionedCall_time_x0_1_1_1, y = model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_add_2_y)[name = tensor&lt;string, []&gt;("model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_add_2")];
                    tensor&lt;fp32, [1, 100]&gt; model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_TensorArrayV2Read_TensorListGetItem = list_read(index = model_1_lstm_2_PartitionedCall_time_x0_1_1_1, ls = model_1_lstm_2_PartitionedCall_TensorArrayUnstack_TensorListFromTensor)[name = tensor&lt;string, []&gt;("model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_TensorArrayV2Read_TensorListGetItem")];
                    tensor&lt;bool, []&gt; model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_MatMul_1_transpose_x_1 = const()[name = tensor&lt;string, []&gt;("model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_MatMul_1_transpose_x_1"), val = tensor&lt;bool, []&gt;(false)];
                    tensor&lt;bool, []&gt; model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_MatMul_1_transpose_y_1 = const()[name = tensor&lt;string, []&gt;("model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_MatMul_1_transpose_y_1"), val = tensor&lt;bool, []&gt;(false)];
                    tensor&lt;fp32, [1, 400]&gt; model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_MatMul_1 = matmul(transpose_x = model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_MatMul_1_transpose_x_1, transpose_y = model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_MatMul_1_transpose_y_1, x = model_1_lstm_2_zeros_x0_1_1_1, y = Func_model_1_lstm_2_PartitionedCall_input__15)[name = tensor&lt;string, []&gt;("model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_MatMul_1")];
                    tensor&lt;bool, []&gt; model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_MatMul_transpose_x_1 = const()[name = tensor&lt;string, []&gt;("model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_MatMul_transpose_x_1"), val = tensor&lt;bool, []&gt;(false)];
                    tensor&lt;bool, []&gt; model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_MatMul_transpose_y_1 = const()[name = tensor&lt;string, []&gt;("model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_MatMul_transpose_y_1"), val = tensor&lt;bool, []&gt;(false)];
                    tensor&lt;fp32, [1, 400]&gt; model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_MatMul = matmul(transpose_x = model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_MatMul_transpose_x_1, transpose_y = model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_MatMul_transpose_y_1, x = model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_TensorArrayV2Read_TensorListGetItem, y = Func_model_1_lstm_2_PartitionedCall_input__14)[name = tensor&lt;string, []&gt;("model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_MatMul")];
                    tensor&lt;fp32, [1, 400]&gt; model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_add = add(x = model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_MatMul, y = model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_MatMul_1)[name = tensor&lt;string, []&gt;("model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_add")];
                    tensor&lt;fp32, [1, 400]&gt; model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_BiasAdd = add(x = model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_add, y = Func_model_1_lstm_2_PartitionedCall_input__16)[name = tensor&lt;string, []&gt;("model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_BiasAdd")];
                    tensor&lt;int32, []&gt; model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_split_num_splits_1 = const()[name = tensor&lt;string, []&gt;("model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_split_num_splits_1"), val = tensor&lt;int32, []&gt;(4)];
                    tensor&lt;fp32, [1, 100]&gt; model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_split_0, tensor&lt;fp32, [1, 100]&gt; model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_split_1, tensor&lt;fp32, [1, 100]&gt; model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_split_2, tensor&lt;fp32, [1, 100]&gt; model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_split_3 = split(axis = model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_split_split_dim, num_splits = model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_split_num_splits_1, x = model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_BiasAdd)[name = tensor&lt;string, []&gt;("model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_split")];
                    tensor&lt;fp32, [1, 100]&gt; model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_Sigmoid = sigmoid(x = model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_split_0)[name = tensor&lt;string, []&gt;("model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_Sigmoid")];
                    tensor&lt;fp32, [1, 100]&gt; model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_Sigmoid_1 = sigmoid(x = model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_split_1)[name = tensor&lt;string, []&gt;("model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_Sigmoid_1")];
                    tensor&lt;fp32, [1, 100]&gt; model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_Tanh = tanh(x = model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_split_2)[name = tensor&lt;string, []&gt;("model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_Tanh")];
                    tensor&lt;fp32, [1, 100]&gt; model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_Sigmoid_2 = sigmoid(x = model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_split_3)[name = tensor&lt;string, []&gt;("model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_Sigmoid_2")];
                    tensor&lt;fp32, [1, 100]&gt; model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_mul = mul(x = model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_Sigmoid_1, y = model_1_lstm_2_zeros_1_x0_1_1_1)[name = tensor&lt;string, []&gt;("model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_mul")];
                    tensor&lt;fp32, [1, 100]&gt; model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_mul_1 = mul(x = model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_Sigmoid, y = model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_Tanh)[name = tensor&lt;string, []&gt;("model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_mul_1")];
                    tensor&lt;fp32, [1, 100]&gt; model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_add_1 = add(x = model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_mul, y = model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_mul_1)[name = tensor&lt;string, []&gt;("model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_add_1")];
                    tensor&lt;fp32, [1, 100]&gt; model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_Tanh_1 = tanh(x = model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_add_1)[name = tensor&lt;string, []&gt;("model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_Tanh_1")];
                    tensor&lt;fp32, [1, 100]&gt; model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_mul_2 = mul(x = model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_Sigmoid_2, y = model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_Tanh_1)[name = tensor&lt;string, []&gt;("model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_mul_2")];
                    list&lt;tensor&lt;fp32, [1, 100]&gt;, 50&gt; model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_TensorArrayV2Write_TensorListSetItem = list_write(index = model_1_lstm_2_PartitionedCall_time_x0_1_1_1, ls = model_1_lstm_2_PartitionedCall_TensorArrayV2_1_x0_1, value = model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_mul_2)[name = tensor&lt;string, []&gt;("model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_TensorArrayV2Write_TensorListSetItem")];
                } -&gt; (model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_add_2, model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_TensorArrayV2Write_TensorListSetItem, model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_mul_2, model_1_lstm_2_PartitionedCall_while_while_body_48809140_while_add_1);
            tensor&lt;int32, []&gt; model_1_lstm_3_PartitionedCall_while_0, list&lt;tensor&lt;fp32, [1, 16]&gt;, 10&gt; model_1_lstm_3_PartitionedCall_while_1, tensor&lt;fp32, [1, 16]&gt; model_1_lstm_3_PartitionedCall_while_2, tensor&lt;fp32, [1, 16]&gt; model_1_lstm_3_PartitionedCall_while_3 = while_loop(loop_vars = (model_1_lstm_3_PartitionedCall_time, model_1_lstm_3_PartitionedCall_TensorArrayV2_1, model_1_lstm_3_zeros, model_1_lstm_3_zeros_1))[name = tensor&lt;string, []&gt;("model_1_lstm_3_PartitionedCall_while_renamed")]
                (tensor&lt;int32, []&gt; model_1_lstm_3_PartitionedCall_time_x0_1_1_0, list&lt;tensor&lt;fp32, [1, 16]&gt;, 10&gt; model_1_lstm_3_PartitionedCall_TensorArrayV2_1_x0, tensor&lt;fp32, [1, 16]&gt; model_1_lstm_3_zeros_x0_1_1_0, tensor&lt;fp32, [1, 16]&gt; model_1_lstm_3_zeros_1_x0_1_1_0) {
                    tensor&lt;bool, []&gt; model_1_lstm_3_PartitionedCall_while_while_cond_48808716_while_Less = less(x = model_1_lstm_3_PartitionedCall_time_x0_1_1_0, y = model_1_lstm_3_PartitionedCall_strided_slice)[name = tensor&lt;string, []&gt;("model_1_lstm_3_PartitionedCall_while_while_cond_48808716_while_Less")];
                } -&gt; (model_1_lstm_3_PartitionedCall_while_while_cond_48808716_while_Less)
                (tensor&lt;int32, []&gt; model_1_lstm_3_PartitionedCall_time_x0_1_1_1, list&lt;tensor&lt;fp32, [1, 16]&gt;, 10&gt; model_1_lstm_3_PartitionedCall_TensorArrayV2_1_x0_1, tensor&lt;fp32, [1, 16]&gt; model_1_lstm_3_zeros_x0_1_1_1, tensor&lt;fp32, [1, 16]&gt; model_1_lstm_3_zeros_1_x0_1_1_1) {
                    tensor&lt;int32, []&gt; model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_split_split_dim = const()[name = tensor&lt;string, []&gt;("model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_split_split_dim"), val = tensor&lt;int32, []&gt;(1)];
                    tensor&lt;int32, []&gt; model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_add_2_y = const()[name = tensor&lt;string, []&gt;("model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_add_2_y"), val = tensor&lt;int32, []&gt;(1)];
                    tensor&lt;int32, []&gt; model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_add_2 = add(x = model_1_lstm_3_PartitionedCall_time_x0_1_1_1, y = model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_add_2_y)[name = tensor&lt;string, []&gt;("model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_add_2")];
                    tensor&lt;fp32, [1, 10]&gt; model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_TensorArrayV2Read_TensorListGetItem = list_read(index = model_1_lstm_3_PartitionedCall_time_x0_1_1_1, ls = model_1_lstm_3_PartitionedCall_TensorArrayUnstack_TensorListFromTensor)[name = tensor&lt;string, []&gt;("model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_TensorArrayV2Read_TensorListGetItem")];
                    tensor&lt;bool, []&gt; model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_MatMul_1_transpose_x_1 = const()[name = tensor&lt;string, []&gt;("model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_MatMul_1_transpose_x_1"), val = tensor&lt;bool, []&gt;(false)];
                    tensor&lt;bool, []&gt; model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_MatMul_1_transpose_y_1 = const()[name = tensor&lt;string, []&gt;("model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_MatMul_1_transpose_y_1"), val = tensor&lt;bool, []&gt;(false)];
                    tensor&lt;fp32, [1, 64]&gt; model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_MatMul_1 = matmul(transpose_x = model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_MatMul_1_transpose_x_1, transpose_y = model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_MatMul_1_transpose_y_1, x = model_1_lstm_3_zeros_x0_1_1_1, y = Func_model_1_lstm_3_PartitionedCall_input__4)[name = tensor&lt;string, []&gt;("model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_MatMul_1")];
                    tensor&lt;bool, []&gt; model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_MatMul_transpose_x_1 = const()[name = tensor&lt;string, []&gt;("model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_MatMul_transpose_x_1"), val = tensor&lt;bool, []&gt;(false)];
                    tensor&lt;bool, []&gt; model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_MatMul_transpose_y_1 = const()[name = tensor&lt;string, []&gt;("model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_MatMul_transpose_y_1"), val = tensor&lt;bool, []&gt;(false)];
                    tensor&lt;fp32, [1, 64]&gt; model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_MatMul = matmul(transpose_x = model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_MatMul_transpose_x_1, transpose_y = model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_MatMul_transpose_y_1, x = model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_TensorArrayV2Read_TensorListGetItem, y = Func_model_1_lstm_3_PartitionedCall_input__3)[name = tensor&lt;string, []&gt;("model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_MatMul")];
                    tensor&lt;fp32, [1, 64]&gt; model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_add = add(x = model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_MatMul, y = model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_MatMul_1)[name = tensor&lt;string, []&gt;("model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_add")];
                    tensor&lt;fp32, [1, 64]&gt; model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_BiasAdd = add(x = model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_add, y = Func_model_1_lstm_3_PartitionedCall_input__5)[name = tensor&lt;string, []&gt;("model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_BiasAdd")];
                    tensor&lt;int32, []&gt; model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_split_num_splits_1 = const()[name = tensor&lt;string, []&gt;("model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_split_num_splits_1"), val = tensor&lt;int32, []&gt;(4)];
                    tensor&lt;fp32, [1, 16]&gt; model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_split_0, tensor&lt;fp32, [1, 16]&gt; model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_split_1, tensor&lt;fp32, [1, 16]&gt; model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_split_2, tensor&lt;fp32, [1, 16]&gt; model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_split_3 = split(axis = model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_split_split_dim, num_splits = model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_split_num_splits_1, x = model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_BiasAdd)[name = tensor&lt;string, []&gt;("model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_split")];
                    tensor&lt;fp32, [1, 16]&gt; model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_Sigmoid = sigmoid(x = model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_split_0)[name = tensor&lt;string, []&gt;("model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_Sigmoid")];
                    tensor&lt;fp32, [1, 16]&gt; model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_Sigmoid_1 = sigmoid(x = model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_split_1)[name = tensor&lt;string, []&gt;("model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_Sigmoid_1")];
                    tensor&lt;fp32, [1, 16]&gt; model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_Tanh = tanh(x = model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_split_2)[name = tensor&lt;string, []&gt;("model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_Tanh")];
                    tensor&lt;fp32, [1, 16]&gt; model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_Sigmoid_2 = sigmoid(x = model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_split_3)[name = tensor&lt;string, []&gt;("model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_Sigmoid_2")];
                    tensor&lt;fp32, [1, 16]&gt; model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_mul = mul(x = model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_Sigmoid_1, y = model_1_lstm_3_zeros_1_x0_1_1_1)[name = tensor&lt;string, []&gt;("model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_mul")];
                    tensor&lt;fp32, [1, 16]&gt; model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_mul_1 = mul(x = model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_Sigmoid, y = model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_Tanh)[name = tensor&lt;string, []&gt;("model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_mul_1")];
                    tensor&lt;fp32, [1, 16]&gt; model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_add_1 = add(x = model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_mul, y = model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_mul_1)[name = tensor&lt;string, []&gt;("model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_add_1")];
                    tensor&lt;fp32, [1, 16]&gt; model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_Tanh_1 = tanh(x = model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_add_1)[name = tensor&lt;string, []&gt;("model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_Tanh_1")];
                    tensor&lt;fp32, [1, 16]&gt; model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_mul_2 = mul(x = model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_Sigmoid_2, y = model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_Tanh_1)[name = tensor&lt;string, []&gt;("model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_mul_2")];
                    list&lt;tensor&lt;fp32, [1, 16]&gt;, 10&gt; model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_TensorArrayV2Write_TensorListSetItem = list_write(index = model_1_lstm_3_PartitionedCall_time_x0_1_1_1, ls = model_1_lstm_3_PartitionedCall_TensorArrayV2_1_x0_1, value = model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_mul_2)[name = tensor&lt;string, []&gt;("model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_TensorArrayV2Write_TensorListSetItem")];
                } -&gt; (model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_add_2, model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_TensorArrayV2Write_TensorListSetItem, model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_mul_2, model_1_lstm_3_PartitionedCall_while_while_body_48808717_while_add_1);
            tensor&lt;int32, [50]&gt; range_1d_2 = const()[name = tensor&lt;string, []&gt;("range_1d_2"), val = tensor&lt;int32, [50]&gt;([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49])];
            tensor&lt;fp32, [50, 1, 100]&gt; model_1_lstm_2_PartitionedCall_TensorArrayV2Stack_TensorListStack = list_gather(indices = range_1d_2, ls = model_1_lstm_2_PartitionedCall_while_1)[name = tensor&lt;string, []&gt;("model_1_lstm_2_PartitionedCall_TensorArrayV2Stack_TensorListStack")];
            tensor&lt;int32, [10]&gt; range_1d_3 = const()[name = tensor&lt;string, []&gt;("range_1d_3"), val = tensor&lt;int32, [10]&gt;([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])];
            tensor&lt;fp32, [10, 1, 16]&gt; model_1_lstm_3_PartitionedCall_TensorArrayV2Stack_TensorListStack = list_gather(indices = range_1d_3, ls = model_1_lstm_3_PartitionedCall_while_1)[name = tensor&lt;string, []&gt;("model_1_lstm_3_PartitionedCall_TensorArrayV2Stack_TensorListStack")];
            tensor&lt;int32, [3]&gt; model_1_lstm_2_PartitionedCall_strided_slice_2_begin_0 = const()[name = tensor&lt;string, []&gt;("model_1_lstm_2_PartitionedCall_strided_slice_2_begin_0"), val = tensor&lt;int32, [3]&gt;([-1, 0, 0])];
            tensor&lt;int32, [3]&gt; model_1_lstm_2_PartitionedCall_strided_slice_2_end_0 = const()[name = tensor&lt;string, []&gt;("model_1_lstm_2_PartitionedCall_strided_slice_2_end_0"), val = tensor&lt;int32, [3]&gt;([0, 0, 0])];
            tensor&lt;int32, [3]&gt; model_1_lstm_2_PartitionedCall_strided_slice_2_stride_0 = const()[name = tensor&lt;string, []&gt;("model_1_lstm_2_PartitionedCall_strided_slice_2_stride_0"), val = tensor&lt;int32, [3]&gt;([1, 1, 1])];
            tensor&lt;bool, [3]&gt; model_1_lstm_2_PartitionedCall_strided_slice_2_begin_mask_0 = const()[name = tensor&lt;string, []&gt;("model_1_lstm_2_PartitionedCall_strided_slice_2_begin_mask_0"), val = tensor&lt;bool, [3]&gt;([false, true, true])];
            tensor&lt;bool, [3]&gt; model_1_lstm_2_PartitionedCall_strided_slice_2_end_mask_0 = const()[name = tensor&lt;string, []&gt;("model_1_lstm_2_PartitionedCall_strided_slice_2_end_mask_0"), val = tensor&lt;bool, [3]&gt;([false, true, true])];
            tensor&lt;bool, [3]&gt; model_1_lstm_2_PartitionedCall_strided_slice_2_squeeze_mask_0 = const()[name = tensor&lt;string, []&gt;("model_1_lstm_2_PartitionedCall_strided_slice_2_squeeze_mask_0"), val = tensor&lt;bool, [3]&gt;([true, false, false])];
            tensor&lt;fp32, [1, 100]&gt; model_1_lstm_2_PartitionedCall_strided_slice_2 = slice_by_index(begin = model_1_lstm_2_PartitionedCall_strided_slice_2_begin_0, begin_mask = model_1_lstm_2_PartitionedCall_strided_slice_2_begin_mask_0, end = model_1_lstm_2_PartitionedCall_strided_slice_2_end_0, end_mask = model_1_lstm_2_PartitionedCall_strided_slice_2_end_mask_0, squeeze_mask = model_1_lstm_2_PartitionedCall_strided_slice_2_squeeze_mask_0, stride = model_1_lstm_2_PartitionedCall_strided_slice_2_stride_0, x = model_1_lstm_2_PartitionedCall_TensorArrayV2Stack_TensorListStack)[name = tensor&lt;string, []&gt;("model_1_lstm_2_PartitionedCall_strided_slice_2")];
            tensor&lt;int32, [3]&gt; model_1_lstm_3_PartitionedCall_strided_slice_2_begin_0 = const()[name = tensor&lt;string, []&gt;("model_1_lstm_3_PartitionedCall_strided_slice_2_begin_0"), val = tensor&lt;int32, [3]&gt;([-1, 0, 0])];
            tensor&lt;int32, [3]&gt; model_1_lstm_3_PartitionedCall_strided_slice_2_end_0 = const()[name = tensor&lt;string, []&gt;("model_1_lstm_3_PartitionedCall_strided_slice_2_end_0"), val = tensor&lt;int32, [3]&gt;([0, 0, 0])];
            tensor&lt;int32, [3]&gt; model_1_lstm_3_PartitionedCall_strided_slice_2_stride_0 = const()[name = tensor&lt;string, []&gt;("model_1_lstm_3_PartitionedCall_strided_slice_2_stride_0"), val = tensor&lt;int32, [3]&gt;([1, 1, 1])];
            tensor&lt;bool, [3]&gt; model_1_lstm_3_PartitionedCall_strided_slice_2_begin_mask_0 = const()[name = tensor&lt;string, []&gt;("model_1_lstm_3_PartitionedCall_strided_slice_2_begin_mask_0"), val = tensor&lt;bool, [3]&gt;([false, true, true])];
            tensor&lt;bool, [3]&gt; model_1_lstm_3_PartitionedCall_strided_slice_2_end_mask_0 = const()[name = tensor&lt;string, []&gt;("model_1_lstm_3_PartitionedCall_strided_slice_2_end_mask_0"), val = tensor&lt;bool, [3]&gt;([false, true, true])];
            tensor&lt;bool, [3]&gt; model_1_lstm_3_PartitionedCall_strided_slice_2_squeeze_mask_0 = const()[name = tensor&lt;string, []&gt;("model_1_lstm_3_PartitionedCall_strided_slice_2_squeeze_mask_0"), val = tensor&lt;bool, [3]&gt;([true, false, false])];
            tensor&lt;fp32, [1, 16]&gt; model_1_lstm_3_PartitionedCall_strided_slice_2 = slice_by_index(begin = model_1_lstm_3_PartitionedCall_strided_slice_2_begin_0, begin_mask = model_1_lstm_3_PartitionedCall_strided_slice_2_begin_mask_0, end = model_1_lstm_3_PartitionedCall_strided_slice_2_end_0, end_mask = model_1_lstm_3_PartitionedCall_strided_slice_2_end_mask_0, squeeze_mask = model_1_lstm_3_PartitionedCall_strided_slice_2_squeeze_mask_0, stride = model_1_lstm_3_PartitionedCall_strided_slice_2_stride_0, x = model_1_lstm_3_PartitionedCall_TensorArrayV2Stack_TensorListStack)[name = tensor&lt;string, []&gt;("model_1_lstm_3_PartitionedCall_strided_slice_2")];
            tensor&lt;bool, []&gt; model_1_concatenate_1_concat_interleave_0 = const()[name = tensor&lt;string, []&gt;("model_1_concatenate_1_concat_interleave_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp32, [1, 116]&gt; model_1_concatenate_1_concat = concat(axis = model_1_concatenate_1_concat_axis, interleave = model_1_concatenate_1_concat_interleave_0, values = (model_1_lstm_2_PartitionedCall_strided_slice_2, model_1_lstm_3_PartitionedCall_strided_slice_2))[name = tensor&lt;string, []&gt;("model_1_concatenate_1_concat")];
            tensor&lt;string, []&gt; model_1_concatenate_1_concat_to_fp16_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_1_concatenate_1_concat_to_fp16_dtype_0"), val = tensor&lt;string, []&gt;("fp16")];
            tensor&lt;fp16, [7, 116]&gt; transpose_0_to_fp16 = const()[name = tensor&lt;string, []&gt;("transpose_0_to_fp16"), val = tensor&lt;fp16, [7, 116]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(2349888)))];
            tensor&lt;fp16, [7]&gt; model_1_dense_1_BiasAdd_bias_0_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_dense_1_BiasAdd_bias_0_to_fp16"), val = tensor&lt;fp16, [7]&gt;([-0x1.804p+1, -0x1.ed8p-2, 0x1.794p-4, -0x1.1fp-1, 0x1.388p-1, 0x1.31p+1, -0x1.facp+0])];
            tensor&lt;fp16, [1, 116]&gt; cast_1 = cast(dtype = model_1_concatenate_1_concat_to_fp16_dtype_0, x = model_1_concatenate_1_concat)[name = tensor&lt;string, []&gt;("cast_1")];
            tensor&lt;fp16, [1, 7]&gt; model_1_dense_1_BiasAdd_cast_fp16 = linear(bias = model_1_dense_1_BiasAdd_bias_0_to_fp16, weight = transpose_0_to_fp16, x = cast_1)[name = tensor&lt;string, []&gt;("model_1_dense_1_BiasAdd_cast_fp16")];
            tensor&lt;string, []&gt; model_1_dense_1_BiasAdd_cast_fp16_to_fp32_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_1_dense_1_BiasAdd_cast_fp16_to_fp32_dtype_0"), val = tensor&lt;string, []&gt;("fp32")];
            tensor&lt;int32, []&gt; model_1_dense_1_Softmax_axis_0 = const()[name = tensor&lt;string, []&gt;("model_1_dense_1_Softmax_axis_0"), val = tensor&lt;int32, []&gt;(-1)];
            tensor&lt;fp32, [1, 7]&gt; cast_0 = cast(dtype = model_1_dense_1_BiasAdd_cast_fp16_to_fp32_dtype_0, x = model_1_dense_1_BiasAdd_cast_fp16)[name = tensor&lt;string, []&gt;("cast_0")];
            tensor&lt;fp32, [1, 7]&gt; Identity = softmax(axis = model_1_dense_1_Softmax_axis_0, x = cast_0)[name = tensor&lt;string, []&gt;("model_1_dense_1_Softmax")];
        } -&gt; (Identity);
}</code></pre>
                    </div>
                </div>
            
                <div class="file-section" id="file-5">
                    <div class="file-header">
                        <h2>ðŸ“„ model.mil 20</h2>
                        <div class="file-meta">Size: 10.79 KB</div>
                    </div>
                    <div class="file-content">
                        <pre><code>program(1.0)
[buildInfo = dict&lt;tensor&lt;string, []&gt;, tensor&lt;string, []&gt;&gt;({{"coremlc-component-MIL", "3405.2.1"}, {"coremlc-version", "3405.2.1"}, {"coremltools-component-tensorflow", "2.13.0"}, {"coremltools-version", "7.1"}, {"mldb_token", "mldb-wzxy8wby59"}})]
{
    func main&lt;ios15&gt;(tensor&lt;fp32, [1, 2002]&gt; x) {
            tensor&lt;int32, [3]&gt; model_tf_split_Const = const()[name = tensor&lt;string, []&gt;("model_tf_split_Const"), val = tensor&lt;int32, [3]&gt;([2000, 1, 1])];
            tensor&lt;int32, []&gt; model_tf_split_split_split_dim = const()[name = tensor&lt;string, []&gt;("model_tf_split_split_split_dim"), val = tensor&lt;int32, []&gt;(1)];
            tensor&lt;int32, [2]&gt; model_flatten_Const = const()[name = tensor&lt;string, []&gt;("model_flatten_Const"), val = tensor&lt;int32, [2]&gt;([-1, 128])];
            tensor&lt;int32, []&gt; model_concatenate_concat_axis = const()[name = tensor&lt;string, []&gt;("model_concatenate_concat_axis"), val = tensor&lt;int32, []&gt;(1)];
            tensor&lt;fp32, [257]&gt; model_batch_normalization_batchnorm_mul = const()[name = tensor&lt;string, []&gt;("model_batch_normalization_batchnorm_mul"), val = tensor&lt;fp32, [257]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(64)))];
            tensor&lt;fp32, [257]&gt; model_batch_normalization_batchnorm_sub = const()[name = tensor&lt;string, []&gt;("model_batch_normalization_batchnorm_sub"), val = tensor&lt;fp32, [257]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(1216)))];
            tensor&lt;int32, []&gt; model_tf_split_split_num_splits_0 = const()[name = tensor&lt;string, []&gt;("model_tf_split_split_num_splits_0"), val = tensor&lt;int32, []&gt;(3)];
            tensor&lt;fp32, [1, 2000]&gt; model_tf_split_split_0, tensor&lt;fp32, [1, 1]&gt; model_tf_split_split_1, tensor&lt;fp32, [1, 1]&gt; model_tf_split_split_2 = split(axis = model_tf_split_split_split_dim, num_splits = model_tf_split_split_num_splits_0, split_sizes = model_tf_split_Const, x = x)[name = tensor&lt;string, []&gt;("model_tf_split_split")];
            tensor&lt;string, []&gt; model_tf_split_split_0_to_fp16_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_tf_split_split_0_to_fp16_dtype_0"), val = tensor&lt;string, []&gt;("fp16")];
            tensor&lt;fp16, [128, 2000]&gt; transpose_0_to_fp16 = const()[name = tensor&lt;string, []&gt;("transpose_0_to_fp16"), val = tensor&lt;fp16, [128, 2000]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(2368)))];
            tensor&lt;fp16, [128]&gt; model_dense_BiasAdd_bias_0_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_dense_BiasAdd_bias_0_to_fp16"), val = tensor&lt;fp16, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(514432)))];
            tensor&lt;fp16, [1, 2000]&gt; cast_9 = cast(dtype = model_tf_split_split_0_to_fp16_dtype_0, x = model_tf_split_split_0)[name = tensor&lt;string, []&gt;("cast_9")];
            tensor&lt;fp16, [1, 128]&gt; model_dense_BiasAdd_cast_fp16 = linear(bias = model_dense_BiasAdd_bias_0_to_fp16, weight = transpose_0_to_fp16, x = cast_9)[name = tensor&lt;string, []&gt;("model_dense_BiasAdd_cast_fp16")];
            tensor&lt;string, []&gt; model_dense_BiasAdd_cast_fp16_to_fp32_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_dense_BiasAdd_cast_fp16_to_fp32_dtype_0"), val = tensor&lt;string, []&gt;("fp32")];
            tensor&lt;string, []&gt; model_embedding_Cast_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_embedding_Cast_dtype_0"), val = tensor&lt;string, []&gt;("int32")];
            tensor&lt;int32, []&gt; model_embedding_embedding_lookup_axis_0 = const()[name = tensor&lt;string, []&gt;("model_embedding_embedding_lookup_axis_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;fp16, [621, 128]&gt; model_embedding_embedding_lookup_81025_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_embedding_embedding_lookup_81025_to_fp16"), val = tensor&lt;fp16, [621, 128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(514752)))];
            tensor&lt;int32, [1, 1]&gt; cast_7 = cast(dtype = model_embedding_Cast_dtype_0, x = model_tf_split_split_2)[name = tensor&lt;string, []&gt;("cast_7")];
            tensor&lt;fp16, [1, 1, 128]&gt; model_embedding_embedding_lookup_cast_fp16 = gather(axis = model_embedding_embedding_lookup_axis_0, indices = cast_7, x = model_embedding_embedding_lookup_81025_to_fp16)[name = tensor&lt;string, []&gt;("model_embedding_embedding_lookup_cast_fp16")];
            tensor&lt;string, []&gt; model_embedding_embedding_lookup_cast_fp16_to_fp32_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_embedding_embedding_lookup_cast_fp16_to_fp32_dtype_0"), val = tensor&lt;string, []&gt;("fp32")];
            tensor&lt;fp32, [1, 128]&gt; cast_8 = cast(dtype = model_dense_BiasAdd_cast_fp16_to_fp32_dtype_0, x = model_dense_BiasAdd_cast_fp16)[name = tensor&lt;string, []&gt;("cast_8")];
            tensor&lt;fp32, [1, 128]&gt; model_dense_Relu = relu(x = cast_8)[name = tensor&lt;string, []&gt;("model_dense_Relu")];
            tensor&lt;fp32, [1, 1, 128]&gt; cast_6 = cast(dtype = model_embedding_embedding_lookup_cast_fp16_to_fp32_dtype_0, x = model_embedding_embedding_lookup_cast_fp16)[name = tensor&lt;string, []&gt;("cast_6")];
            tensor&lt;fp32, [1, 128]&gt; model_flatten_Reshape = reshape(shape = model_flatten_Const, x = cast_6)[name = tensor&lt;string, []&gt;("model_flatten_Reshape")];
            tensor&lt;bool, []&gt; model_concatenate_concat_interleave_0 = const()[name = tensor&lt;string, []&gt;("model_concatenate_concat_interleave_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp32, [1, 257]&gt; model_concatenate_concat = concat(axis = model_concatenate_concat_axis, interleave = model_concatenate_concat_interleave_0, values = (model_dense_Relu, model_tf_split_split_1, model_flatten_Reshape))[name = tensor&lt;string, []&gt;("model_concatenate_concat")];
            tensor&lt;fp32, [1, 257]&gt; model_batch_normalization_batchnorm_mul_1 = mul(x = model_concatenate_concat, y = model_batch_normalization_batchnorm_mul)[name = tensor&lt;string, []&gt;("model_batch_normalization_batchnorm_mul_1")];
            tensor&lt;fp32, [1, 257]&gt; model_batch_normalization_batchnorm_add_1 = add(x = model_batch_normalization_batchnorm_mul_1, y = model_batch_normalization_batchnorm_sub)[name = tensor&lt;string, []&gt;("model_batch_normalization_batchnorm_add_1")];
            tensor&lt;string, []&gt; model_batch_normalization_batchnorm_add_1_to_fp16_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_batch_normalization_batchnorm_add_1_to_fp16_dtype_0"), val = tensor&lt;string, []&gt;("fp16")];
            tensor&lt;fp16, [128, 257]&gt; transpose_1_to_fp16 = const()[name = tensor&lt;string, []&gt;("transpose_1_to_fp16"), val = tensor&lt;fp16, [128, 257]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(673792)))];
            tensor&lt;fp16, [128]&gt; model_dense_1_BiasAdd_bias_0_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_dense_1_BiasAdd_bias_0_to_fp16"), val = tensor&lt;fp16, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(739648)))];
            tensor&lt;fp16, [1, 257]&gt; cast_5 = cast(dtype = model_batch_normalization_batchnorm_add_1_to_fp16_dtype_0, x = model_batch_normalization_batchnorm_add_1)[name = tensor&lt;string, []&gt;("cast_5")];
            tensor&lt;fp16, [1, 128]&gt; model_dense_1_BiasAdd_cast_fp16 = linear(bias = model_dense_1_BiasAdd_bias_0_to_fp16, weight = transpose_1_to_fp16, x = cast_5)[name = tensor&lt;string, []&gt;("model_dense_1_BiasAdd_cast_fp16")];
            tensor&lt;string, []&gt; model_dense_1_BiasAdd_cast_fp16_to_fp32_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_dense_1_BiasAdd_cast_fp16_to_fp32_dtype_0"), val = tensor&lt;string, []&gt;("fp32")];
            tensor&lt;fp32, [1, 128]&gt; cast_4 = cast(dtype = model_dense_1_BiasAdd_cast_fp16_to_fp32_dtype_0, x = model_dense_1_BiasAdd_cast_fp16)[name = tensor&lt;string, []&gt;("cast_4")];
            tensor&lt;fp32, [1, 128]&gt; model_dense_1_Relu = relu(x = cast_4)[name = tensor&lt;string, []&gt;("model_dense_1_Relu")];
            tensor&lt;string, []&gt; model_dense_1_Relu_to_fp16_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_dense_1_Relu_to_fp16_dtype_0"), val = tensor&lt;string, []&gt;("fp16")];
            tensor&lt;fp16, [64, 128]&gt; transpose_2_to_fp16 = const()[name = tensor&lt;string, []&gt;("transpose_2_to_fp16"), val = tensor&lt;fp16, [64, 128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(739968)))];
            tensor&lt;fp16, [64]&gt; model_dense_2_BiasAdd_bias_0_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_dense_2_BiasAdd_bias_0_to_fp16"), val = tensor&lt;fp16, [64]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(756416)))];
            tensor&lt;fp16, [1, 128]&gt; cast_3 = cast(dtype = model_dense_1_Relu_to_fp16_dtype_0, x = model_dense_1_Relu)[name = tensor&lt;string, []&gt;("cast_3")];
            tensor&lt;fp16, [1, 64]&gt; model_dense_2_BiasAdd_cast_fp16 = linear(bias = model_dense_2_BiasAdd_bias_0_to_fp16, weight = transpose_2_to_fp16, x = cast_3)[name = tensor&lt;string, []&gt;("model_dense_2_BiasAdd_cast_fp16")];
            tensor&lt;string, []&gt; model_dense_2_BiasAdd_cast_fp16_to_fp32_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_dense_2_BiasAdd_cast_fp16_to_fp32_dtype_0"), val = tensor&lt;string, []&gt;("fp32")];
            tensor&lt;fp32, [1, 64]&gt; cast_2 = cast(dtype = model_dense_2_BiasAdd_cast_fp16_to_fp32_dtype_0, x = model_dense_2_BiasAdd_cast_fp16)[name = tensor&lt;string, []&gt;("cast_2")];
            tensor&lt;fp32, [1, 64]&gt; model_dense_2_Relu = relu(x = cast_2)[name = tensor&lt;string, []&gt;("model_dense_2_Relu")];
            tensor&lt;string, []&gt; model_dense_2_Relu_to_fp16_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_dense_2_Relu_to_fp16_dtype_0"), val = tensor&lt;string, []&gt;("fp16")];
            tensor&lt;fp16, [1, 64]&gt; transpose_3_to_fp16 = const()[name = tensor&lt;string, []&gt;("transpose_3_to_fp16"), val = tensor&lt;fp16, [1, 64]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(756608)))];
            tensor&lt;fp16, [1]&gt; model_dense_3_BiasAdd_bias_0_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_dense_3_BiasAdd_bias_0_to_fp16"), val = tensor&lt;fp16, [1]&gt;([-0x1.6a4p-3])];
            tensor&lt;fp16, [1, 64]&gt; cast_1 = cast(dtype = model_dense_2_Relu_to_fp16_dtype_0, x = model_dense_2_Relu)[name = tensor&lt;string, []&gt;("cast_1")];
            tensor&lt;fp16, [1, 1]&gt; model_dense_3_BiasAdd_cast_fp16 = linear(bias = model_dense_3_BiasAdd_bias_0_to_fp16, weight = transpose_3_to_fp16, x = cast_1)[name = tensor&lt;string, []&gt;("model_dense_3_BiasAdd_cast_fp16")];
            tensor&lt;string, []&gt; model_dense_3_BiasAdd_cast_fp16_to_fp32_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_dense_3_BiasAdd_cast_fp16_to_fp32_dtype_0"), val = tensor&lt;string, []&gt;("fp32")];
            tensor&lt;fp32, [1, 1]&gt; cast_0 = cast(dtype = model_dense_3_BiasAdd_cast_fp16_to_fp32_dtype_0, x = model_dense_3_BiasAdd_cast_fp16)[name = tensor&lt;string, []&gt;("cast_0")];
            tensor&lt;fp32, [1, 1]&gt; Identity = sigmoid(x = cast_0)[name = tensor&lt;string, []&gt;("model_dense_3_Sigmoid")];
        } -&gt; (Identity);
}</code></pre>
                    </div>
                </div>
            
                <div class="file-section" id="file-6">
                    <div class="file-header">
                        <h2>ðŸ“„ model.mil 4</h2>
                        <div class="file-meta">Size: 985.88 KB</div>
                    </div>
                    <div class="file-content">
                        <pre><code>program(1.0)
[buildInfo = dict&lt;tensor&lt;string, []&gt;, tensor&lt;string, []&gt;&gt;({{"coremlc-component-MIL", "3405.2.1"}, {"coremlc-version", "3405.2.1"}})]
{
    func main&lt;ios16&gt;(tensor&lt;fp16, [1, 192, 768]&gt; aenc, tensor&lt;int32, [1, 192]&gt; bssid, tensor&lt;int32, [1, 192]&gt; cid, tensor&lt;int32, [1, 192]&gt; ctx, tensor&lt;int32, [1, 192, 1]&gt; d, tensor&lt;int32, [1, 192]&gt; ftid, tensor&lt;int32, [1, 192]&gt; gtid, tensor&lt;int32, [1, 192, 24]&gt; tid) {
            tensor&lt;int32, []&gt; var_20 = const()[name = tensor&lt;string, []&gt;("op_20"), val = tensor&lt;int32, []&gt;(-1)];
            tensor&lt;bool, []&gt; var_25 = const()[name = tensor&lt;string, []&gt;("op_25"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;int32, []&gt; cemb_1_axis_0 = const()[name = tensor&lt;string, []&gt;("cemb_1_axis_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, []&gt; cemb_1_batch_dims_0 = const()[name = tensor&lt;string, []&gt;("cemb_1_batch_dims_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;fp16, [1997, 256]&gt; hist_encoder_channel_emb_weight_to_fp16 = const()[name = tensor&lt;string, []&gt;("hist_encoder_channel_emb_weight_to_fp16"), val = tensor&lt;fp16, [1997, 256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(64)))];
            tensor&lt;fp16, [1, 192, 256]&gt; cemb_1_cast = gather(axis = cemb_1_axis_0, batch_dims = cemb_1_batch_dims_0, indices = cid, x = hist_encoder_channel_emb_weight_to_fp16);
            tensor&lt;int32, []&gt; temb_1_axis_0 = const()[name = tensor&lt;string, []&gt;("temb_1_axis_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, []&gt; temb_1_batch_dims_0 = const()[name = tensor&lt;string, []&gt;("temb_1_batch_dims_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;fp16, [14215, 256]&gt; hist_encoder_topic_emb_weight_to_fp16 = const()[name = tensor&lt;string, []&gt;("hist_encoder_topic_emb_weight_to_fp16"), val = tensor&lt;fp16, [14215, 256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(1022592)))];
            tensor&lt;fp16, [1, 192, 24, 256]&gt; temb_1_cast = gather(axis = temb_1_axis_0, batch_dims = temb_1_batch_dims_0, indices = tid, x = hist_encoder_topic_emb_weight_to_fp16);
            tensor&lt;int32, [1]&gt; var_74 = const()[name = tensor&lt;string, []&gt;("op_74"), val = tensor&lt;int32, [1]&gt;([-2])];
            tensor&lt;fp16, [1, 192, 256]&gt; temb_3_cast = reduce_mean(axes = var_74, keep_dims = var_25, x = temb_1_cast);
            tensor&lt;int32, []&gt; ftemb_axis_0 = const()[name = tensor&lt;string, []&gt;("ftemb_axis_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, []&gt; ftemb_batch_dims_0 = const()[name = tensor&lt;string, []&gt;("ftemb_batch_dims_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;fp16, [13, 256]&gt; hist_encoder_feed_type_emb_weight_to_fp16 = const()[name = tensor&lt;string, []&gt;("hist_encoder_feed_type_emb_weight_to_fp16"), val = tensor&lt;fp16, [13, 256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(8300736)))];
            tensor&lt;fp16, [1, 192, 256]&gt; ftemb_cast = gather(axis = ftemb_axis_0, batch_dims = ftemb_batch_dims_0, indices = ftid, x = hist_encoder_feed_type_emb_weight_to_fp16);
            tensor&lt;int32, []&gt; gtemb_axis_0 = const()[name = tensor&lt;string, []&gt;("gtemb_axis_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, []&gt; gtemb_batch_dims_0 = const()[name = tensor&lt;string, []&gt;("gtemb_batch_dims_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;fp16, [32, 256]&gt; hist_encoder_group_type_emb_weight_to_fp16 = const()[name = tensor&lt;string, []&gt;("hist_encoder_group_type_emb_weight_to_fp16"), val = tensor&lt;fp16, [32, 256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(8307456)))];
            tensor&lt;fp16, [1, 192, 256]&gt; gtemb_cast = gather(axis = gtemb_axis_0, batch_dims = gtemb_batch_dims_0, indices = gtid, x = hist_encoder_group_type_emb_weight_to_fp16);
            tensor&lt;int32, []&gt; bssemb_axis_0 = const()[name = tensor&lt;string, []&gt;("bssemb_axis_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, []&gt; bssemb_batch_dims_0 = const()[name = tensor&lt;string, []&gt;("bssemb_batch_dims_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;fp16, [7, 256]&gt; hist_encoder_bundle_subscription_status_emb_weight_to_fp16 = const()[name = tensor&lt;string, []&gt;("hist_encoder_bundle_subscription_status_emb_weight_to_fp16"), val = tensor&lt;fp16, [7, 256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(8323904)))];
            tensor&lt;fp16, [1, 192, 256]&gt; bssemb_cast = gather(axis = bssemb_axis_0, batch_dims = bssemb_batch_dims_0, indices = bssid, x = hist_encoder_bundle_subscription_status_emb_weight_to_fp16);
            tensor&lt;string, []&gt; var_79_to_fp16_dtype_0 = const()[name = tensor&lt;string, []&gt;("op_79_to_fp16_dtype_0"), val = tensor&lt;string, []&gt;("fp16")];
            tensor&lt;fp16, [1]&gt; _inversed_80_y_0_to_fp16 = const()[name = tensor&lt;string, []&gt;("_inversed_80_y_0_to_fp16"), val = tensor&lt;fp16, [1]&gt;([0x1.11p-8])];
            tensor&lt;fp16, [1, 192, 1]&gt; cast_147 = cast(dtype = var_79_to_fp16_dtype_0, x = d);
            tensor&lt;fp16, [1, 192, 1]&gt; _inversed_80_cast = mul(x = cast_147, y = _inversed_80_y_0_to_fp16);
            tensor&lt;fp16, []&gt; var_26_promoted_to_fp16 = const()[name = tensor&lt;string, []&gt;("op_26_promoted_to_fp16"), val = tensor&lt;fp16, []&gt;(0x0p+0)];
            tensor&lt;fp16, []&gt; var_30_promoted_to_fp16 = const()[name = tensor&lt;string, []&gt;("op_30_promoted_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1p+0)];
            tensor&lt;fp16, [1, 192, 1]&gt; input_1_cast = clip(alpha = var_26_promoted_to_fp16, beta = var_30_promoted_to_fp16, x = _inversed_80_cast);
            tensor&lt;fp16, [256, 1]&gt; hist_encoder_dur_emb_ff_0_weight_to_fp16 = const()[name = tensor&lt;string, []&gt;("hist_encoder_dur_emb_ff_0_weight_to_fp16"), val = tensor&lt;fp16, [256, 1]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(8327552)))];
            tensor&lt;fp16, [256]&gt; hist_encoder_dur_emb_ff_0_bias_to_fp16 = const()[name = tensor&lt;string, []&gt;("hist_encoder_dur_emb_ff_0_bias_to_fp16"), val = tensor&lt;fp16, [256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(8328128)))];
            tensor&lt;fp16, [1, 192, 256]&gt; input_3_cast = linear(bias = hist_encoder_dur_emb_ff_0_bias_to_fp16, weight = hist_encoder_dur_emb_ff_0_weight_to_fp16, x = input_1_cast);
            tensor&lt;string, []&gt; input_5_mode_0 = const()[name = tensor&lt;string, []&gt;("input_5_mode_0"), val = tensor&lt;string, []&gt;("EXACT")];
            tensor&lt;fp16, [1, 192, 256]&gt; input_5_cast = gelu(mode = input_5_mode_0, x = input_3_cast);
            tensor&lt;fp16, [256, 256]&gt; hist_encoder_dur_emb_ff_2_weight_to_fp16 = const()[name = tensor&lt;string, []&gt;("hist_encoder_dur_emb_ff_2_weight_to_fp16"), val = tensor&lt;fp16, [256, 256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(8328704)))];
            tensor&lt;fp16, [256]&gt; hist_encoder_dur_emb_ff_2_bias_to_fp16 = const()[name = tensor&lt;string, []&gt;("hist_encoder_dur_emb_ff_2_bias_to_fp16"), val = tensor&lt;fp16, [256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(8459840)))];
            tensor&lt;fp16, [1, 192, 256]&gt; duremb_1_cast = linear(bias = hist_encoder_dur_emb_ff_2_bias_to_fp16, weight = hist_encoder_dur_emb_ff_2_weight_to_fp16, x = input_5_cast);
            tensor&lt;fp16, [1, 192, 256]&gt; var_85_cast = add(x = cemb_1_cast, y = temb_3_cast);
            tensor&lt;fp16, [1, 192, 256]&gt; var_86_cast = add(x = var_85_cast, y = ftemb_cast);
            tensor&lt;fp16, [1, 192, 256]&gt; var_87_cast = add(x = var_86_cast, y = gtemb_cast);
            tensor&lt;fp16, [1, 192, 256]&gt; var_88_cast = add(x = var_87_cast, y = bssemb_cast);
            tensor&lt;fp16, [1, 192, 256]&gt; x_1_cast = add(x = var_88_cast, y = duremb_1_cast);
            tensor&lt;fp16, [192, 256]&gt; input_7_to_fp16 = const()[name = tensor&lt;string, []&gt;("input_7_to_fp16"), val = tensor&lt;fp16, [192, 256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(8460416)))];
            tensor&lt;fp16, [1, 192, 256]&gt; all_emb_1_cast = add(x = x_1_cast, y = input_7_to_fp16);
            tensor&lt;fp16, [256, 768]&gt; hist_encoder_article_ff_0_weight_to_fp16 = const()[name = tensor&lt;string, []&gt;("hist_encoder_article_ff_0_weight_to_fp16"), val = tensor&lt;fp16, [256, 768]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(8558784)))];
            tensor&lt;fp16, [256]&gt; hist_encoder_article_ff_0_bias_to_fp16 = const()[name = tensor&lt;string, []&gt;("hist_encoder_article_ff_0_bias_to_fp16"), val = tensor&lt;fp16, [256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(8952064)))];
            tensor&lt;fp16, [1, 192, 256]&gt; article_enc_cast = linear(bias = hist_encoder_article_ff_0_bias_to_fp16, weight = hist_encoder_article_ff_0_weight_to_fp16, x = aenc);
            tensor&lt;fp16, [1, 192, 256]&gt; x_3_cast = add(x = all_emb_1_cast, y = article_enc_cast);
            tensor&lt;fp16, [768, 256]&gt; hist_encoder_transformer_blocks_0_self_attn_c_attn_weight_to_fp16 = const()[name = tensor&lt;string, []&gt;("hist_encoder_transformer_blocks_0_self_attn_c_attn_weight_to_fp16"), val = tensor&lt;fp16, [768, 256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(8952640)))];
            tensor&lt;fp16, [768]&gt; hist_encoder_transformer_blocks_0_self_attn_c_attn_bias_to_fp16 = const()[name = tensor&lt;string, []&gt;("hist_encoder_transformer_blocks_0_self_attn_c_attn_bias_to_fp16"), val = tensor&lt;fp16, [768]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(9345920)))];
            tensor&lt;fp16, [1, 192, 768]&gt; var_110_cast = linear(bias = hist_encoder_transformer_blocks_0_self_attn_c_attn_bias_to_fp16, weight = hist_encoder_transformer_blocks_0_self_attn_c_attn_weight_to_fp16, x = x_3_cast);
            tensor&lt;int32, [3]&gt; tile_0 = const()[name = tensor&lt;string, []&gt;("tile_0"), val = tensor&lt;int32, [3]&gt;([256, 256, 256])];
            tensor&lt;int32, []&gt; var_111_axis_0 = const()[name = tensor&lt;string, []&gt;("op_111_axis_0"), val = tensor&lt;int32, []&gt;(2)];
            tensor&lt;fp16, [1, 192, 256]&gt; var_111_cast_0, tensor&lt;fp16, [1, 192, 256]&gt; var_111_cast_1, tensor&lt;fp16, [1, 192, 256]&gt; var_111_cast_2 = split(axis = var_111_axis_0, split_sizes = tile_0, x = var_110_cast);
            tensor&lt;int32, [4]&gt; var_115 = const()[name = tensor&lt;string, []&gt;("op_115"), val = tensor&lt;int32, [4]&gt;([1, 192, 8, 32])];
            tensor&lt;fp16, [1, 192, 8, 32]&gt; var_116_cast = reshape(shape = var_115, x = var_111_cast_1);
            tensor&lt;int32, [4]&gt; var_118 = const()[name = tensor&lt;string, []&gt;("op_118"), val = tensor&lt;int32, [4]&gt;([1, 192, 8, 32])];
            tensor&lt;fp16, [1, 192, 8, 32]&gt; var_119_cast = reshape(shape = var_118, x = var_111_cast_0);
            tensor&lt;int32, [4]&gt; var_121 = const()[name = tensor&lt;string, []&gt;("op_121"), val = tensor&lt;int32, [4]&gt;([1, 192, 8, 32])];
            tensor&lt;fp16, [1, 192, 8, 32]&gt; var_122_cast = reshape(shape = var_121, x = var_111_cast_2);
            tensor&lt;int32, [4]&gt; v_3_perm_0 = const()[name = tensor&lt;string, []&gt;("v_3_perm_0"), val = tensor&lt;int32, [4]&gt;([0, 2, 1, 3])];
            tensor&lt;bool, []&gt; att_1_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("att_1_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; att_1_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("att_1_transpose_y_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;int32, [4]&gt; transpose_4_perm_0 = const()[name = tensor&lt;string, []&gt;("transpose_4_perm_0"), val = tensor&lt;int32, [4]&gt;([0, 2, 1, 3])];
            tensor&lt;int32, [4]&gt; transpose_5_perm_0 = const()[name = tensor&lt;string, []&gt;("transpose_5_perm_0"), val = tensor&lt;int32, [4]&gt;([0, 2, 3, 1])];
            tensor&lt;fp16, [1, 8, 32, 192]&gt; transpose_25 = transpose(perm = transpose_5_perm_0, x = var_116_cast);
            tensor&lt;fp16, [1, 8, 192, 32]&gt; transpose_26 = transpose(perm = transpose_4_perm_0, x = var_119_cast);
            tensor&lt;fp16, [1, 8, 192, 192]&gt; att_1_cast = matmul(transpose_x = att_1_transpose_x_0, transpose_y = att_1_transpose_y_0, x = transpose_26, y = transpose_25);
            tensor&lt;fp16, []&gt; var_126_to_fp16 = const()[name = tensor&lt;string, []&gt;("op_126_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1.6ap-3)];
            tensor&lt;fp16, [1, 8, 192, 192]&gt; att_3_cast = mul(x = att_1_cast, y = var_126_to_fp16);
            tensor&lt;string, []&gt; att_3_cast_to_fp32_dtype_0 = const()[name = tensor&lt;string, []&gt;("att_3_cast_to_fp32_dtype_0"), val = tensor&lt;string, []&gt;("fp32")];
            tensor&lt;bool, [1, 1, 192, 192]&gt; var_132 = const()[name = tensor&lt;string, []&gt;("op_132"), val = tensor&lt;bool, [1, 1, 192, 192]&gt;([[[[false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false]]]])];
            tensor&lt;fp32, [1, 8, 192, 192]&gt; input_11_value = const()[name = tensor&lt;string, []&gt;("input_11_value"), val = tensor&lt;fp32, [1, 8, 192, 192]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(9347520)))];
            tensor&lt;fp32, [1, 8, 192, 192]&gt; cast_122 = cast(dtype = att_3_cast_to_fp32_dtype_0, x = att_3_cast);
            tensor&lt;fp32, [1, 8, 192, 192]&gt; input_11 = select(a = input_11_value, b = cast_122, cond = var_132);
            tensor&lt;string, []&gt; input_11_to_fp16_dtype_0 = const()[name = tensor&lt;string, []&gt;("input_11_to_fp16_dtype_0"), val = tensor&lt;string, []&gt;("fp16")];
            tensor&lt;fp16, [1, 8, 192, 192]&gt; cast_121 = cast(dtype = input_11_to_fp16_dtype_0, x = input_11);
            tensor&lt;fp16, [1, 8, 192, 192]&gt; input_13_cast = softmax(axis = var_20, x = cast_121);
            tensor&lt;bool, []&gt; y_1_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("y_1_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; y_1_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("y_1_transpose_y_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [1, 8, 192, 32]&gt; transpose_27 = transpose(perm = v_3_perm_0, x = var_122_cast);
            tensor&lt;fp16, [1, 8, 192, 32]&gt; y_1_cast = matmul(transpose_x = y_1_transpose_x_0, transpose_y = y_1_transpose_y_0, x = input_13_cast, y = transpose_27);
            tensor&lt;int32, [4]&gt; var_137_perm_0 = const()[name = tensor&lt;string, []&gt;("op_137_perm_0"), val = tensor&lt;int32, [4]&gt;([0, 2, 1, 3])];
            tensor&lt;int32, [3]&gt; var_139 = const()[name = tensor&lt;string, []&gt;("op_139"), val = tensor&lt;int32, [3]&gt;([1, 192, 256])];
            tensor&lt;fp16, [1, 192, 8, 32]&gt; transpose_24 = transpose(perm = var_137_perm_0, x = y_1_cast);
            tensor&lt;fp16, [1, 192, 256]&gt; input_15_cast = reshape(shape = var_139, x = transpose_24);
            tensor&lt;fp16, [256, 256]&gt; hist_encoder_transformer_blocks_0_self_attn_c_proj_weight_to_fp16 = const()[name = tensor&lt;string, []&gt;("hist_encoder_transformer_blocks_0_self_attn_c_proj_weight_to_fp16"), val = tensor&lt;fp16, [256, 256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(10527232)))];
            tensor&lt;fp16, [256]&gt; hist_encoder_transformer_blocks_0_self_attn_c_proj_bias_to_fp16 = const()[name = tensor&lt;string, []&gt;("hist_encoder_transformer_blocks_0_self_attn_c_proj_bias_to_fp16"), val = tensor&lt;fp16, [256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(10658368)))];
            tensor&lt;fp16, [1, 192, 256]&gt; input_17_cast = linear(bias = hist_encoder_transformer_blocks_0_self_attn_c_proj_bias_to_fp16, weight = hist_encoder_transformer_blocks_0_self_attn_c_proj_weight_to_fp16, x = input_15_cast);
            tensor&lt;fp16, [1]&gt; hist_encoder_transformer_blocks_0_alpha_to_fp16 = const()[name = tensor&lt;string, []&gt;("hist_encoder_transformer_blocks_0_alpha_to_fp16"), val = tensor&lt;fp16, [1]&gt;([-0x1.a08p-3])];
            tensor&lt;fp16, [1, 192, 256]&gt; var_145_cast = mul(x = hist_encoder_transformer_blocks_0_alpha_to_fp16, y = input_17_cast);
            tensor&lt;fp16, [1, 192, 256]&gt; input_19_cast = add(x = x_3_cast, y = var_145_cast);
            tensor&lt;fp16, [512, 256]&gt; hist_encoder_transformer_blocks_0_mlpf_0_weight_to_fp16 = const()[name = tensor&lt;string, []&gt;("hist_encoder_transformer_blocks_0_mlpf_0_weight_to_fp16"), val = tensor&lt;fp16, [512, 256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(10658944)))];
            tensor&lt;fp16, [512]&gt; hist_encoder_transformer_blocks_0_mlpf_0_bias_to_fp16 = const()[name = tensor&lt;string, []&gt;("hist_encoder_transformer_blocks_0_mlpf_0_bias_to_fp16"), val = tensor&lt;fp16, [512]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(10921152)))];
            tensor&lt;fp16, [1, 192, 512]&gt; input_21_cast = linear(bias = hist_encoder_transformer_blocks_0_mlpf_0_bias_to_fp16, weight = hist_encoder_transformer_blocks_0_mlpf_0_weight_to_fp16, x = input_19_cast);
            tensor&lt;string, []&gt; input_23_mode_0 = const()[name = tensor&lt;string, []&gt;("input_23_mode_0"), val = tensor&lt;string, []&gt;("EXACT")];
            tensor&lt;fp16, [1, 192, 512]&gt; input_23_cast = gelu(mode = input_23_mode_0, x = input_21_cast);
            tensor&lt;fp16, [256, 512]&gt; hist_encoder_transformer_blocks_0_mlpf_2_weight_to_fp16 = const()[name = tensor&lt;string, []&gt;("hist_encoder_transformer_blocks_0_mlpf_2_weight_to_fp16"), val = tensor&lt;fp16, [256, 512]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(10922240)))];
            tensor&lt;fp16, [256]&gt; hist_encoder_transformer_blocks_0_mlpf_2_bias_to_fp16 = const()[name = tensor&lt;string, []&gt;("hist_encoder_transformer_blocks_0_mlpf_2_bias_to_fp16"), val = tensor&lt;fp16, [256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(11184448)))];
            tensor&lt;fp16, [1, 192, 256]&gt; input_25_cast = linear(bias = hist_encoder_transformer_blocks_0_mlpf_2_bias_to_fp16, weight = hist_encoder_transformer_blocks_0_mlpf_2_weight_to_fp16, x = input_23_cast);
            tensor&lt;fp16, [1, 192, 256]&gt; var_157_cast = mul(x = hist_encoder_transformer_blocks_0_alpha_to_fp16, y = input_25_cast);
            tensor&lt;fp16, [1, 192, 256]&gt; x_5_cast = add(x = input_19_cast, y = var_157_cast);
            tensor&lt;fp16, [768, 256]&gt; hist_encoder_transformer_blocks_1_self_attn_c_attn_weight_to_fp16 = const()[name = tensor&lt;string, []&gt;("hist_encoder_transformer_blocks_1_self_attn_c_attn_weight_to_fp16"), val = tensor&lt;fp16, [768, 256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(11185024)))];
            tensor&lt;fp16, [768]&gt; hist_encoder_transformer_blocks_1_self_attn_c_attn_bias_to_fp16 = const()[name = tensor&lt;string, []&gt;("hist_encoder_transformer_blocks_1_self_attn_c_attn_bias_to_fp16"), val = tensor&lt;fp16, [768]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(11578304)))];
            tensor&lt;fp16, [1, 192, 768]&gt; var_169_cast = linear(bias = hist_encoder_transformer_blocks_1_self_attn_c_attn_bias_to_fp16, weight = hist_encoder_transformer_blocks_1_self_attn_c_attn_weight_to_fp16, x = x_5_cast);
            tensor&lt;int32, [3]&gt; tile_1 = const()[name = tensor&lt;string, []&gt;("tile_1"), val = tensor&lt;int32, [3]&gt;([256, 256, 256])];
            tensor&lt;int32, []&gt; var_170_axis_0 = const()[name = tensor&lt;string, []&gt;("op_170_axis_0"), val = tensor&lt;int32, []&gt;(2)];
            tensor&lt;fp16, [1, 192, 256]&gt; var_170_cast_0, tensor&lt;fp16, [1, 192, 256]&gt; var_170_cast_1, tensor&lt;fp16, [1, 192, 256]&gt; var_170_cast_2 = split(axis = var_170_axis_0, split_sizes = tile_1, x = var_169_cast);
            tensor&lt;int32, [4]&gt; var_174 = const()[name = tensor&lt;string, []&gt;("op_174"), val = tensor&lt;int32, [4]&gt;([1, 192, 8, 32])];
            tensor&lt;fp16, [1, 192, 8, 32]&gt; var_175_cast = reshape(shape = var_174, x = var_170_cast_1);
            tensor&lt;int32, [4]&gt; var_177 = const()[name = tensor&lt;string, []&gt;("op_177"), val = tensor&lt;int32, [4]&gt;([1, 192, 8, 32])];
            tensor&lt;fp16, [1, 192, 8, 32]&gt; var_178_cast = reshape(shape = var_177, x = var_170_cast_0);
            tensor&lt;int32, [4]&gt; var_180 = const()[name = tensor&lt;string, []&gt;("op_180"), val = tensor&lt;int32, [4]&gt;([1, 192, 8, 32])];
            tensor&lt;fp16, [1, 192, 8, 32]&gt; var_181_cast = reshape(shape = var_180, x = var_170_cast_2);
            tensor&lt;int32, [4]&gt; v_7_perm_0 = const()[name = tensor&lt;string, []&gt;("v_7_perm_0"), val = tensor&lt;int32, [4]&gt;([0, 2, 1, 3])];
            tensor&lt;bool, []&gt; att_7_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("att_7_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; att_7_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("att_7_transpose_y_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;int32, [4]&gt; transpose_6_perm_0 = const()[name = tensor&lt;string, []&gt;("transpose_6_perm_0"), val = tensor&lt;int32, [4]&gt;([0, 2, 1, 3])];
            tensor&lt;int32, [4]&gt; transpose_7_perm_0 = const()[name = tensor&lt;string, []&gt;("transpose_7_perm_0"), val = tensor&lt;int32, [4]&gt;([0, 2, 3, 1])];
            tensor&lt;fp16, [1, 8, 32, 192]&gt; transpose_21 = transpose(perm = transpose_7_perm_0, x = var_175_cast);
            tensor&lt;fp16, [1, 8, 192, 32]&gt; transpose_22 = transpose(perm = transpose_6_perm_0, x = var_178_cast);
            tensor&lt;fp16, [1, 8, 192, 192]&gt; att_7_cast = matmul(transpose_x = att_7_transpose_x_0, transpose_y = att_7_transpose_y_0, x = transpose_22, y = transpose_21);
            tensor&lt;fp16, []&gt; var_185_to_fp16 = const()[name = tensor&lt;string, []&gt;("op_185_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1.6ap-3)];
            tensor&lt;fp16, [1, 8, 192, 192]&gt; att_9_cast = mul(x = att_7_cast, y = var_185_to_fp16);
            tensor&lt;string, []&gt; att_9_cast_to_fp32_dtype_0 = const()[name = tensor&lt;string, []&gt;("att_9_cast_to_fp32_dtype_0"), val = tensor&lt;string, []&gt;("fp32")];
            tensor&lt;bool, [1, 1, 192, 192]&gt; var_191 = const()[name = tensor&lt;string, []&gt;("op_191"), val = tensor&lt;bool, [1, 1, 192, 192]&gt;([[[[false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false]]]])];
            tensor&lt;fp32, [1, 8, 192, 192]&gt; input_27_value = const()[name = tensor&lt;string, []&gt;("input_27_value"), val = tensor&lt;fp32, [1, 8, 192, 192]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(11579904)))];
            tensor&lt;fp32, [1, 8, 192, 192]&gt; cast_97 = cast(dtype = att_9_cast_to_fp32_dtype_0, x = att_9_cast);
            tensor&lt;fp32, [1, 8, 192, 192]&gt; input_27 = select(a = input_27_value, b = cast_97, cond = var_191);
            tensor&lt;string, []&gt; input_27_to_fp16_dtype_0 = const()[name = tensor&lt;string, []&gt;("input_27_to_fp16_dtype_0"), val = tensor&lt;string, []&gt;("fp16")];
            tensor&lt;fp16, [1, 8, 192, 192]&gt; cast_96 = cast(dtype = input_27_to_fp16_dtype_0, x = input_27);
            tensor&lt;fp16, [1, 8, 192, 192]&gt; input_29_cast = softmax(axis = var_20, x = cast_96);
            tensor&lt;bool, []&gt; y_3_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("y_3_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; y_3_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("y_3_transpose_y_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [1, 8, 192, 32]&gt; transpose_23 = transpose(perm = v_7_perm_0, x = var_181_cast);
            tensor&lt;fp16, [1, 8, 192, 32]&gt; y_3_cast = matmul(transpose_x = y_3_transpose_x_0, transpose_y = y_3_transpose_y_0, x = input_29_cast, y = transpose_23);
            tensor&lt;int32, [4]&gt; var_196_perm_0 = const()[name = tensor&lt;string, []&gt;("op_196_perm_0"), val = tensor&lt;int32, [4]&gt;([0, 2, 1, 3])];
            tensor&lt;int32, [3]&gt; var_198 = const()[name = tensor&lt;string, []&gt;("op_198"), val = tensor&lt;int32, [3]&gt;([1, 192, 256])];
            tensor&lt;fp16, [1, 192, 8, 32]&gt; transpose_20 = transpose(perm = var_196_perm_0, x = y_3_cast);
            tensor&lt;fp16, [1, 192, 256]&gt; input_31_cast = reshape(shape = var_198, x = transpose_20);
            tensor&lt;fp16, [256, 256]&gt; hist_encoder_transformer_blocks_1_self_attn_c_proj_weight_to_fp16 = const()[name = tensor&lt;string, []&gt;("hist_encoder_transformer_blocks_1_self_attn_c_proj_weight_to_fp16"), val = tensor&lt;fp16, [256, 256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(12759616)))];
            tensor&lt;fp16, [256]&gt; hist_encoder_transformer_blocks_1_self_attn_c_proj_bias_to_fp16 = const()[name = tensor&lt;string, []&gt;("hist_encoder_transformer_blocks_1_self_attn_c_proj_bias_to_fp16"), val = tensor&lt;fp16, [256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(12890752)))];
            tensor&lt;fp16, [1, 192, 256]&gt; input_33_cast = linear(bias = hist_encoder_transformer_blocks_1_self_attn_c_proj_bias_to_fp16, weight = hist_encoder_transformer_blocks_1_self_attn_c_proj_weight_to_fp16, x = input_31_cast);
            tensor&lt;fp16, [1]&gt; hist_encoder_transformer_blocks_1_alpha_to_fp16 = const()[name = tensor&lt;string, []&gt;("hist_encoder_transformer_blocks_1_alpha_to_fp16"), val = tensor&lt;fp16, [1]&gt;([0x1.ddcp-2])];
            tensor&lt;fp16, [1, 192, 256]&gt; var_204_cast = mul(x = hist_encoder_transformer_blocks_1_alpha_to_fp16, y = input_33_cast);
            tensor&lt;fp16, [1, 192, 256]&gt; input_35_cast = add(x = x_5_cast, y = var_204_cast);
            tensor&lt;fp16, [512, 256]&gt; hist_encoder_transformer_blocks_1_mlpf_0_weight_to_fp16 = const()[name = tensor&lt;string, []&gt;("hist_encoder_transformer_blocks_1_mlpf_0_weight_to_fp16"), val = tensor&lt;fp16, [512, 256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(12891328)))];
            tensor&lt;fp16, [512]&gt; hist_encoder_transformer_blocks_1_mlpf_0_bias_to_fp16 = const()[name = tensor&lt;string, []&gt;("hist_encoder_transformer_blocks_1_mlpf_0_bias_to_fp16"), val = tensor&lt;fp16, [512]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(13153536)))];
            tensor&lt;fp16, [1, 192, 512]&gt; input_37_cast = linear(bias = hist_encoder_transformer_blocks_1_mlpf_0_bias_to_fp16, weight = hist_encoder_transformer_blocks_1_mlpf_0_weight_to_fp16, x = input_35_cast);
            tensor&lt;string, []&gt; input_39_mode_0 = const()[name = tensor&lt;string, []&gt;("input_39_mode_0"), val = tensor&lt;string, []&gt;("EXACT")];
            tensor&lt;fp16, [1, 192, 512]&gt; input_39_cast = gelu(mode = input_39_mode_0, x = input_37_cast);
            tensor&lt;fp16, [256, 512]&gt; hist_encoder_transformer_blocks_1_mlpf_2_weight_to_fp16 = const()[name = tensor&lt;string, []&gt;("hist_encoder_transformer_blocks_1_mlpf_2_weight_to_fp16"), val = tensor&lt;fp16, [256, 512]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(13154624)))];
            tensor&lt;fp16, [256]&gt; hist_encoder_transformer_blocks_1_mlpf_2_bias_to_fp16 = const()[name = tensor&lt;string, []&gt;("hist_encoder_transformer_blocks_1_mlpf_2_bias_to_fp16"), val = tensor&lt;fp16, [256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(13416832)))];
            tensor&lt;fp16, [1, 192, 256]&gt; input_41_cast = linear(bias = hist_encoder_transformer_blocks_1_mlpf_2_bias_to_fp16, weight = hist_encoder_transformer_blocks_1_mlpf_2_weight_to_fp16, x = input_39_cast);
            tensor&lt;fp16, [1, 192, 256]&gt; var_216_cast = mul(x = hist_encoder_transformer_blocks_1_alpha_to_fp16, y = input_41_cast);
            tensor&lt;fp16, [1, 192, 256]&gt; x_7_cast = add(x = input_35_cast, y = var_216_cast);
            tensor&lt;fp16, [768, 256]&gt; hist_encoder_transformer_blocks_2_self_attn_c_attn_weight_to_fp16 = const()[name = tensor&lt;string, []&gt;("hist_encoder_transformer_blocks_2_self_attn_c_attn_weight_to_fp16"), val = tensor&lt;fp16, [768, 256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(13417408)))];
            tensor&lt;fp16, [768]&gt; hist_encoder_transformer_blocks_2_self_attn_c_attn_bias_to_fp16 = const()[name = tensor&lt;string, []&gt;("hist_encoder_transformer_blocks_2_self_attn_c_attn_bias_to_fp16"), val = tensor&lt;fp16, [768]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(13810688)))];
            tensor&lt;fp16, [1, 192, 768]&gt; var_228_cast = linear(bias = hist_encoder_transformer_blocks_2_self_attn_c_attn_bias_to_fp16, weight = hist_encoder_transformer_blocks_2_self_attn_c_attn_weight_to_fp16, x = x_7_cast);
            tensor&lt;int32, [3]&gt; tile_2 = const()[name = tensor&lt;string, []&gt;("tile_2"), val = tensor&lt;int32, [3]&gt;([256, 256, 256])];
            tensor&lt;int32, []&gt; var_229_axis_0 = const()[name = tensor&lt;string, []&gt;("op_229_axis_0"), val = tensor&lt;int32, []&gt;(2)];
            tensor&lt;fp16, [1, 192, 256]&gt; var_229_cast_0, tensor&lt;fp16, [1, 192, 256]&gt; var_229_cast_1, tensor&lt;fp16, [1, 192, 256]&gt; var_229_cast_2 = split(axis = var_229_axis_0, split_sizes = tile_2, x = var_228_cast);
            tensor&lt;int32, [4]&gt; var_233 = const()[name = tensor&lt;string, []&gt;("op_233"), val = tensor&lt;int32, [4]&gt;([1, 192, 8, 32])];
            tensor&lt;fp16, [1, 192, 8, 32]&gt; var_234_cast = reshape(shape = var_233, x = var_229_cast_1);
            tensor&lt;int32, [4]&gt; var_236 = const()[name = tensor&lt;string, []&gt;("op_236"), val = tensor&lt;int32, [4]&gt;([1, 192, 8, 32])];
            tensor&lt;fp16, [1, 192, 8, 32]&gt; var_237_cast = reshape(shape = var_236, x = var_229_cast_0);
            tensor&lt;int32, [4]&gt; var_239 = const()[name = tensor&lt;string, []&gt;("op_239"), val = tensor&lt;int32, [4]&gt;([1, 192, 8, 32])];
            tensor&lt;fp16, [1, 192, 8, 32]&gt; var_240_cast = reshape(shape = var_239, x = var_229_cast_2);
            tensor&lt;int32, [4]&gt; v_11_perm_0 = const()[name = tensor&lt;string, []&gt;("v_11_perm_0"), val = tensor&lt;int32, [4]&gt;([0, 2, 1, 3])];
            tensor&lt;bool, []&gt; att_13_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("att_13_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; att_13_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("att_13_transpose_y_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;int32, [4]&gt; transpose_8_perm_0 = const()[name = tensor&lt;string, []&gt;("transpose_8_perm_0"), val = tensor&lt;int32, [4]&gt;([0, 2, 1, 3])];
            tensor&lt;int32, [4]&gt; transpose_9_perm_0 = const()[name = tensor&lt;string, []&gt;("transpose_9_perm_0"), val = tensor&lt;int32, [4]&gt;([0, 2, 3, 1])];
            tensor&lt;fp16, [1, 8, 32, 192]&gt; transpose_17 = transpose(perm = transpose_9_perm_0, x = var_234_cast);
            tensor&lt;fp16, [1, 8, 192, 32]&gt; transpose_18 = transpose(perm = transpose_8_perm_0, x = var_237_cast);
            tensor&lt;fp16, [1, 8, 192, 192]&gt; att_13_cast = matmul(transpose_x = att_13_transpose_x_0, transpose_y = att_13_transpose_y_0, x = transpose_18, y = transpose_17);
            tensor&lt;fp16, []&gt; var_244_to_fp16 = const()[name = tensor&lt;string, []&gt;("op_244_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1.6ap-3)];
            tensor&lt;fp16, [1, 8, 192, 192]&gt; att_15_cast = mul(x = att_13_cast, y = var_244_to_fp16);
            tensor&lt;string, []&gt; att_15_cast_to_fp32_dtype_0 = const()[name = tensor&lt;string, []&gt;("att_15_cast_to_fp32_dtype_0"), val = tensor&lt;string, []&gt;("fp32")];
            tensor&lt;bool, [1, 1, 192, 192]&gt; var_250 = const()[name = tensor&lt;string, []&gt;("op_250"), val = tensor&lt;bool, [1, 1, 192, 192]&gt;([[[[false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false]]]])];
            tensor&lt;fp32, [1, 8, 192, 192]&gt; input_43_value = const()[name = tensor&lt;string, []&gt;("input_43_value"), val = tensor&lt;fp32, [1, 8, 192, 192]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(13812288)))];
            tensor&lt;fp32, [1, 8, 192, 192]&gt; cast_72 = cast(dtype = att_15_cast_to_fp32_dtype_0, x = att_15_cast);
            tensor&lt;fp32, [1, 8, 192, 192]&gt; input_43 = select(a = input_43_value, b = cast_72, cond = var_250);
            tensor&lt;string, []&gt; input_43_to_fp16_dtype_0 = const()[name = tensor&lt;string, []&gt;("input_43_to_fp16_dtype_0"), val = tensor&lt;string, []&gt;("fp16")];
            tensor&lt;fp16, [1, 8, 192, 192]&gt; cast_71 = cast(dtype = input_43_to_fp16_dtype_0, x = input_43);
            tensor&lt;fp16, [1, 8, 192, 192]&gt; input_45_cast = softmax(axis = var_20, x = cast_71);
            tensor&lt;bool, []&gt; y_5_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("y_5_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; y_5_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("y_5_transpose_y_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [1, 8, 192, 32]&gt; transpose_19 = transpose(perm = v_11_perm_0, x = var_240_cast);
            tensor&lt;fp16, [1, 8, 192, 32]&gt; y_5_cast = matmul(transpose_x = y_5_transpose_x_0, transpose_y = y_5_transpose_y_0, x = input_45_cast, y = transpose_19);
            tensor&lt;int32, [4]&gt; var_255_perm_0 = const()[name = tensor&lt;string, []&gt;("op_255_perm_0"), val = tensor&lt;int32, [4]&gt;([0, 2, 1, 3])];
            tensor&lt;int32, [3]&gt; var_257 = const()[name = tensor&lt;string, []&gt;("op_257"), val = tensor&lt;int32, [3]&gt;([1, 192, 256])];
            tensor&lt;fp16, [1, 192, 8, 32]&gt; transpose_16 = transpose(perm = var_255_perm_0, x = y_5_cast);
            tensor&lt;fp16, [1, 192, 256]&gt; input_47_cast = reshape(shape = var_257, x = transpose_16);
            tensor&lt;fp16, [256, 256]&gt; hist_encoder_transformer_blocks_2_self_attn_c_proj_weight_to_fp16 = const()[name = tensor&lt;string, []&gt;("hist_encoder_transformer_blocks_2_self_attn_c_proj_weight_to_fp16"), val = tensor&lt;fp16, [256, 256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(14992000)))];
            tensor&lt;fp16, [256]&gt; hist_encoder_transformer_blocks_2_self_attn_c_proj_bias_to_fp16 = const()[name = tensor&lt;string, []&gt;("hist_encoder_transformer_blocks_2_self_attn_c_proj_bias_to_fp16"), val = tensor&lt;fp16, [256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(15123136)))];
            tensor&lt;fp16, [1, 192, 256]&gt; input_49_cast = linear(bias = hist_encoder_transformer_blocks_2_self_attn_c_proj_bias_to_fp16, weight = hist_encoder_transformer_blocks_2_self_attn_c_proj_weight_to_fp16, x = input_47_cast);
            tensor&lt;fp16, [1]&gt; hist_encoder_transformer_blocks_2_alpha_to_fp16 = const()[name = tensor&lt;string, []&gt;("hist_encoder_transformer_blocks_2_alpha_to_fp16"), val = tensor&lt;fp16, [1]&gt;([0x1.2dcp-1])];
            tensor&lt;fp16, [1, 192, 256]&gt; var_263_cast = mul(x = hist_encoder_transformer_blocks_2_alpha_to_fp16, y = input_49_cast);
            tensor&lt;fp16, [1, 192, 256]&gt; input_51_cast = add(x = x_7_cast, y = var_263_cast);
            tensor&lt;fp16, [512, 256]&gt; hist_encoder_transformer_blocks_2_mlpf_0_weight_to_fp16 = const()[name = tensor&lt;string, []&gt;("hist_encoder_transformer_blocks_2_mlpf_0_weight_to_fp16"), val = tensor&lt;fp16, [512, 256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(15123712)))];
            tensor&lt;fp16, [512]&gt; hist_encoder_transformer_blocks_2_mlpf_0_bias_to_fp16 = const()[name = tensor&lt;string, []&gt;("hist_encoder_transformer_blocks_2_mlpf_0_bias_to_fp16"), val = tensor&lt;fp16, [512]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(15385920)))];
            tensor&lt;fp16, [1, 192, 512]&gt; input_53_cast = linear(bias = hist_encoder_transformer_blocks_2_mlpf_0_bias_to_fp16, weight = hist_encoder_transformer_blocks_2_mlpf_0_weight_to_fp16, x = input_51_cast);
            tensor&lt;string, []&gt; input_55_mode_0 = const()[name = tensor&lt;string, []&gt;("input_55_mode_0"), val = tensor&lt;string, []&gt;("EXACT")];
            tensor&lt;fp16, [1, 192, 512]&gt; input_55_cast = gelu(mode = input_55_mode_0, x = input_53_cast);
            tensor&lt;fp16, [256, 512]&gt; hist_encoder_transformer_blocks_2_mlpf_2_weight_to_fp16 = const()[name = tensor&lt;string, []&gt;("hist_encoder_transformer_blocks_2_mlpf_2_weight_to_fp16"), val = tensor&lt;fp16, [256, 512]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(15387008)))];
            tensor&lt;fp16, [256]&gt; hist_encoder_transformer_blocks_2_mlpf_2_bias_to_fp16 = const()[name = tensor&lt;string, []&gt;("hist_encoder_transformer_blocks_2_mlpf_2_bias_to_fp16"), val = tensor&lt;fp16, [256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(15649216)))];
            tensor&lt;fp16, [1, 192, 256]&gt; input_57_cast = linear(bias = hist_encoder_transformer_blocks_2_mlpf_2_bias_to_fp16, weight = hist_encoder_transformer_blocks_2_mlpf_2_weight_to_fp16, x = input_55_cast);
            tensor&lt;fp16, [1, 192, 256]&gt; var_275_cast = mul(x = hist_encoder_transformer_blocks_2_alpha_to_fp16, y = input_57_cast);
            tensor&lt;fp16, [1, 192, 256]&gt; x_cast = add(x = input_51_cast, y = var_275_cast);
            tensor&lt;fp16, [768, 256]&gt; hist_encoder_transformer_blocks_3_self_attn_c_attn_weight_to_fp16 = const()[name = tensor&lt;string, []&gt;("hist_encoder_transformer_blocks_3_self_attn_c_attn_weight_to_fp16"), val = tensor&lt;fp16, [768, 256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(15649792)))];
            tensor&lt;fp16, [768]&gt; hist_encoder_transformer_blocks_3_self_attn_c_attn_bias_to_fp16 = const()[name = tensor&lt;string, []&gt;("hist_encoder_transformer_blocks_3_self_attn_c_attn_bias_to_fp16"), val = tensor&lt;fp16, [768]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(16043072)))];
            tensor&lt;fp16, [1, 192, 768]&gt; var_287_cast = linear(bias = hist_encoder_transformer_blocks_3_self_attn_c_attn_bias_to_fp16, weight = hist_encoder_transformer_blocks_3_self_attn_c_attn_weight_to_fp16, x = x_cast);
            tensor&lt;int32, [3]&gt; tile_3 = const()[name = tensor&lt;string, []&gt;("tile_3"), val = tensor&lt;int32, [3]&gt;([256, 256, 256])];
            tensor&lt;int32, []&gt; var_288_axis_0 = const()[name = tensor&lt;string, []&gt;("op_288_axis_0"), val = tensor&lt;int32, []&gt;(2)];
            tensor&lt;fp16, [1, 192, 256]&gt; var_288_cast_0, tensor&lt;fp16, [1, 192, 256]&gt; var_288_cast_1, tensor&lt;fp16, [1, 192, 256]&gt; var_288_cast_2 = split(axis = var_288_axis_0, split_sizes = tile_3, x = var_287_cast);
            tensor&lt;int32, [4]&gt; var_292 = const()[name = tensor&lt;string, []&gt;("op_292"), val = tensor&lt;int32, [4]&gt;([1, 192, 8, 32])];
            tensor&lt;fp16, [1, 192, 8, 32]&gt; var_293_cast = reshape(shape = var_292, x = var_288_cast_1);
            tensor&lt;int32, [4]&gt; var_295 = const()[name = tensor&lt;string, []&gt;("op_295"), val = tensor&lt;int32, [4]&gt;([1, 192, 8, 32])];
            tensor&lt;fp16, [1, 192, 8, 32]&gt; var_296_cast = reshape(shape = var_295, x = var_288_cast_0);
            tensor&lt;int32, [4]&gt; var_298 = const()[name = tensor&lt;string, []&gt;("op_298"), val = tensor&lt;int32, [4]&gt;([1, 192, 8, 32])];
            tensor&lt;fp16, [1, 192, 8, 32]&gt; var_299_cast = reshape(shape = var_298, x = var_288_cast_2);
            tensor&lt;int32, [4]&gt; v_perm_0 = const()[name = tensor&lt;string, []&gt;("v_perm_0"), val = tensor&lt;int32, [4]&gt;([0, 2, 1, 3])];
            tensor&lt;bool, []&gt; att_19_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("att_19_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; att_19_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("att_19_transpose_y_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;int32, [4]&gt; transpose_10_perm_0 = const()[name = tensor&lt;string, []&gt;("transpose_10_perm_0"), val = tensor&lt;int32, [4]&gt;([0, 2, 1, 3])];
            tensor&lt;int32, [4]&gt; transpose_11_perm_0 = const()[name = tensor&lt;string, []&gt;("transpose_11_perm_0"), val = tensor&lt;int32, [4]&gt;([0, 2, 3, 1])];
            tensor&lt;fp16, [1, 8, 32, 192]&gt; transpose_13 = transpose(perm = transpose_11_perm_0, x = var_293_cast);
            tensor&lt;fp16, [1, 8, 192, 32]&gt; transpose_14 = transpose(perm = transpose_10_perm_0, x = var_296_cast);
            tensor&lt;fp16, [1, 8, 192, 192]&gt; att_19_cast = matmul(transpose_x = att_19_transpose_x_0, transpose_y = att_19_transpose_y_0, x = transpose_14, y = transpose_13);
            tensor&lt;fp16, []&gt; var_303_to_fp16 = const()[name = tensor&lt;string, []&gt;("op_303_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1.6ap-3)];
            tensor&lt;fp16, [1, 8, 192, 192]&gt; att_21_cast = mul(x = att_19_cast, y = var_303_to_fp16);
            tensor&lt;string, []&gt; att_21_cast_to_fp32_dtype_0 = const()[name = tensor&lt;string, []&gt;("att_21_cast_to_fp32_dtype_0"), val = tensor&lt;string, []&gt;("fp32")];
            tensor&lt;bool, [1, 1, 192, 192]&gt; var_309 = const()[name = tensor&lt;string, []&gt;("op_309"), val = tensor&lt;bool, [1, 1, 192, 192]&gt;([[[[false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false], [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false]]]])];
            tensor&lt;fp32, [1, 8, 192, 192]&gt; input_59_value = const()[name = tensor&lt;string, []&gt;("input_59_value"), val = tensor&lt;fp32, [1, 8, 192, 192]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(16044672)))];
            tensor&lt;fp32, [1, 8, 192, 192]&gt; cast_47 = cast(dtype = att_21_cast_to_fp32_dtype_0, x = att_21_cast);
            tensor&lt;fp32, [1, 8, 192, 192]&gt; input_59 = select(a = input_59_value, b = cast_47, cond = var_309);
            tensor&lt;string, []&gt; input_59_to_fp16_dtype_0 = const()[name = tensor&lt;string, []&gt;("input_59_to_fp16_dtype_0"), val = tensor&lt;string, []&gt;("fp16")];
            tensor&lt;fp16, [1, 8, 192, 192]&gt; cast_46 = cast(dtype = input_59_to_fp16_dtype_0, x = input_59);
            tensor&lt;fp16, [1, 8, 192, 192]&gt; input_61_cast = softmax(axis = var_20, x = cast_46);
            tensor&lt;bool, []&gt; y_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("y_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; y_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("y_transpose_y_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [1, 8, 192, 32]&gt; transpose_15 = transpose(perm = v_perm_0, x = var_299_cast);
            tensor&lt;fp16, [1, 8, 192, 32]&gt; y_cast = matmul(transpose_x = y_transpose_x_0, transpose_y = y_transpose_y_0, x = input_61_cast, y = transpose_15);
            tensor&lt;int32, [4]&gt; var_314_perm_0 = const()[name = tensor&lt;string, []&gt;("op_314_perm_0"), val = tensor&lt;int32, [4]&gt;([0, 2, 1, 3])];
            tensor&lt;int32, [3]&gt; var_316 = const()[name = tensor&lt;string, []&gt;("op_316"), val = tensor&lt;int32, [3]&gt;([1, 192, 256])];
            tensor&lt;fp16, [1, 192, 8, 32]&gt; transpose_12 = transpose(perm = var_314_perm_0, x = y_cast);
            tensor&lt;fp16, [1, 192, 256]&gt; input_63_cast = reshape(shape = var_316, x = transpose_12);
            tensor&lt;fp16, [256, 256]&gt; hist_encoder_transformer_blocks_3_self_attn_c_proj_weight_to_fp16 = const()[name = tensor&lt;string, []&gt;("hist_encoder_transformer_blocks_3_self_attn_c_proj_weight_to_fp16"), val = tensor&lt;fp16, [256, 256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(17224384)))];
            tensor&lt;fp16, [256]&gt; hist_encoder_transformer_blocks_3_self_attn_c_proj_bias_to_fp16 = const()[name = tensor&lt;string, []&gt;("hist_encoder_transformer_blocks_3_self_attn_c_proj_bias_to_fp16"), val = tensor&lt;fp16, [256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(17355520)))];
            tensor&lt;fp16, [1, 192, 256]&gt; input_65_cast = linear(bias = hist_encoder_transformer_blocks_3_self_attn_c_proj_bias_to_fp16, weight = hist_encoder_transformer_blocks_3_self_attn_c_proj_weight_to_fp16, x = input_63_cast);
            tensor&lt;fp16, [1]&gt; hist_encoder_transformer_blocks_3_alpha_to_fp16 = const()[name = tensor&lt;string, []&gt;("hist_encoder_transformer_blocks_3_alpha_to_fp16"), val = tensor&lt;fp16, [1]&gt;([0x1.204p-1])];
            tensor&lt;fp16, [1, 192, 256]&gt; var_322_cast = mul(x = hist_encoder_transformer_blocks_3_alpha_to_fp16, y = input_65_cast);
            tensor&lt;fp16, [1, 192, 256]&gt; input_67_cast = add(x = x_cast, y = var_322_cast);
            tensor&lt;fp16, [512, 256]&gt; hist_encoder_transformer_blocks_3_mlpf_0_weight_to_fp16 = const()[name = tensor&lt;string, []&gt;("hist_encoder_transformer_blocks_3_mlpf_0_weight_to_fp16"), val = tensor&lt;fp16, [512, 256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(17356096)))];
            tensor&lt;fp16, [512]&gt; hist_encoder_transformer_blocks_3_mlpf_0_bias_to_fp16 = const()[name = tensor&lt;string, []&gt;("hist_encoder_transformer_blocks_3_mlpf_0_bias_to_fp16"), val = tensor&lt;fp16, [512]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(17618304)))];
            tensor&lt;fp16, [1, 192, 512]&gt; input_69_cast = linear(bias = hist_encoder_transformer_blocks_3_mlpf_0_bias_to_fp16, weight = hist_encoder_transformer_blocks_3_mlpf_0_weight_to_fp16, x = input_67_cast);
            tensor&lt;string, []&gt; input_71_mode_0 = const()[name = tensor&lt;string, []&gt;("input_71_mode_0"), val = tensor&lt;string, []&gt;("EXACT")];
            tensor&lt;fp16, [1, 192, 512]&gt; input_71_cast = gelu(mode = input_71_mode_0, x = input_69_cast);
            tensor&lt;fp16, [256, 512]&gt; hist_encoder_transformer_blocks_3_mlpf_2_weight_to_fp16 = const()[name = tensor&lt;string, []&gt;("hist_encoder_transformer_blocks_3_mlpf_2_weight_to_fp16"), val = tensor&lt;fp16, [256, 512]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(17619392)))];
            tensor&lt;fp16, [256]&gt; hist_encoder_transformer_blocks_3_mlpf_2_bias_to_fp16 = const()[name = tensor&lt;string, []&gt;("hist_encoder_transformer_blocks_3_mlpf_2_bias_to_fp16"), val = tensor&lt;fp16, [256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(17881600)))];
            tensor&lt;fp16, [1, 192, 256]&gt; input_cast = linear(bias = hist_encoder_transformer_blocks_3_mlpf_2_bias_to_fp16, weight = hist_encoder_transformer_blocks_3_mlpf_2_weight_to_fp16, x = input_71_cast);
            tensor&lt;fp16, [1, 192, 256]&gt; var_334_cast = mul(x = hist_encoder_transformer_blocks_3_alpha_to_fp16, y = input_cast);
            tensor&lt;fp16, [1, 192, 256]&gt; enc_cast = add(x = input_67_cast, y = var_334_cast);
            tensor&lt;int32, []&gt; w_axis_0 = const()[name = tensor&lt;string, []&gt;("w_axis_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, []&gt; w_batch_dims_0 = const()[name = tensor&lt;string, []&gt;("w_batch_dims_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;fp16, [12, 256]&gt; context_cw_weight_to_fp16 = const()[name = tensor&lt;string, []&gt;("context_cw_weight_to_fp16"), val = tensor&lt;fp16, [12, 256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(17882176)))];
            tensor&lt;fp16, [1, 192, 256]&gt; w_cast = gather(axis = w_axis_0, batch_dims = w_batch_dims_0, indices = ctx, x = context_cw_weight_to_fp16);
            tensor&lt;int32, []&gt; b_axis_0 = const()[name = tensor&lt;string, []&gt;("b_axis_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, []&gt; b_batch_dims_0 = const()[name = tensor&lt;string, []&gt;("b_batch_dims_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;fp16, [12, 256]&gt; context_cb_weight_to_fp16 = const()[name = tensor&lt;string, []&gt;("context_cb_weight_to_fp16"), val = tensor&lt;fp16, [12, 256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(17888384)))];
            tensor&lt;fp16, [1, 192, 256]&gt; b_cast = gather(axis = b_axis_0, batch_dims = b_batch_dims_0, indices = ctx, x = context_cb_weight_to_fp16);
            tensor&lt;fp16, [1, 192, 256]&gt; var_345_cast = mul(x = w_cast, y = enc_cast);
            tensor&lt;fp16, [1, 192, 256]&gt; hist_enc_cast = add(x = var_345_cast, y = b_cast);
            tensor&lt;bool, []&gt; var_353_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("op_353_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; var_353_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("op_353_transpose_y_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [1, 256, 1997]&gt; c_weights_to_fp16 = const()[name = tensor&lt;string, []&gt;("c_weights_to_fp16"), val = tensor&lt;fp16, [1, 256, 1997]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(17894592)))];
            tensor&lt;fp16, [1, 192, 1997]&gt; var_353_cast = matmul(transpose_x = var_353_transpose_x_0, transpose_y = var_353_transpose_y_0, x = hist_enc_cast, y = c_weights_to_fp16);
            tensor&lt;string, []&gt; var_353_cast_to_fp32_dtype_0 = const()[name = tensor&lt;string, []&gt;("op_353_cast_to_fp32_dtype_0"), val = tensor&lt;string, []&gt;("fp32")];
            tensor&lt;bool, []&gt; var_360_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("op_360_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; var_360_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("op_360_transpose_y_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [1, 256, 14215]&gt; t_weights_to_fp16 = const()[name = tensor&lt;string, []&gt;("t_weights_to_fp16"), val = tensor&lt;fp16, [1, 256, 14215]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(18917120)))];
            tensor&lt;fp16, [1, 192, 14215]&gt; var_360_cast = matmul(transpose_x = var_360_transpose_x_0, transpose_y = var_360_transpose_y_0, x = hist_enc_cast, y = t_weights_to_fp16);
            tensor&lt;string, []&gt; var_360_cast_to_fp32_dtype_0 = const()[name = tensor&lt;string, []&gt;("op_360_cast_to_fp32_dtype_0"), val = tensor&lt;string, []&gt;("fp32")];
            tensor&lt;fp32, [1, 192, 14215]&gt; var_360 = cast(dtype = var_360_cast_to_fp32_dtype_0, x = var_360_cast);
            tensor&lt;fp32, [1, 192, 1997]&gt; var_353 = cast(dtype = var_353_cast_to_fp32_dtype_0, x = var_353_cast);
        } -&gt; (var_353, var_360);
}</code></pre>
                    </div>
                </div>
            
                <div class="file-section" id="file-7">
                    <div class="file-header">
                        <h2>ðŸ“„ model.mil 9</h2>
                        <div class="file-meta">Size: 16.99 KB</div>
                    </div>
                    <div class="file-content">
                        <pre><code>program(1.0)
[buildInfo = dict&lt;tensor&lt;string, []&gt;, tensor&lt;string, []&gt;&gt;({{"coremlc-component-MIL", "3405.2.1"}, {"coremlc-version", "3405.2.1"}})]
{
    func main&lt;ios16&gt;(tensor&lt;fp16, [?, 768]&gt; input_11, tensor&lt;int32, [?, 1]&gt; input_12, tensor&lt;int32, [?, 1]&gt; input_13, tensor&lt;int32, [?, 24]&gt; input_14, tensor&lt;int32, [?, 1]&gt; input_15, tensor&lt;int32, [?, 1]&gt; input_16, tensor&lt;int32, [?, 1]&gt; input_17) [FlexibleShapeInformation = tuple&lt;tuple&lt;tensor&lt;string, []&gt;, dict&lt;tensor&lt;string, []&gt;, tensor&lt;int32, [?]&gt;&gt;&gt;, tuple&lt;tensor&lt;string, []&gt;, dict&lt;tensor&lt;string, []&gt;, list&lt;tensor&lt;int32, [2]&gt;, ?&gt;&gt;&gt;&gt;((("DefaultShapes", {{"input_11", [1, 768]}, {"input_12", [1, 1]}, {"input_13", [1, 1]}, {"input_14", [1, 24]}, {"input_15", [1, 1]}, {"input_16", [1, 1]}, {"input_17", [1, 1]}}), ("RangeDims", {{"input_11", [[1, 768], [768, 768]]}, {"input_12", [[1, 768], [1, 1]]}, {"input_13", [[1, 768], [1, 1]]}, {"input_14", [[1, 768], [24, 24]]}, {"input_15", [[1, 768], [1, 1]]}, {"input_16", [[1, 768], [1, 1]]}, {"input_17", [[1, 768], [1, 1]]}})))] {
            tensor&lt;string, []&gt; input_11_dtype_0 = const()[name = tensor&lt;string, []&gt;("input_11_dtype_0"), val = tensor&lt;string, []&gt;("fp32")];
            tensor&lt;int32, []&gt; model_6_extend_layer_concat_axis = const()[name = tensor&lt;string, []&gt;("model_6_extend_layer_concat_axis"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, []&gt; model_6_extend_layer_concat_1_axis = const()[name = tensor&lt;string, []&gt;("model_6_extend_layer_concat_1_axis"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, []&gt; model_6_tf_math_equal_Equal_y = const()[name = tensor&lt;string, []&gt;("model_6_tf_math_equal_Equal_y"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, []&gt; model_6_tf_math_equal_1_Equal_y = const()[name = tensor&lt;string, []&gt;("model_6_tf_math_equal_1_Equal_y"), val = tensor&lt;int32, []&gt;(2)];
            tensor&lt;int32, [192]&gt; model_6_706 = const()[name = tensor&lt;string, []&gt;("model_6_706"), val = tensor&lt;int32, [192]&gt;([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])];
            tensor&lt;int32, []&gt; model_6_tf_concat_concat_axis = const()[name = tensor&lt;string, []&gt;("model_6_tf_concat_concat_axis"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, []&gt; model_6_extend_layer_concat_6_axis = const()[name = tensor&lt;string, []&gt;("model_6_extend_layer_concat_6_axis"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, []&gt; model_6_extend_layer_concat_5_axis = const()[name = tensor&lt;string, []&gt;("model_6_extend_layer_concat_5_axis"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, []&gt; model_6_extend_layer_concat_4_axis = const()[name = tensor&lt;string, []&gt;("model_6_extend_layer_concat_4_axis"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, []&gt; model_6_extend_layer_concat_3_axis = const()[name = tensor&lt;string, []&gt;("model_6_extend_layer_concat_3_axis"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, []&gt; model_6_extend_layer_concat_2_axis = const()[name = tensor&lt;string, []&gt;("model_6_extend_layer_concat_2_axis"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, [1, 1]&gt; model_6_extend_layer_zeros_6 = const()[name = tensor&lt;string, []&gt;("model_6_extend_layer_zeros_6"), val = tensor&lt;int32, [1, 1]&gt;([[0]])];
            tensor&lt;int32, [1, 1]&gt; model_6_extend_layer_zeros_2 = const()[name = tensor&lt;string, []&gt;("model_6_extend_layer_zeros_2"), val = tensor&lt;int32, [1, 1]&gt;([[0]])];
            tensor&lt;int32, [1, 24]&gt; model_6_extend_layer_zeros_4 = const()[name = tensor&lt;string, []&gt;("model_6_extend_layer_zeros_4"), val = tensor&lt;int32, [1, 24]&gt;([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])];
            tensor&lt;int32, [1, 1]&gt; model_6_extend_layer_zeros_5 = const()[name = tensor&lt;string, []&gt;("model_6_extend_layer_zeros_5"), val = tensor&lt;int32, [1, 1]&gt;([[0]])];
            tensor&lt;int32, [1, 1]&gt; model_6_extend_layer_zeros_1 = const()[name = tensor&lt;string, []&gt;("model_6_extend_layer_zeros_1"), val = tensor&lt;int32, [1, 1]&gt;([[0]])];
            tensor&lt;int32, [1, 1]&gt; model_6_extend_layer_zeros_3 = const()[name = tensor&lt;string, []&gt;("model_6_extend_layer_zeros_3"), val = tensor&lt;int32, [1, 1]&gt;([[0]])];
            tensor&lt;bool, []&gt; model_6_extend_layer_concat_6_interleave_0 = const()[name = tensor&lt;string, []&gt;("model_6_extend_layer_concat_6_interleave_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;int32, [?, 1]&gt; model_6_extend_layer_concat_6 = concat(axis = model_6_extend_layer_concat_6_axis, interleave = model_6_extend_layer_concat_6_interleave_0, values = (model_6_extend_layer_zeros_6, input_12));
            tensor&lt;bool, []&gt; model_6_extend_layer_concat_2_interleave_0 = const()[name = tensor&lt;string, []&gt;("model_6_extend_layer_concat_2_interleave_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;int32, [?, 1]&gt; model_6_extend_layer_concat_2 = concat(axis = model_6_extend_layer_concat_2_axis, interleave = model_6_extend_layer_concat_2_interleave_0, values = (model_6_extend_layer_zeros_2, input_17));
            tensor&lt;fp32, [1, 768]&gt; model_6_extend_layer_zeros = const()[name = tensor&lt;string, []&gt;("model_6_extend_layer_zeros"), val = tensor&lt;fp32, [1, 768]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(64)))];
            tensor&lt;bool, []&gt; model_6_extend_layer_concat_4_interleave_0 = const()[name = tensor&lt;string, []&gt;("model_6_extend_layer_concat_4_interleave_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;int32, [?, 24]&gt; model_6_extend_layer_concat_4 = concat(axis = model_6_extend_layer_concat_4_axis, interleave = model_6_extend_layer_concat_4_interleave_0, values = (model_6_extend_layer_zeros_4, input_14));
            tensor&lt;bool, []&gt; model_6_extend_layer_concat_5_interleave_0 = const()[name = tensor&lt;string, []&gt;("model_6_extend_layer_concat_5_interleave_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;int32, [?, 1]&gt; model_6_extend_layer_concat_5 = concat(axis = model_6_extend_layer_concat_5_axis, interleave = model_6_extend_layer_concat_5_interleave_0, values = (model_6_extend_layer_zeros_5, input_13));
            tensor&lt;bool, []&gt; model_6_extend_layer_concat_1_interleave_0 = const()[name = tensor&lt;string, []&gt;("model_6_extend_layer_concat_1_interleave_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;int32, [?, 1]&gt; model_6_extend_layer_concat_1 = concat(axis = model_6_extend_layer_concat_1_axis, interleave = model_6_extend_layer_concat_1_interleave_0, values = (model_6_extend_layer_zeros_1, input_15));
            tensor&lt;bool, []&gt; model_6_extend_layer_concat_3_interleave_0 = const()[name = tensor&lt;string, []&gt;("model_6_extend_layer_concat_3_interleave_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;int32, [?, 1]&gt; model_6_extend_layer_concat_3 = concat(axis = model_6_extend_layer_concat_3_axis, interleave = model_6_extend_layer_concat_3_interleave_0, values = (model_6_extend_layer_zeros_3, input_16));
            tensor&lt;bool, []&gt; model_6_extend_layer_concat_interleave_0 = const()[name = tensor&lt;string, []&gt;("model_6_extend_layer_concat_interleave_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp32, [?, 768]&gt; cast_1 = cast(dtype = input_11_dtype_0, x = input_11);
            tensor&lt;fp32, [?, 768]&gt; model_6_extend_layer_concat = concat(axis = model_6_extend_layer_concat_axis, interleave = model_6_extend_layer_concat_interleave_0, values = (model_6_extend_layer_zeros, cast_1));
            tensor&lt;int32, [2]&gt; model_6_tf___operators___getitem_2_strided_slice_begin_0 = const()[name = tensor&lt;string, []&gt;("model_6_tf___operators___getitem_2_strided_slice_begin_0"), val = tensor&lt;int32, [2]&gt;([0, 0])];
            tensor&lt;int32, [2]&gt; model_6_tf___operators___getitem_2_strided_slice_end_0 = const()[name = tensor&lt;string, []&gt;("model_6_tf___operators___getitem_2_strided_slice_end_0"), val = tensor&lt;int32, [2]&gt;([0, 1])];
            tensor&lt;int32, [2]&gt; model_6_tf___operators___getitem_2_strided_slice_stride_0 = const()[name = tensor&lt;string, []&gt;("model_6_tf___operators___getitem_2_strided_slice_stride_0"), val = tensor&lt;int32, [2]&gt;([1, 1])];
            tensor&lt;bool, [2]&gt; model_6_tf___operators___getitem_2_strided_slice_begin_mask_0 = const()[name = tensor&lt;string, []&gt;("model_6_tf___operators___getitem_2_strided_slice_begin_mask_0"), val = tensor&lt;bool, [2]&gt;([true, false])];
            tensor&lt;bool, [2]&gt; model_6_tf___operators___getitem_2_strided_slice_end_mask_0 = const()[name = tensor&lt;string, []&gt;("model_6_tf___operators___getitem_2_strided_slice_end_mask_0"), val = tensor&lt;bool, [2]&gt;([true, false])];
            tensor&lt;bool, [2]&gt; model_6_tf___operators___getitem_2_strided_slice_squeeze_mask_0 = const()[name = tensor&lt;string, []&gt;("model_6_tf___operators___getitem_2_strided_slice_squeeze_mask_0"), val = tensor&lt;bool, [2]&gt;([false, true])];
            tensor&lt;int32, [?]&gt; model_6_tf___operators___getitem_2_strided_slice = slice_by_index(begin = model_6_tf___operators___getitem_2_strided_slice_begin_0, begin_mask = model_6_tf___operators___getitem_2_strided_slice_begin_mask_0, end = model_6_tf___operators___getitem_2_strided_slice_end_0, end_mask = model_6_tf___operators___getitem_2_strided_slice_end_mask_0, squeeze_mask = model_6_tf___operators___getitem_2_strided_slice_squeeze_mask_0, stride = model_6_tf___operators___getitem_2_strided_slice_stride_0, x = model_6_extend_layer_concat_1);
            tensor&lt;bool, [?]&gt; model_6_tf_math_equal_Equal = equal(x = model_6_tf___operators___getitem_2_strided_slice, y = model_6_tf_math_equal_Equal_y);
            tensor&lt;bool, [?]&gt; model_6_tf_math_equal_1_Equal = equal(x = model_6_tf___operators___getitem_2_strided_slice, y = model_6_tf_math_equal_1_Equal_y);
            tensor&lt;bool, [?]&gt; model_6_tf_math_logical_or_LogicalOr = logical_or(x = model_6_tf_math_equal_Equal, y = model_6_tf_math_equal_1_Equal);
            tensor&lt;int32, [?, 1]&gt; model_6_tf_where_Where = non_zero(x = model_6_tf_math_logical_or_LogicalOr);
            tensor&lt;int32, [1]&gt; model_6_tf_compat_v1_squeeze_Squeeze_axes_0 = const()[name = tensor&lt;string, []&gt;("model_6_tf_compat_v1_squeeze_Squeeze_axes_0"), val = tensor&lt;int32, [1]&gt;([-1])];
            tensor&lt;int32, [?]&gt; model_6_tf_compat_v1_squeeze_Squeeze = squeeze(axes = model_6_tf_compat_v1_squeeze_Squeeze_axes_0, x = model_6_tf_where_Where);
            tensor&lt;bool, []&gt; model_6_tf_concat_concat_interleave_0 = const()[name = tensor&lt;string, []&gt;("model_6_tf_concat_concat_interleave_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;int32, [?]&gt; model_6_tf_concat_concat = concat(axis = model_6_tf_concat_concat_axis, interleave = model_6_tf_concat_concat_interleave_0, values = (model_6_tf_compat_v1_squeeze_Squeeze, model_6_706));
            tensor&lt;int32, [1]&gt; model_6_tf___operators___getitem_4_strided_slice_begin_0 = const()[name = tensor&lt;string, []&gt;("model_6_tf___operators___getitem_4_strided_slice_begin_0"), val = tensor&lt;int32, [1]&gt;([1])];
            tensor&lt;int32, [1]&gt; model_6_tf___operators___getitem_4_strided_slice_end_0 = const()[name = tensor&lt;string, []&gt;("model_6_tf___operators___getitem_4_strided_slice_end_0"), val = tensor&lt;int32, [1]&gt;([193])];
            tensor&lt;int32, [1]&gt; model_6_tf___operators___getitem_4_strided_slice_stride_0 = const()[name = tensor&lt;string, []&gt;("model_6_tf___operators___getitem_4_strided_slice_stride_0"), val = tensor&lt;int32, [1]&gt;([1])];
            tensor&lt;bool, [1]&gt; model_6_tf___operators___getitem_4_strided_slice_begin_mask_0 = const()[name = tensor&lt;string, []&gt;("model_6_tf___operators___getitem_4_strided_slice_begin_mask_0"), val = tensor&lt;bool, [1]&gt;([false])];
            tensor&lt;bool, [1]&gt; model_6_tf___operators___getitem_4_strided_slice_end_mask_0 = const()[name = tensor&lt;string, []&gt;("model_6_tf___operators___getitem_4_strided_slice_end_mask_0"), val = tensor&lt;bool, [1]&gt;([false])];
            tensor&lt;bool, [1]&gt; model_6_tf___operators___getitem_4_strided_slice_squeeze_mask_0 = const()[name = tensor&lt;string, []&gt;("model_6_tf___operators___getitem_4_strided_slice_squeeze_mask_0"), val = tensor&lt;bool, [1]&gt;([false])];
            tensor&lt;int32, [192]&gt; model_6_tf___operators___getitem_4_strided_slice = slice_by_index(begin = model_6_tf___operators___getitem_4_strided_slice_begin_0, begin_mask = model_6_tf___operators___getitem_4_strided_slice_begin_mask_0, end = model_6_tf___operators___getitem_4_strided_slice_end_0, end_mask = model_6_tf___operators___getitem_4_strided_slice_end_mask_0, squeeze_mask = model_6_tf___operators___getitem_4_strided_slice_squeeze_mask_0, stride = model_6_tf___operators___getitem_4_strided_slice_stride_0, x = model_6_tf_concat_concat);
            tensor&lt;int32, []&gt; model_6_tf_compat_v1_gather_2_GatherV2_axis_0 = const()[name = tensor&lt;string, []&gt;("model_6_tf_compat_v1_gather_2_GatherV2_axis_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, []&gt; model_6_tf_compat_v1_gather_2_GatherV2_batch_dims_0 = const()[name = tensor&lt;string, []&gt;("model_6_tf_compat_v1_gather_2_GatherV2_batch_dims_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;fp32, [192, 768]&gt; model_6_tf_compat_v1_gather_2_GatherV2 = gather(axis = model_6_tf_compat_v1_gather_2_GatherV2_axis_0, batch_dims = model_6_tf_compat_v1_gather_2_GatherV2_batch_dims_0, indices = model_6_tf___operators___getitem_4_strided_slice, x = model_6_extend_layer_concat);
            tensor&lt;int32, []&gt; model_6_tf_compat_v1_gather_3_GatherV2_axis_0 = const()[name = tensor&lt;string, []&gt;("model_6_tf_compat_v1_gather_3_GatherV2_axis_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, []&gt; model_6_tf_compat_v1_gather_3_GatherV2_batch_dims_0 = const()[name = tensor&lt;string, []&gt;("model_6_tf_compat_v1_gather_3_GatherV2_batch_dims_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, [192, 1]&gt; Identity_1 = gather(axis = model_6_tf_compat_v1_gather_3_GatherV2_axis_0, batch_dims = model_6_tf_compat_v1_gather_3_GatherV2_batch_dims_0, indices = model_6_tf___operators___getitem_4_strided_slice, x = model_6_extend_layer_concat_6);
            tensor&lt;int32, []&gt; model_6_tf_compat_v1_gather_4_GatherV2_axis_0 = const()[name = tensor&lt;string, []&gt;("model_6_tf_compat_v1_gather_4_GatherV2_axis_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, []&gt; model_6_tf_compat_v1_gather_4_GatherV2_batch_dims_0 = const()[name = tensor&lt;string, []&gt;("model_6_tf_compat_v1_gather_4_GatherV2_batch_dims_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, [192, 1]&gt; Identity_2 = gather(axis = model_6_tf_compat_v1_gather_4_GatherV2_axis_0, batch_dims = model_6_tf_compat_v1_gather_4_GatherV2_batch_dims_0, indices = model_6_tf___operators___getitem_4_strided_slice, x = model_6_extend_layer_concat_5);
            tensor&lt;int32, []&gt; model_6_tf_compat_v1_gather_5_GatherV2_axis_0 = const()[name = tensor&lt;string, []&gt;("model_6_tf_compat_v1_gather_5_GatherV2_axis_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, []&gt; model_6_tf_compat_v1_gather_5_GatherV2_batch_dims_0 = const()[name = tensor&lt;string, []&gt;("model_6_tf_compat_v1_gather_5_GatherV2_batch_dims_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, [192, 24]&gt; Identity_3 = gather(axis = model_6_tf_compat_v1_gather_5_GatherV2_axis_0, batch_dims = model_6_tf_compat_v1_gather_5_GatherV2_batch_dims_0, indices = model_6_tf___operators___getitem_4_strided_slice, x = model_6_extend_layer_concat_4);
            tensor&lt;int32, []&gt; model_6_tf_compat_v1_gather_6_GatherV2_axis_0 = const()[name = tensor&lt;string, []&gt;("model_6_tf_compat_v1_gather_6_GatherV2_axis_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, []&gt; model_6_tf_compat_v1_gather_6_GatherV2_batch_dims_0 = const()[name = tensor&lt;string, []&gt;("model_6_tf_compat_v1_gather_6_GatherV2_batch_dims_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, [192, 1]&gt; Identity_4 = gather(axis = model_6_tf_compat_v1_gather_6_GatherV2_axis_0, batch_dims = model_6_tf_compat_v1_gather_6_GatherV2_batch_dims_0, indices = model_6_tf___operators___getitem_4_strided_slice, x = model_6_extend_layer_concat_3);
            tensor&lt;int32, []&gt; model_6_tf_compat_v1_gather_7_GatherV2_axis_0 = const()[name = tensor&lt;string, []&gt;("model_6_tf_compat_v1_gather_7_GatherV2_axis_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, []&gt; model_6_tf_compat_v1_gather_7_GatherV2_batch_dims_0 = const()[name = tensor&lt;string, []&gt;("model_6_tf_compat_v1_gather_7_GatherV2_batch_dims_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, [192, 1]&gt; Identity_5 = gather(axis = model_6_tf_compat_v1_gather_7_GatherV2_axis_0, batch_dims = model_6_tf_compat_v1_gather_7_GatherV2_batch_dims_0, indices = model_6_tf___operators___getitem_4_strided_slice, x = model_6_extend_layer_concat_2);
            tensor&lt;string, []&gt; model_6_tf_cast_2_Cast_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_6_tf_cast_2_Cast_dtype_0"), val = tensor&lt;string, []&gt;("fp16")];
            tensor&lt;fp16, [192, 768]&gt; Identity = cast(dtype = model_6_tf_cast_2_Cast_dtype_0, x = model_6_tf_compat_v1_gather_2_GatherV2);
        } -&gt; (Identity, Identity_1, Identity_2, Identity_3, Identity_4, Identity_5);
}</code></pre>
                    </div>
                </div>
            
                <div class="file-section" id="file-8">
                    <div class="file-header">
                        <h2>ðŸ“„ model.mil 21</h2>
                        <div class="file-meta">Size: 7.02 KB</div>
                    </div>
                    <div class="file-content">
                        <pre><code>program(1.0)
[buildInfo = dict&lt;tensor&lt;string, []&gt;, tensor&lt;string, []&gt;&gt;({{"coremlc-component-MIL", "3405.2.1"}, {"coremlc-version", "3405.2.1"}})]
{
    func main&lt;ios16&gt;(tensor&lt;fp32, [?, 192, 1997]&gt; input_1, tensor&lt;fp32, [?, 192, 14215]&gt; input_2) [FlexibleShapeInformation = tuple&lt;tuple&lt;tensor&lt;string, []&gt;, dict&lt;tensor&lt;string, []&gt;, tensor&lt;int32, [?]&gt;&gt;&gt;, tuple&lt;tensor&lt;string, []&gt;, dict&lt;tensor&lt;string, []&gt;, list&lt;tensor&lt;int32, [2]&gt;, ?&gt;&gt;&gt;&gt;((("DefaultShapes", {{"input_1", [1, 192, 1997]}, {"input_2", [1, 192, 14215]}}), ("RangeDims", {{"input_1", [[1, -1], [192, 192], [1997, 1997]]}, {"input_2", [[1, -1], [192, 192], [14215, 14215]]}})))] {
            tensor&lt;int32, [3]&gt; model_tf___operators___getitem_strided_slice_begin_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_strided_slice_begin_0"), val = tensor&lt;int32, [3]&gt;([0, 0, 0])];
            tensor&lt;int32, [3]&gt; model_tf___operators___getitem_strided_slice_end_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_strided_slice_end_0"), val = tensor&lt;int32, [3]&gt;([0, 1, 0])];
            tensor&lt;int32, [3]&gt; model_tf___operators___getitem_strided_slice_stride_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_strided_slice_stride_0"), val = tensor&lt;int32, [3]&gt;([1, 1, 1])];
            tensor&lt;bool, [3]&gt; model_tf___operators___getitem_strided_slice_begin_mask_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_strided_slice_begin_mask_0"), val = tensor&lt;bool, [3]&gt;([true, false, true])];
            tensor&lt;bool, [3]&gt; model_tf___operators___getitem_strided_slice_end_mask_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_strided_slice_end_mask_0"), val = tensor&lt;bool, [3]&gt;([true, false, true])];
            tensor&lt;bool, [3]&gt; model_tf___operators___getitem_strided_slice_squeeze_mask_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_strided_slice_squeeze_mask_0"), val = tensor&lt;bool, [3]&gt;([false, true, false])];
            tensor&lt;string, []&gt; input_1_to_fp16_dtype_0 = const()[name = tensor&lt;string, []&gt;("input_1_to_fp16_dtype_0"), val = tensor&lt;string, []&gt;("fp16")];
            tensor&lt;fp16, [?, 192, 1997]&gt; cast_7 = cast(dtype = input_1_to_fp16_dtype_0, x = input_1);
            tensor&lt;fp16, [?, 1997]&gt; model_tf___operators___getitem_strided_slice_cast = slice_by_index(begin = model_tf___operators___getitem_strided_slice_begin_0, begin_mask = model_tf___operators___getitem_strided_slice_begin_mask_0, end = model_tf___operators___getitem_strided_slice_end_0, end_mask = model_tf___operators___getitem_strided_slice_end_mask_0, squeeze_mask = model_tf___operators___getitem_strided_slice_squeeze_mask_0, stride = model_tf___operators___getitem_strided_slice_stride_0, x = cast_7);
            tensor&lt;int32, [3]&gt; model_tf___operators___getitem_1_strided_slice_begin_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_1_strided_slice_begin_0"), val = tensor&lt;int32, [3]&gt;([0, 0, 0])];
            tensor&lt;int32, [3]&gt; model_tf___operators___getitem_1_strided_slice_end_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_1_strided_slice_end_0"), val = tensor&lt;int32, [3]&gt;([0, 1, 0])];
            tensor&lt;int32, [3]&gt; model_tf___operators___getitem_1_strided_slice_stride_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_1_strided_slice_stride_0"), val = tensor&lt;int32, [3]&gt;([1, 1, 1])];
            tensor&lt;bool, [3]&gt; model_tf___operators___getitem_1_strided_slice_begin_mask_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_1_strided_slice_begin_mask_0"), val = tensor&lt;bool, [3]&gt;([true, false, true])];
            tensor&lt;bool, [3]&gt; model_tf___operators___getitem_1_strided_slice_end_mask_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_1_strided_slice_end_mask_0"), val = tensor&lt;bool, [3]&gt;([true, false, true])];
            tensor&lt;bool, [3]&gt; model_tf___operators___getitem_1_strided_slice_squeeze_mask_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_1_strided_slice_squeeze_mask_0"), val = tensor&lt;bool, [3]&gt;([false, true, false])];
            tensor&lt;string, []&gt; input_2_to_fp16_dtype_0 = const()[name = tensor&lt;string, []&gt;("input_2_to_fp16_dtype_0"), val = tensor&lt;string, []&gt;("fp16")];
            tensor&lt;fp16, [?, 192, 14215]&gt; cast_5 = cast(dtype = input_2_to_fp16_dtype_0, x = input_2);
            tensor&lt;fp16, [?, 14215]&gt; model_tf___operators___getitem_1_strided_slice_cast = slice_by_index(begin = model_tf___operators___getitem_1_strided_slice_begin_0, begin_mask = model_tf___operators___getitem_1_strided_slice_begin_mask_0, end = model_tf___operators___getitem_1_strided_slice_end_0, end_mask = model_tf___operators___getitem_1_strided_slice_end_mask_0, squeeze_mask = model_tf___operators___getitem_1_strided_slice_squeeze_mask_0, stride = model_tf___operators___getitem_1_strided_slice_stride_0, x = cast_5);
            tensor&lt;int32, []&gt; model_softmax_Softmax_axis_0 = const()[name = tensor&lt;string, []&gt;("model_softmax_Softmax_axis_0"), val = tensor&lt;int32, []&gt;(-1)];
            tensor&lt;fp16, [?, 1997]&gt; model_softmax_Softmax_cast = softmax(axis = model_softmax_Softmax_axis_0, x = model_tf___operators___getitem_strided_slice_cast);
            tensor&lt;int32, []&gt; model_softmax_1_Softmax_axis_0 = const()[name = tensor&lt;string, []&gt;("model_softmax_1_Softmax_axis_0"), val = tensor&lt;int32, []&gt;(-1)];
            tensor&lt;fp16, [?, 14215]&gt; model_softmax_1_Softmax_cast = softmax(axis = model_softmax_1_Softmax_axis_0, x = model_tf___operators___getitem_1_strided_slice_cast);
            tensor&lt;fp16, [1997]&gt; model_tf_math_multiply_Mul_y_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_tf_math_multiply_Mul_y_to_fp16"), val = tensor&lt;fp16, [1997]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(64)))];
            tensor&lt;fp16, [?, 1997]&gt; model_tf_math_multiply_Mul_cast = mul(x = model_softmax_Softmax_cast, y = model_tf_math_multiply_Mul_y_to_fp16);
            tensor&lt;string, []&gt; model_tf_math_multiply_Mul_cast_to_fp32_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_tf_math_multiply_Mul_cast_to_fp32_dtype_0"), val = tensor&lt;string, []&gt;("fp32")];
            tensor&lt;fp16, [14215]&gt; model_tf_math_multiply_1_Mul_y_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_tf_math_multiply_1_Mul_y_to_fp16"), val = tensor&lt;fp16, [14215]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4160)))];
            tensor&lt;fp16, [?, 14215]&gt; model_tf_math_multiply_1_Mul_cast = mul(x = model_softmax_1_Softmax_cast, y = model_tf_math_multiply_1_Mul_y_to_fp16);
            tensor&lt;string, []&gt; model_tf_math_multiply_1_Mul_cast_to_fp32_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_tf_math_multiply_1_Mul_cast_to_fp32_dtype_0"), val = tensor&lt;string, []&gt;("fp32")];
            tensor&lt;fp32, [?, 14215]&gt; Identity_1 = cast(dtype = model_tf_math_multiply_1_Mul_cast_to_fp32_dtype_0, x = model_tf_math_multiply_1_Mul_cast);
            tensor&lt;fp32, [?, 1997]&gt; Identity = cast(dtype = model_tf_math_multiply_Mul_cast_to_fp32_dtype_0, x = model_tf_math_multiply_Mul_cast);
        } -&gt; (Identity, Identity_1);
}</code></pre>
                    </div>
                </div>
            
                <div class="file-section" id="file-9">
                    <div class="file-header">
                        <h2>ðŸ“„ model.mil 26</h2>
                        <div class="file-meta">Size: 11.35 KB</div>
                    </div>
                    <div class="file-content">
                        <pre><code>program(1.0)
[buildInfo = dict&lt;tensor&lt;string, []&gt;, tensor&lt;string, []&gt;&gt;({{"coremlc-component-MIL", "3404.16.1"}, {"coremlc-version", "3404.24.1"}})]
{
    func main&lt;ios16&gt;(tensor&lt;int32, [1, 192, 1]&gt; input_4, tensor&lt;int32, [1, 192, 24]&gt; input_8) {
            tensor&lt;int32, []&gt; model_1_tf_math_count_nonzero_count_nonzero_zeros = const()[name = tensor&lt;string, []&gt;("model_1_tf_math_count_nonzero_count_nonzero_zeros"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, []&gt; model_1_embedding_embedding_lookup_axis_0 = const()[name = tensor&lt;string, []&gt;("model_1_embedding_embedding_lookup_axis_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, []&gt; model_1_embedding_embedding_lookup_batch_dims_0 = const()[name = tensor&lt;string, []&gt;("model_1_embedding_embedding_lookup_batch_dims_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;fp16, [2364, 64]&gt; model_1_embedding_embedding_lookup_757_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_embedding_embedding_lookup_757_to_fp16"), val = tensor&lt;fp16, [2364, 64]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(64)))];
            tensor&lt;fp16, [1, 192, 1, 64]&gt; model_1_embedding_embedding_lookup_cast = gather(axis = model_1_embedding_embedding_lookup_axis_0, batch_dims = model_1_embedding_embedding_lookup_batch_dims_0, indices = input_4, x = model_1_embedding_embedding_lookup_757_to_fp16);
            tensor&lt;int32, []&gt; model_1_embedding_1_embedding_lookup_axis_0 = const()[name = tensor&lt;string, []&gt;("model_1_embedding_1_embedding_lookup_axis_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, []&gt; model_1_embedding_1_embedding_lookup_batch_dims_0 = const()[name = tensor&lt;string, []&gt;("model_1_embedding_1_embedding_lookup_batch_dims_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;fp16, [11581, 64]&gt; model_1_embedding_1_embedding_lookup_748_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_embedding_1_embedding_lookup_748_to_fp16"), val = tensor&lt;fp16, [11581, 64]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(302720)))];
            tensor&lt;fp16, [1, 192, 24, 64]&gt; model_1_embedding_1_embedding_lookup_cast = gather(axis = model_1_embedding_1_embedding_lookup_axis_0, batch_dims = model_1_embedding_1_embedding_lookup_batch_dims_0, indices = input_8, x = model_1_embedding_1_embedding_lookup_748_to_fp16);
            tensor&lt;bool, [1, 192, 1]&gt; model_1_tf_math_count_nonzero_count_nonzero_NotEqual = not_equal(x = input_4, y = model_1_tf_math_count_nonzero_count_nonzero_zeros);
            tensor&lt;string, []&gt; model_1_tf_math_count_nonzero_count_nonzero_Cast_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_1_tf_math_count_nonzero_count_nonzero_Cast_dtype_0"), val = tensor&lt;string, []&gt;("int32")];
            tensor&lt;int32, [1]&gt; model_1_tf_math_count_nonzero_count_nonzero_Sum_axes_0 = const()[name = tensor&lt;string, []&gt;("model_1_tf_math_count_nonzero_count_nonzero_Sum_axes_0"), val = tensor&lt;int32, [1]&gt;([1])];
            tensor&lt;bool, []&gt; model_1_tf_math_count_nonzero_count_nonzero_Sum_keep_dims_0 = const()[name = tensor&lt;string, []&gt;("model_1_tf_math_count_nonzero_count_nonzero_Sum_keep_dims_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;int32, [1, 192, 1]&gt; cast_19 = cast(dtype = model_1_tf_math_count_nonzero_count_nonzero_Cast_dtype_0, x = model_1_tf_math_count_nonzero_count_nonzero_NotEqual);
            tensor&lt;int32, [1, 1]&gt; model_1_tf_math_count_nonzero_count_nonzero_Sum = reduce_sum(axes = model_1_tf_math_count_nonzero_count_nonzero_Sum_axes_0, keep_dims = model_1_tf_math_count_nonzero_count_nonzero_Sum_keep_dims_0, x = cast_19);
            tensor&lt;int32, [1]&gt; model_1_tf_math_reduce_mean_Mean_axes_0 = const()[name = tensor&lt;string, []&gt;("model_1_tf_math_reduce_mean_Mean_axes_0"), val = tensor&lt;int32, [1]&gt;([1])];
            tensor&lt;bool, []&gt; model_1_tf_math_reduce_mean_Mean_keep_dims_0 = const()[name = tensor&lt;string, []&gt;("model_1_tf_math_reduce_mean_Mean_keep_dims_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [1, 1, 64]&gt; model_1_tf_math_reduce_mean_Mean_cast = reduce_mean(axes = model_1_tf_math_reduce_mean_Mean_axes_0, keep_dims = model_1_tf_math_reduce_mean_Mean_keep_dims_0, x = model_1_embedding_embedding_lookup_cast);
            tensor&lt;int32, [1]&gt; model_1_tf_math_reduce_mean_1_Mean_axes_0 = const()[name = tensor&lt;string, []&gt;("model_1_tf_math_reduce_mean_1_Mean_axes_0"), val = tensor&lt;int32, [1]&gt;([1])];
            tensor&lt;bool, []&gt; model_1_tf_math_reduce_mean_1_Mean_keep_dims_0 = const()[name = tensor&lt;string, []&gt;("model_1_tf_math_reduce_mean_1_Mean_keep_dims_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [1, 24, 64]&gt; model_1_tf_math_reduce_mean_1_Mean_cast = reduce_mean(axes = model_1_tf_math_reduce_mean_1_Mean_axes_0, keep_dims = model_1_tf_math_reduce_mean_1_Mean_keep_dims_0, x = model_1_embedding_1_embedding_lookup_cast);
            tensor&lt;fp16, [1, 1, 64]&gt; model_1_tf_math_l2_normalize_l2_normalize_Square_cast = mul(x = model_1_tf_math_reduce_mean_Mean_cast, y = model_1_tf_math_reduce_mean_Mean_cast);
            tensor&lt;int32, [1]&gt; model_1_tf_math_reduce_mean_2_Mean_axes_0 = const()[name = tensor&lt;string, []&gt;("model_1_tf_math_reduce_mean_2_Mean_axes_0"), val = tensor&lt;int32, [1]&gt;([1])];
            tensor&lt;bool, []&gt; model_1_tf_math_reduce_mean_2_Mean_keep_dims_0 = const()[name = tensor&lt;string, []&gt;("model_1_tf_math_reduce_mean_2_Mean_keep_dims_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [1, 64]&gt; model_1_tf_math_reduce_mean_2_Mean_cast = reduce_mean(axes = model_1_tf_math_reduce_mean_2_Mean_axes_0, keep_dims = model_1_tf_math_reduce_mean_2_Mean_keep_dims_0, x = model_1_tf_math_reduce_mean_1_Mean_cast);
            tensor&lt;string, []&gt; model_1_tf_cast_1_Cast_to_fp16_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_1_tf_cast_1_Cast_to_fp16_dtype_0"), val = tensor&lt;string, []&gt;("fp16")];
            tensor&lt;fp16, []&gt; _inversed_model_1_tf_math_truediv_truediv_y_0_to_fp16 = const()[name = tensor&lt;string, []&gt;("_inversed_model_1_tf_math_truediv_truediv_y_0_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1.554p-8)];
            tensor&lt;fp16, [1, 1]&gt; cast_13 = cast(dtype = model_1_tf_cast_1_Cast_to_fp16_dtype_0, x = model_1_tf_math_count_nonzero_count_nonzero_Sum);
            tensor&lt;fp16, [1, 1]&gt; _inversed_model_1_tf_math_truediv_truediv_cast = mul(x = cast_13, y = _inversed_model_1_tf_math_truediv_truediv_y_0_to_fp16);
            tensor&lt;int32, [1]&gt; model_1_tf_math_l2_normalize_l2_normalize_Sum_axes_0 = const()[name = tensor&lt;string, []&gt;("model_1_tf_math_l2_normalize_l2_normalize_Sum_axes_0"), val = tensor&lt;int32, [1]&gt;([-1])];
            tensor&lt;bool, []&gt; model_1_tf_math_l2_normalize_l2_normalize_Sum_keep_dims_0 = const()[name = tensor&lt;string, []&gt;("model_1_tf_math_l2_normalize_l2_normalize_Sum_keep_dims_0"), val = tensor&lt;bool, []&gt;(true)];
            tensor&lt;fp16, [1, 1, 1]&gt; model_1_tf_math_l2_normalize_l2_normalize_Sum_cast = reduce_sum(axes = model_1_tf_math_l2_normalize_l2_normalize_Sum_axes_0, keep_dims = model_1_tf_math_l2_normalize_l2_normalize_Sum_keep_dims_0, x = model_1_tf_math_l2_normalize_l2_normalize_Square_cast);
            tensor&lt;fp16, [1, 64]&gt; model_1_tf_math_l2_normalize_2_l2_normalize_Square_cast = mul(x = model_1_tf_math_reduce_mean_2_Mean_cast, y = model_1_tf_math_reduce_mean_2_Mean_cast);
            tensor&lt;fp16, [1, 1]&gt; model_1_tf_math_sqrt_Sqrt_cast = sqrt(x = _inversed_model_1_tf_math_truediv_truediv_cast);
            tensor&lt;string, []&gt; model_1_tf_math_sqrt_Sqrt_cast_to_fp32_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_1_tf_math_sqrt_Sqrt_cast_to_fp32_dtype_0"), val = tensor&lt;string, []&gt;("fp32")];
            tensor&lt;fp16, []&gt; model_1_tf_math_l2_normalize_l2_normalize_Maximum_y_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_tf_math_l2_normalize_l2_normalize_Maximum_y_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1p-24)];
            tensor&lt;fp16, [1, 1, 1]&gt; model_1_tf_math_l2_normalize_l2_normalize_Maximum_cast = maximum(x = model_1_tf_math_l2_normalize_l2_normalize_Sum_cast, y = model_1_tf_math_l2_normalize_l2_normalize_Maximum_y_to_fp16);
            tensor&lt;int32, [1]&gt; model_1_tf_math_l2_normalize_2_l2_normalize_Sum_axes_0 = const()[name = tensor&lt;string, []&gt;("model_1_tf_math_l2_normalize_2_l2_normalize_Sum_axes_0"), val = tensor&lt;int32, [1]&gt;([-1])];
            tensor&lt;bool, []&gt; model_1_tf_math_l2_normalize_2_l2_normalize_Sum_keep_dims_0 = const()[name = tensor&lt;string, []&gt;("model_1_tf_math_l2_normalize_2_l2_normalize_Sum_keep_dims_0"), val = tensor&lt;bool, []&gt;(true)];
            tensor&lt;fp16, [1, 1]&gt; model_1_tf_math_l2_normalize_2_l2_normalize_Sum_cast = reduce_sum(axes = model_1_tf_math_l2_normalize_2_l2_normalize_Sum_axes_0, keep_dims = model_1_tf_math_l2_normalize_2_l2_normalize_Sum_keep_dims_0, x = model_1_tf_math_l2_normalize_2_l2_normalize_Square_cast);
            tensor&lt;fp16, []&gt; model_1_tf_math_l2_normalize_l2_normalize_Rsqrt_epsilon_0_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_tf_math_l2_normalize_l2_normalize_Rsqrt_epsilon_0_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1p-24)];
            tensor&lt;fp16, [1, 1, 1]&gt; model_1_tf_math_l2_normalize_l2_normalize_Rsqrt_cast = rsqrt(epsilon = model_1_tf_math_l2_normalize_l2_normalize_Rsqrt_epsilon_0_to_fp16, x = model_1_tf_math_l2_normalize_l2_normalize_Maximum_cast);
            tensor&lt;fp16, []&gt; model_1_tf_math_l2_normalize_2_l2_normalize_Maximum_y_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_tf_math_l2_normalize_2_l2_normalize_Maximum_y_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1p-24)];
            tensor&lt;fp16, [1, 1]&gt; model_1_tf_math_l2_normalize_2_l2_normalize_Maximum_cast = maximum(x = model_1_tf_math_l2_normalize_2_l2_normalize_Sum_cast, y = model_1_tf_math_l2_normalize_2_l2_normalize_Maximum_y_to_fp16);
            tensor&lt;fp16, [1, 1, 64]&gt; model_1_tf_math_l2_normalize_l2_normalize_cast = mul(x = model_1_tf_math_reduce_mean_Mean_cast, y = model_1_tf_math_l2_normalize_l2_normalize_Rsqrt_cast);
            tensor&lt;string, []&gt; model_1_tf_math_l2_normalize_l2_normalize_cast_to_fp32_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_1_tf_math_l2_normalize_l2_normalize_cast_to_fp32_dtype_0"), val = tensor&lt;string, []&gt;("fp32")];
            tensor&lt;fp16, []&gt; model_1_tf_math_l2_normalize_2_l2_normalize_Rsqrt_epsilon_0_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_tf_math_l2_normalize_2_l2_normalize_Rsqrt_epsilon_0_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1p-24)];
            tensor&lt;fp16, [1, 1]&gt; model_1_tf_math_l2_normalize_2_l2_normalize_Rsqrt_cast = rsqrt(epsilon = model_1_tf_math_l2_normalize_2_l2_normalize_Rsqrt_epsilon_0_to_fp16, x = model_1_tf_math_l2_normalize_2_l2_normalize_Maximum_cast);
            tensor&lt;fp16, [1, 64]&gt; model_1_tf_math_l2_normalize_2_l2_normalize_cast = mul(x = model_1_tf_math_reduce_mean_2_Mean_cast, y = model_1_tf_math_l2_normalize_2_l2_normalize_Rsqrt_cast);
            tensor&lt;string, []&gt; model_1_tf_math_l2_normalize_2_l2_normalize_cast_to_fp32_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_1_tf_math_l2_normalize_2_l2_normalize_cast_to_fp32_dtype_0"), val = tensor&lt;string, []&gt;("fp32")];
            tensor&lt;fp32, [1, 64]&gt; Identity_1 = cast(dtype = model_1_tf_math_l2_normalize_2_l2_normalize_cast_to_fp32_dtype_0, x = model_1_tf_math_l2_normalize_2_l2_normalize_cast);
            tensor&lt;fp32, [1, 1, 64]&gt; Identity = cast(dtype = model_1_tf_math_l2_normalize_l2_normalize_cast_to_fp32_dtype_0, x = model_1_tf_math_l2_normalize_l2_normalize_cast);
            tensor&lt;fp32, [1, 1]&gt; Identity_2 = cast(dtype = model_1_tf_math_sqrt_Sqrt_cast_to_fp32_dtype_0, x = model_1_tf_math_sqrt_Sqrt_cast);
        } -&gt; (Identity, Identity_1, Identity_2);
}</code></pre>
                    </div>
                </div>
            
                <div class="file-section" id="file-10">
                    <div class="file-header">
                        <h2>ðŸ“„ model.mil 12</h2>
                        <div class="file-meta">Size: 10.2 KB</div>
                    </div>
                    <div class="file-content">
                        <pre><code>program(1.0)
[buildInfo = dict&lt;tensor&lt;string, []&gt;, tensor&lt;string, []&gt;&gt;({{"coremlc-component-MIL", "3404.16.1"}, {"coremlc-version", "3404.24.1"}})]
{
    func main&lt;ios16&gt;(tensor&lt;int32, [1, 384, 1]&gt; input_1, tensor&lt;int32, [1, 384, 24]&gt; input_2, tensor&lt;int32, [1, 384, 1]&gt; input_3) {
            tensor&lt;int32, []&gt; model_extend_layer_concat_axis = const()[name = tensor&lt;string, []&gt;("model_extend_layer_concat_axis"), val = tensor&lt;int32, []&gt;(1)];
            tensor&lt;int32, []&gt; model_extend_layer_concat_1_axis = const()[name = tensor&lt;string, []&gt;("model_extend_layer_concat_1_axis"), val = tensor&lt;int32, []&gt;(1)];
            tensor&lt;int32, []&gt; model_tf_math_equal_Equal_y = const()[name = tensor&lt;string, []&gt;("model_tf_math_equal_Equal_y"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, []&gt; model_tf_math_equal_1_Equal_y = const()[name = tensor&lt;string, []&gt;("model_tf_math_equal_1_Equal_y"), val = tensor&lt;int32, []&gt;(2)];
            tensor&lt;int32, [192]&gt; model_178 = const()[name = tensor&lt;string, []&gt;("model_178"), val = tensor&lt;int32, [192]&gt;([384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384])];
            tensor&lt;int32, []&gt; model_tf_concat_concat_axis = const()[name = tensor&lt;string, []&gt;("model_tf_concat_concat_axis"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, []&gt; model_extend_layer_concat_2_axis = const()[name = tensor&lt;string, []&gt;("model_extend_layer_concat_2_axis"), val = tensor&lt;int32, []&gt;(1)];
            tensor&lt;int32, [1, 1, 1]&gt; model_extend_layer_zeros = const()[name = tensor&lt;string, []&gt;("model_extend_layer_zeros"), val = tensor&lt;int32, [1, 1, 1]&gt;([[[0]]])];
            tensor&lt;int32, [1, 1, 1]&gt; model_extend_layer_zeros_1 = const()[name = tensor&lt;string, []&gt;("model_extend_layer_zeros_1"), val = tensor&lt;int32, [1, 1, 1]&gt;([[[0]]])];
            tensor&lt;int32, [1, 1, 24]&gt; model_extend_layer_zeros_2 = const()[name = tensor&lt;string, []&gt;("model_extend_layer_zeros_2"), val = tensor&lt;int32, [1, 1, 24]&gt;([[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]])];
            tensor&lt;bool, []&gt; model_extend_layer_concat_interleave_0 = const()[name = tensor&lt;string, []&gt;("model_extend_layer_concat_interleave_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;int32, [1, 385, 1]&gt; model_extend_layer_concat = concat(axis = model_extend_layer_concat_axis, interleave = model_extend_layer_concat_interleave_0, values = (input_1, model_extend_layer_zeros));
            tensor&lt;bool, []&gt; model_extend_layer_concat_1_interleave_0 = const()[name = tensor&lt;string, []&gt;("model_extend_layer_concat_1_interleave_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;int32, [1, 385, 1]&gt; model_extend_layer_concat_1 = concat(axis = model_extend_layer_concat_1_axis, interleave = model_extend_layer_concat_1_interleave_0, values = (input_3, model_extend_layer_zeros_1));
            tensor&lt;bool, []&gt; model_extend_layer_concat_2_interleave_0 = const()[name = tensor&lt;string, []&gt;("model_extend_layer_concat_2_interleave_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;int32, [1, 385, 24]&gt; model_extend_layer_concat_2 = concat(axis = model_extend_layer_concat_2_axis, interleave = model_extend_layer_concat_2_interleave_0, values = (input_2, model_extend_layer_zeros_2));
            tensor&lt;int32, [3]&gt; model_tf___operators___getitem_strided_slice_begin_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_strided_slice_begin_0"), val = tensor&lt;int32, [3]&gt;([0, 0, 0])];
            tensor&lt;int32, [3]&gt; model_tf___operators___getitem_strided_slice_end_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_strided_slice_end_0"), val = tensor&lt;int32, [3]&gt;([1, 0, 1])];
            tensor&lt;int32, [3]&gt; model_tf___operators___getitem_strided_slice_stride_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_strided_slice_stride_0"), val = tensor&lt;int32, [3]&gt;([1, 1, 1])];
            tensor&lt;bool, [3]&gt; model_tf___operators___getitem_strided_slice_begin_mask_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_strided_slice_begin_mask_0"), val = tensor&lt;bool, [3]&gt;([false, true, false])];
            tensor&lt;bool, [3]&gt; model_tf___operators___getitem_strided_slice_end_mask_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_strided_slice_end_mask_0"), val = tensor&lt;bool, [3]&gt;([false, true, false])];
            tensor&lt;bool, [3]&gt; model_tf___operators___getitem_strided_slice_squeeze_mask_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_strided_slice_squeeze_mask_0"), val = tensor&lt;bool, [3]&gt;([true, false, true])];
            tensor&lt;int32, [385]&gt; model_tf___operators___getitem_strided_slice = slice_by_index(begin = model_tf___operators___getitem_strided_slice_begin_0, begin_mask = model_tf___operators___getitem_strided_slice_begin_mask_0, end = model_tf___operators___getitem_strided_slice_end_0, end_mask = model_tf___operators___getitem_strided_slice_end_mask_0, squeeze_mask = model_tf___operators___getitem_strided_slice_squeeze_mask_0, stride = model_tf___operators___getitem_strided_slice_stride_0, x = model_extend_layer_concat_1);
            tensor&lt;bool, [385]&gt; model_tf_math_equal_Equal = equal(x = model_tf___operators___getitem_strided_slice, y = model_tf_math_equal_Equal_y);
            tensor&lt;bool, [385]&gt; model_tf_math_equal_1_Equal = equal(x = model_tf___operators___getitem_strided_slice, y = model_tf_math_equal_1_Equal_y);
            tensor&lt;bool, [385]&gt; model_tf_math_logical_or_LogicalOr = logical_or(x = model_tf_math_equal_Equal, y = model_tf_math_equal_1_Equal);
            tensor&lt;int32, [?, 1]&gt; model_tf_where_Where = non_zero(x = model_tf_math_logical_or_LogicalOr);
            tensor&lt;int32, [1]&gt; model_tf_compat_v1_squeeze_Squeeze_axes_0 = const()[name = tensor&lt;string, []&gt;("model_tf_compat_v1_squeeze_Squeeze_axes_0"), val = tensor&lt;int32, [1]&gt;([-1])];
            tensor&lt;int32, [?]&gt; model_tf_compat_v1_squeeze_Squeeze = squeeze(axes = model_tf_compat_v1_squeeze_Squeeze_axes_0, x = model_tf_where_Where);
            tensor&lt;bool, []&gt; model_tf_concat_concat_interleave_0 = const()[name = tensor&lt;string, []&gt;("model_tf_concat_concat_interleave_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;int32, [?]&gt; model_tf_concat_concat = concat(axis = model_tf_concat_concat_axis, interleave = model_tf_concat_concat_interleave_0, values = (model_tf_compat_v1_squeeze_Squeeze, model_178));
            tensor&lt;int32, [1]&gt; model_tf___operators___getitem_2_strided_slice_begin_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_2_strided_slice_begin_0"), val = tensor&lt;int32, [1]&gt;([0])];
            tensor&lt;int32, [1]&gt; model_tf___operators___getitem_2_strided_slice_end_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_2_strided_slice_end_0"), val = tensor&lt;int32, [1]&gt;([192])];
            tensor&lt;int32, [1]&gt; model_tf___operators___getitem_2_strided_slice_stride_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_2_strided_slice_stride_0"), val = tensor&lt;int32, [1]&gt;([1])];
            tensor&lt;bool, [1]&gt; model_tf___operators___getitem_2_strided_slice_begin_mask_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_2_strided_slice_begin_mask_0"), val = tensor&lt;bool, [1]&gt;([true])];
            tensor&lt;bool, [1]&gt; model_tf___operators___getitem_2_strided_slice_end_mask_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_2_strided_slice_end_mask_0"), val = tensor&lt;bool, [1]&gt;([false])];
            tensor&lt;bool, [1]&gt; model_tf___operators___getitem_2_strided_slice_squeeze_mask_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_2_strided_slice_squeeze_mask_0"), val = tensor&lt;bool, [1]&gt;([false])];
            tensor&lt;int32, [192]&gt; model_tf___operators___getitem_2_strided_slice = slice_by_index(begin = model_tf___operators___getitem_2_strided_slice_begin_0, begin_mask = model_tf___operators___getitem_2_strided_slice_begin_mask_0, end = model_tf___operators___getitem_2_strided_slice_end_0, end_mask = model_tf___operators___getitem_2_strided_slice_end_mask_0, squeeze_mask = model_tf___operators___getitem_2_strided_slice_squeeze_mask_0, stride = model_tf___operators___getitem_2_strided_slice_stride_0, x = model_tf_concat_concat);
            tensor&lt;int32, []&gt; model_tf_compat_v1_gather_GatherV2_axis_0 = const()[name = tensor&lt;string, []&gt;("model_tf_compat_v1_gather_GatherV2_axis_0"), val = tensor&lt;int32, []&gt;(1)];
            tensor&lt;int32, []&gt; model_tf_compat_v1_gather_GatherV2_batch_dims_0 = const()[name = tensor&lt;string, []&gt;("model_tf_compat_v1_gather_GatherV2_batch_dims_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, [1, 192, 1]&gt; Identity = gather(axis = model_tf_compat_v1_gather_GatherV2_axis_0, batch_dims = model_tf_compat_v1_gather_GatherV2_batch_dims_0, indices = model_tf___operators___getitem_2_strided_slice, x = model_extend_layer_concat);
            tensor&lt;int32, []&gt; model_tf_compat_v1_gather_1_GatherV2_axis_0 = const()[name = tensor&lt;string, []&gt;("model_tf_compat_v1_gather_1_GatherV2_axis_0"), val = tensor&lt;int32, []&gt;(1)];
            tensor&lt;int32, []&gt; model_tf_compat_v1_gather_1_GatherV2_batch_dims_0 = const()[name = tensor&lt;string, []&gt;("model_tf_compat_v1_gather_1_GatherV2_batch_dims_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, [1, 192, 24]&gt; Identity_1 = gather(axis = model_tf_compat_v1_gather_1_GatherV2_axis_0, batch_dims = model_tf_compat_v1_gather_1_GatherV2_batch_dims_0, indices = model_tf___operators___getitem_2_strided_slice, x = model_extend_layer_concat_2);
        } -&gt; (Identity, Identity_1);
}</code></pre>
                    </div>
                </div>
            
                <div class="file-section" id="file-11">
                    <div class="file-header">
                        <h2>ðŸ“„ model.mil 14</h2>
                        <div class="file-meta">Size: 32.26 KB</div>
                    </div>
                    <div class="file-content">
                        <pre><code>program(1.0)
[buildInfo = dict&lt;tensor&lt;string, []&gt;, tensor&lt;string, []&gt;&gt;({{"coremlc-component-MIL", "3404.16.1"}, {"coremlc-version", "3404.24.1"}})]
{
    func main&lt;ios16&gt;(tensor&lt;fp16, [?, 1, 768]&gt; article_encoding, tensor&lt;int32, [?]&gt; channel_id, tensor&lt;int32, [?, 1]&gt; channel_quality, tensor&lt;int32, [?, 1]&gt; channel_relevance, tensor&lt;fp32, [?, 1]&gt; clicks, tensor&lt;fp32, [?, 1]&gt; impressions, tensor&lt;fp32, [?, 24, 1]&gt; topic_clicks, tensor&lt;int32, [?, 24]&gt; topic_ids, tensor&lt;fp32, [?, 24, 1]&gt; topic_impressions, tensor&lt;int32, [?, 24]&gt; topic_quality, tensor&lt;int32, [?, 24]&gt; topic_relevance, tensor&lt;fp32, [?, 128]&gt; user_hist_enc) [FlexibleShapeInformation = tuple&lt;tuple&lt;tensor&lt;string, []&gt;, dict&lt;tensor&lt;string, []&gt;, tensor&lt;int32, [?]&gt;&gt;&gt;, tuple&lt;tensor&lt;string, []&gt;, dict&lt;tensor&lt;string, []&gt;, list&lt;tensor&lt;int32, [2]&gt;, ?&gt;&gt;&gt;&gt;((("DefaultShapes", {{"article_encoding", [1, 1, 768]}, {"channel_id", [1]}, {"channel_quality", [1, 1]}, {"channel_relevance", [1, 1]}, {"clicks", [1, 1]}, {"impressions", [1, 1]}, {"topic_clicks", [1, 24, 1]}, {"topic_ids", [1, 24]}, {"topic_impressions", [1, 24, 1]}, {"topic_quality", [1, 24]}, {"topic_relevance", [1, 24]}, {"user_hist_enc", [1, 128]}}), ("RangeDims", {{"article_encoding", [[1, -1], [1, 1], [768, 768]]}, {"channel_id", [[1, -1]]}, {"channel_quality", [[1, -1], [1, 1]]}, {"channel_relevance", [[1, -1], [1, 1]]}, {"clicks", [[1, -1], [1, 1]]}, {"impressions", [[1, -1], [1, 1]]}, {"topic_clicks", [[1, -1], [24, 24], [1, 1]]}, {"topic_ids", [[1, -1], [24, 24]]}, {"topic_impressions", [[1, -1], [24, 24], [1, 1]]}, {"topic_quality", [[1, -1], [24, 24]]}, {"topic_relevance", [[1, -1], [24, 24]]}, {"user_hist_enc", [[1, -1], [128, 128]]}})))] {
            tensor&lt;int32, []&gt; model_2_tf_math_equal_Equal_y = const()[name = tensor&lt;string, []&gt;("model_2_tf_math_equal_Equal_y"), val = tensor&lt;int32, []&gt;(5)];
            tensor&lt;int32, []&gt; model_2_tf_math_multiply_1_Mul_y = const()[name = tensor&lt;string, []&gt;("model_2_tf_math_multiply_1_Mul_y"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, []&gt; model_2_tf_math_greater_Greater_y = const()[name = tensor&lt;string, []&gt;("model_2_tf_math_greater_Greater_y"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, []&gt; model_2_tf_math_equal_1_Equal_y = const()[name = tensor&lt;string, []&gt;("model_2_tf_math_equal_1_Equal_y"), val = tensor&lt;int32, []&gt;(8)];
            tensor&lt;int32, []&gt; model_2_tf_math_multiply_2_Mul_y = const()[name = tensor&lt;string, []&gt;("model_2_tf_math_multiply_2_Mul_y"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, []&gt; model_2_tf_math_greater_1_Greater_y = const()[name = tensor&lt;string, []&gt;("model_2_tf_math_greater_1_Greater_y"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, [1]&gt; model_2_tf_compat_v1_squeeze_1_Squeeze_axes_0 = const()[name = tensor&lt;string, []&gt;("model_2_tf_compat_v1_squeeze_1_Squeeze_axes_0"), val = tensor&lt;int32, [1]&gt;([-2])];
            tensor&lt;fp16, [?, 768]&gt; model_2_tf_compat_v1_squeeze_1_Squeeze_cast = squeeze(axes = model_2_tf_compat_v1_squeeze_1_Squeeze_axes_0, x = article_encoding);
            tensor&lt;int32, []&gt; model_2_embedding_4_embedding_lookup_axis_0 = const()[name = tensor&lt;string, []&gt;("model_2_embedding_4_embedding_lookup_axis_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, []&gt; model_2_embedding_4_embedding_lookup_batch_dims_0 = const()[name = tensor&lt;string, []&gt;("model_2_embedding_4_embedding_lookup_batch_dims_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;fp16, [2012, 128]&gt; model_2_embedding_4_embedding_lookup_11996_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_embedding_4_embedding_lookup_11996_to_fp16"), val = tensor&lt;fp16, [2012, 128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(64)))];
            tensor&lt;fp16, [?, 128]&gt; model_2_embedding_4_embedding_lookup_cast = gather(axis = model_2_embedding_4_embedding_lookup_axis_0, batch_dims = model_2_embedding_4_embedding_lookup_batch_dims_0, indices = channel_id, x = model_2_embedding_4_embedding_lookup_11996_to_fp16);
            tensor&lt;int32, []&gt; model_2_embedding_5_embedding_lookup_axis_0 = const()[name = tensor&lt;string, []&gt;("model_2_embedding_5_embedding_lookup_axis_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, []&gt; model_2_embedding_5_embedding_lookup_batch_dims_0 = const()[name = tensor&lt;string, []&gt;("model_2_embedding_5_embedding_lookup_batch_dims_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;fp16, [13980, 128]&gt; model_2_embedding_5_embedding_lookup_11925_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_embedding_5_embedding_lookup_11925_to_fp16"), val = tensor&lt;fp16, [13980, 128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(515200)))];
            tensor&lt;fp16, [?, 24, 128]&gt; model_2_embedding_5_embedding_lookup_cast = gather(axis = model_2_embedding_5_embedding_lookup_axis_0, batch_dims = model_2_embedding_5_embedding_lookup_batch_dims_0, indices = topic_ids, x = model_2_embedding_5_embedding_lookup_11925_to_fp16);
            tensor&lt;string, []&gt; topic_impressions_to_fp16_dtype_0 = const()[name = tensor&lt;string, []&gt;("topic_impressions_to_fp16_dtype_0"), val = tensor&lt;string, []&gt;("fp16")];
            tensor&lt;fp16, []&gt; model_2_tf___operators___add_2_y_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_tf___operators___add_2_y_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1p+0)];
            tensor&lt;fp16, [?, 24, 1]&gt; cast_73 = cast(dtype = topic_impressions_to_fp16_dtype_0, x = topic_impressions);
            tensor&lt;fp16, [?, 24, 1]&gt; model_2_tf___operators___add_2_AddV2_cast = add(x = cast_73, y = model_2_tf___operators___add_2_y_to_fp16);
            tensor&lt;string, []&gt; impressions_to_fp16_dtype_0 = const()[name = tensor&lt;string, []&gt;("impressions_to_fp16_dtype_0"), val = tensor&lt;string, []&gt;("fp16")];
            tensor&lt;fp16, []&gt; model_2_tf___operators___add_3_y_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_tf___operators___add_3_y_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1p+0)];
            tensor&lt;fp16, [?, 1]&gt; cast_71 = cast(dtype = impressions_to_fp16_dtype_0, x = impressions);
            tensor&lt;fp16, [?, 1]&gt; model_2_tf___operators___add_3_AddV2_cast = add(x = cast_71, y = model_2_tf___operators___add_3_y_to_fp16);
            tensor&lt;bool, [?, 24]&gt; model_2_tf_math_equal_Equal = equal(x = topic_quality, y = model_2_tf_math_equal_Equal_y);
            tensor&lt;int32, [?, 24]&gt; model_2_tf_math_multiply_1_Mul = mul(x = topic_quality, y = model_2_tf_math_multiply_1_Mul_y);
            tensor&lt;bool, [?, 24]&gt; model_2_tf_math_equal_1_Equal = equal(x = topic_relevance, y = model_2_tf_math_equal_1_Equal_y);
            tensor&lt;int32, [?, 24]&gt; model_2_tf_math_multiply_2_Mul = mul(x = topic_relevance, y = model_2_tf_math_multiply_2_Mul_y);
            tensor&lt;string, []&gt; topic_clicks_to_fp16_dtype_0 = const()[name = tensor&lt;string, []&gt;("topic_clicks_to_fp16_dtype_0"), val = tensor&lt;string, []&gt;("fp16")];
            tensor&lt;fp16, [?, 24, 1]&gt; cast_69 = cast(dtype = topic_clicks_to_fp16_dtype_0, x = topic_clicks);
            tensor&lt;fp16, [?, 24, 1]&gt; model_2_tf_math_truediv_truediv_cast = real_div(x = cast_69, y = model_2_tf___operators___add_2_AddV2_cast);
            tensor&lt;string, []&gt; clicks_to_fp16_dtype_0 = const()[name = tensor&lt;string, []&gt;("clicks_to_fp16_dtype_0"), val = tensor&lt;string, []&gt;("fp16")];
            tensor&lt;fp16, [?, 1]&gt; cast_67 = cast(dtype = clicks_to_fp16_dtype_0, x = clicks);
            tensor&lt;fp16, [?, 1]&gt; model_2_tf_math_truediv_1_truediv_cast = real_div(x = cast_67, y = model_2_tf___operators___add_3_AddV2_cast);
            tensor&lt;int32, [?, 24]&gt; model_2_tf_where_SelectV2 = select(a = model_2_tf_math_multiply_1_Mul, b = topic_quality, cond = model_2_tf_math_equal_Equal);
            tensor&lt;int32, [?, 24]&gt; model_2_tf_where_2_SelectV2 = select(a = model_2_tf_math_multiply_2_Mul, b = topic_relevance, cond = model_2_tf_math_equal_1_Equal);
            tensor&lt;fp16, [128, 768]&gt; transpose_0_cast_to_fp32_to_fp16 = const()[name = tensor&lt;string, []&gt;("transpose_0_cast_to_fp32_to_fp16"), val = tensor&lt;fp16, [128, 768]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4094144)))];
            tensor&lt;fp16, [128]&gt; model_2_dense_6_BiasAdd_bias_0_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_dense_6_BiasAdd_bias_0_to_fp16"), val = tensor&lt;fp16, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4290816)))];
            tensor&lt;fp16, [?, 128]&gt; model_2_dense_6_BiasAdd_cast = linear(bias = model_2_dense_6_BiasAdd_bias_0_to_fp16, weight = transpose_0_cast_to_fp32_to_fp16, x = model_2_tf_compat_v1_squeeze_1_Squeeze_cast);
            tensor&lt;fp16, []&gt; model_2_tf_math_minimum_Minimum_y_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_tf_math_minimum_Minimum_y_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1p+0)];
            tensor&lt;fp16, [?, 24, 1]&gt; model_2_tf_math_minimum_Minimum_cast = minimum(x = model_2_tf_math_truediv_truediv_cast, y = model_2_tf_math_minimum_Minimum_y_to_fp16);
            tensor&lt;fp16, []&gt; model_2_tf_math_minimum_1_Minimum_y_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_tf_math_minimum_1_Minimum_y_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1p+0)];
            tensor&lt;fp16, [?, 1]&gt; model_2_tf_math_minimum_1_Minimum_cast = minimum(x = model_2_tf_math_truediv_1_truediv_cast, y = model_2_tf_math_minimum_1_Minimum_y_to_fp16);
            tensor&lt;int32, [1]&gt; model_2_tf_math_reduce_max_Max_axes_0 = const()[name = tensor&lt;string, []&gt;("model_2_tf_math_reduce_max_Max_axes_0"), val = tensor&lt;int32, [1]&gt;([-1])];
            tensor&lt;bool, []&gt; model_2_tf_math_reduce_max_Max_keep_dims_0 = const()[name = tensor&lt;string, []&gt;("model_2_tf_math_reduce_max_Max_keep_dims_0"), val = tensor&lt;bool, []&gt;(true)];
            tensor&lt;int32, [?, 1]&gt; model_2_tf_math_reduce_max_Max = reduce_max(axes = model_2_tf_math_reduce_max_Max_axes_0, keep_dims = model_2_tf_math_reduce_max_Max_keep_dims_0, x = model_2_tf_where_SelectV2);
            tensor&lt;int32, [1]&gt; model_2_tf_math_reduce_max_1_Max_axes_0 = const()[name = tensor&lt;string, []&gt;("model_2_tf_math_reduce_max_1_Max_axes_0"), val = tensor&lt;int32, [1]&gt;([-1])];
            tensor&lt;bool, []&gt; model_2_tf_math_reduce_max_1_Max_keep_dims_0 = const()[name = tensor&lt;string, []&gt;("model_2_tf_math_reduce_max_1_Max_keep_dims_0"), val = tensor&lt;bool, []&gt;(true)];
            tensor&lt;int32, [?, 1]&gt; model_2_tf_math_reduce_max_1_Max = reduce_max(axes = model_2_tf_math_reduce_max_1_Max_axes_0, keep_dims = model_2_tf_math_reduce_max_1_Max_keep_dims_0, x = model_2_tf_where_2_SelectV2);
            tensor&lt;fp16, [20, 1]&gt; transpose_1_cast_to_fp32_to_fp16 = const()[name = tensor&lt;string, []&gt;("transpose_1_cast_to_fp32_to_fp16"), val = tensor&lt;fp16, [20, 1]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4291136)))];
            tensor&lt;fp16, [20]&gt; model_2_dense_12_BiasAdd_bias_0_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_dense_12_BiasAdd_bias_0_to_fp16"), val = tensor&lt;fp16, [20]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4291264)))];
            tensor&lt;fp16, [?, 20]&gt; model_2_dense_12_BiasAdd_cast = linear(bias = model_2_dense_12_BiasAdd_bias_0_to_fp16, weight = transpose_1_cast_to_fp32_to_fp16, x = model_2_tf_math_minimum_1_Minimum_cast);
            tensor&lt;bool, [?, 1]&gt; model_2_tf_math_greater_Greater = greater(x = model_2_tf_math_reduce_max_Max, y = model_2_tf_math_greater_Greater_y);
            tensor&lt;bool, [?, 1]&gt; model_2_tf_math_greater_1_Greater = greater(x = model_2_tf_math_reduce_max_1_Max, y = model_2_tf_math_greater_1_Greater_y);
            tensor&lt;string, []&gt; model_2_dense_6_Gelu_mul_1_mode_0 = const()[name = tensor&lt;string, []&gt;("model_2_dense_6_Gelu_mul_1_mode_0"), val = tensor&lt;string, []&gt;("EXACT")];
            tensor&lt;fp16, [?, 128]&gt; model_2_dense_6_Gelu_mul_1_cast = gelu(mode = model_2_dense_6_Gelu_mul_1_mode_0, x = model_2_dense_6_BiasAdd_cast);
            tensor&lt;int32, [?, 1]&gt; model_2_tf_where_1_SelectV2 = select(a = model_2_tf_math_reduce_max_Max, b = channel_quality, cond = model_2_tf_math_greater_Greater);
            tensor&lt;int32, [?, 1]&gt; model_2_tf_where_3_SelectV2 = select(a = model_2_tf_math_reduce_max_1_Max, b = channel_relevance, cond = model_2_tf_math_greater_1_Greater);
            tensor&lt;string, []&gt; model_2_dense_12_Gelu_mul_1_mode_0 = const()[name = tensor&lt;string, []&gt;("model_2_dense_12_Gelu_mul_1_mode_0"), val = tensor&lt;string, []&gt;("EXACT")];
            tensor&lt;fp16, [?, 20]&gt; model_2_dense_12_Gelu_mul_1_cast = gelu(mode = model_2_dense_12_Gelu_mul_1_mode_0, x = model_2_dense_12_BiasAdd_cast);
            tensor&lt;int32, [1]&gt; model_2_tf_compat_v1_squeeze_2_Squeeze_axes_0 = const()[name = tensor&lt;string, []&gt;("model_2_tf_compat_v1_squeeze_2_Squeeze_axes_0"), val = tensor&lt;int32, [1]&gt;([-1])];
            tensor&lt;int32, [?]&gt; model_2_tf_compat_v1_squeeze_2_Squeeze = squeeze(axes = model_2_tf_compat_v1_squeeze_2_Squeeze_axes_0, x = model_2_tf_where_1_SelectV2);
            tensor&lt;int32, [1]&gt; model_2_tf_compat_v1_squeeze_3_Squeeze_axes_0 = const()[name = tensor&lt;string, []&gt;("model_2_tf_compat_v1_squeeze_3_Squeeze_axes_0"), val = tensor&lt;int32, [1]&gt;([-1])];
            tensor&lt;int32, [?]&gt; model_2_tf_compat_v1_squeeze_3_Squeeze = squeeze(axes = model_2_tf_compat_v1_squeeze_3_Squeeze_axes_0, x = model_2_tf_where_3_SelectV2);
            tensor&lt;int32, []&gt; model_2_embedding_6_embedding_lookup_axis_0 = const()[name = tensor&lt;string, []&gt;("model_2_embedding_6_embedding_lookup_axis_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, []&gt; model_2_embedding_6_embedding_lookup_batch_dims_0 = const()[name = tensor&lt;string, []&gt;("model_2_embedding_6_embedding_lookup_batch_dims_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;fp16, [6, 128]&gt; model_2_embedding_6_embedding_lookup_12032_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_embedding_6_embedding_lookup_12032_to_fp16"), val = tensor&lt;fp16, [6, 128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4291392)))];
            tensor&lt;fp16, [?, 128]&gt; model_2_embedding_6_embedding_lookup_cast = gather(axis = model_2_embedding_6_embedding_lookup_axis_0, batch_dims = model_2_embedding_6_embedding_lookup_batch_dims_0, indices = model_2_tf_compat_v1_squeeze_2_Squeeze, x = model_2_embedding_6_embedding_lookup_12032_to_fp16);
            tensor&lt;int32, []&gt; model_2_embedding_7_embedding_lookup_axis_0 = const()[name = tensor&lt;string, []&gt;("model_2_embedding_7_embedding_lookup_axis_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, []&gt; model_2_embedding_7_embedding_lookup_batch_dims_0 = const()[name = tensor&lt;string, []&gt;("model_2_embedding_7_embedding_lookup_batch_dims_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;fp16, [9, 128]&gt; model_2_embedding_7_embedding_lookup_12038_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_embedding_7_embedding_lookup_12038_to_fp16"), val = tensor&lt;fp16, [9, 128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4292992)))];
            tensor&lt;fp16, [?, 128]&gt; model_2_embedding_7_embedding_lookup_cast = gather(axis = model_2_embedding_7_embedding_lookup_axis_0, batch_dims = model_2_embedding_7_embedding_lookup_batch_dims_0, indices = model_2_tf_compat_v1_squeeze_3_Squeeze, x = model_2_embedding_7_embedding_lookup_12038_to_fp16);
            tensor&lt;int32, [2]&gt; model_2_dense_8_Tensordot_stackx = const()[name = tensor&lt;string, []&gt;("model_2_dense_8_Tensordot_stackx"), val = tensor&lt;int32, [2]&gt;([-1, 1])];
            tensor&lt;fp16, [?, 1]&gt; model_2_dense_8_Tensordot_Reshape_cast = reshape(shape = model_2_dense_8_Tensordot_stackx, x = model_2_tf_math_minimum_Minimum_cast);
            tensor&lt;fp16, [128, 128]&gt; transpose_2_cast_to_fp32_to_fp16 = const()[name = tensor&lt;string, []&gt;("transpose_2_cast_to_fp32_to_fp16"), val = tensor&lt;fp16, [128, 128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4295360)))];
            tensor&lt;fp16, [128]&gt; model_2_dense_7_BiasAdd_bias_0_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_dense_7_BiasAdd_bias_0_to_fp16"), val = tensor&lt;fp16, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4328192)))];
            tensor&lt;fp16, [?, 128]&gt; model_2_dense_7_BiasAdd_cast = linear(bias = model_2_dense_7_BiasAdd_bias_0_to_fp16, weight = transpose_2_cast_to_fp32_to_fp16, x = model_2_dense_6_Gelu_mul_1_cast);
            tensor&lt;bool, []&gt; model_2_dense_8_Tensordot_MatMul_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("model_2_dense_8_Tensordot_MatMul_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; model_2_dense_8_Tensordot_MatMul_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("model_2_dense_8_Tensordot_MatMul_transpose_y_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [1, 20]&gt; model_2_dense_8_Tensordot_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_dense_8_Tensordot_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [1, 20]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4328512)))];
            tensor&lt;fp16, [?, 20]&gt; model_2_dense_8_Tensordot_MatMul_cast = matmul(transpose_x = model_2_dense_8_Tensordot_MatMul_transpose_x_0, transpose_y = model_2_dense_8_Tensordot_MatMul_transpose_y_0, x = model_2_dense_8_Tensordot_Reshape_cast, y = model_2_dense_8_Tensordot_ReadVariableOp_to_fp16);
            tensor&lt;int32, [3]&gt; model_2_dense_8_Tensordot_concat_1x = const()[name = tensor&lt;string, []&gt;("model_2_dense_8_Tensordot_concat_1x"), val = tensor&lt;int32, [3]&gt;([-1, 24, 20])];
            tensor&lt;fp16, [?, 24, 20]&gt; model_2_dense_8_Tensordot_cast = reshape(shape = model_2_dense_8_Tensordot_concat_1x, x = model_2_dense_8_Tensordot_MatMul_cast);
            tensor&lt;fp16, [20, 20]&gt; transpose_3_cast_to_fp32_to_fp16 = const()[name = tensor&lt;string, []&gt;("transpose_3_cast_to_fp32_to_fp16"), val = tensor&lt;fp16, [20, 20]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4328640)))];
            tensor&lt;fp16, [20]&gt; model_2_dense_13_BiasAdd_bias_0_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_dense_13_BiasAdd_bias_0_to_fp16"), val = tensor&lt;fp16, [20]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4329536)))];
            tensor&lt;fp16, [?, 20]&gt; model_2_dense_13_BiasAdd_cast = linear(bias = model_2_dense_13_BiasAdd_bias_0_to_fp16, weight = transpose_3_cast_to_fp32_to_fp16, x = model_2_dense_12_Gelu_mul_1_cast);
            tensor&lt;fp16, [?, 128]&gt; model_2_tf___operators___add_4_AddV2_cast = add(x = model_2_dense_7_BiasAdd_cast, y = model_2_embedding_4_embedding_lookup_cast);
            tensor&lt;fp16, [20]&gt; model_2_dense_8_BiasAdd_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_dense_8_BiasAdd_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [20]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4329664)))];
            tensor&lt;fp16, [?, 24, 20]&gt; model_2_dense_8_BiasAdd_cast = add(x = model_2_dense_8_Tensordot_cast, y = model_2_dense_8_BiasAdd_ReadVariableOp_to_fp16);
            tensor&lt;string, []&gt; model_2_dense_8_Gelu_mul_1_mode_0 = const()[name = tensor&lt;string, []&gt;("model_2_dense_8_Gelu_mul_1_mode_0"), val = tensor&lt;string, []&gt;("EXACT")];
            tensor&lt;fp16, [?, 24, 20]&gt; model_2_dense_8_Gelu_mul_1_cast = gelu(mode = model_2_dense_8_Gelu_mul_1_mode_0, x = model_2_dense_8_BiasAdd_cast);
            tensor&lt;string, []&gt; model_2_dense_13_Gelu_mul_1_mode_0 = const()[name = tensor&lt;string, []&gt;("model_2_dense_13_Gelu_mul_1_mode_0"), val = tensor&lt;string, []&gt;("EXACT")];
            tensor&lt;fp16, [?, 20]&gt; model_2_dense_13_Gelu_mul_1_cast = gelu(mode = model_2_dense_13_Gelu_mul_1_mode_0, x = model_2_dense_13_BiasAdd_cast);
            tensor&lt;fp16, [128, 20]&gt; transpose_4_cast_to_fp32_to_fp16 = const()[name = tensor&lt;string, []&gt;("transpose_4_cast_to_fp32_to_fp16"), val = tensor&lt;fp16, [128, 20]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4329792)))];
            tensor&lt;fp16, [128]&gt; model_2_dense_14_BiasAdd_bias_0_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_dense_14_BiasAdd_bias_0_to_fp16"), val = tensor&lt;fp16, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4334976)))];
            tensor&lt;fp16, [?, 128]&gt; model_2_dense_14_BiasAdd_cast = linear(bias = model_2_dense_14_BiasAdd_bias_0_to_fp16, weight = transpose_4_cast_to_fp32_to_fp16, x = model_2_dense_13_Gelu_mul_1_cast);
            tensor&lt;int32, [2]&gt; model_2_dense_9_Tensordot_stackx = const()[name = tensor&lt;string, []&gt;("model_2_dense_9_Tensordot_stackx"), val = tensor&lt;int32, [2]&gt;([-1, 20])];
            tensor&lt;fp16, [?, 20]&gt; model_2_dense_9_Tensordot_Reshape_cast = reshape(shape = model_2_dense_9_Tensordot_stackx, x = model_2_dense_8_Gelu_mul_1_cast);
            tensor&lt;bool, []&gt; model_2_dense_9_Tensordot_MatMul_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("model_2_dense_9_Tensordot_MatMul_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; model_2_dense_9_Tensordot_MatMul_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("model_2_dense_9_Tensordot_MatMul_transpose_y_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [20, 20]&gt; model_2_dense_9_Tensordot_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_dense_9_Tensordot_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [20, 20]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4335296)))];
            tensor&lt;fp16, [?, 20]&gt; model_2_dense_9_Tensordot_MatMul_cast = matmul(transpose_x = model_2_dense_9_Tensordot_MatMul_transpose_x_0, transpose_y = model_2_dense_9_Tensordot_MatMul_transpose_y_0, x = model_2_dense_9_Tensordot_Reshape_cast, y = model_2_dense_9_Tensordot_ReadVariableOp_to_fp16);
            tensor&lt;int32, [3]&gt; model_2_dense_9_Tensordot_concat_1x = const()[name = tensor&lt;string, []&gt;("model_2_dense_9_Tensordot_concat_1x"), val = tensor&lt;int32, [3]&gt;([-1, 24, 20])];
            tensor&lt;fp16, [?, 24, 20]&gt; model_2_dense_9_Tensordot_cast = reshape(shape = model_2_dense_9_Tensordot_concat_1x, x = model_2_dense_9_Tensordot_MatMul_cast);
            tensor&lt;fp16, [20]&gt; model_2_dense_9_BiasAdd_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_dense_9_BiasAdd_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [20]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4336192)))];
            tensor&lt;fp16, [?, 24, 20]&gt; model_2_dense_9_BiasAdd_cast = add(x = model_2_dense_9_Tensordot_cast, y = model_2_dense_9_BiasAdd_ReadVariableOp_to_fp16);
            tensor&lt;string, []&gt; model_2_dense_9_Gelu_mul_1_mode_0 = const()[name = tensor&lt;string, []&gt;("model_2_dense_9_Gelu_mul_1_mode_0"), val = tensor&lt;string, []&gt;("EXACT")];
            tensor&lt;fp16, [?, 24, 20]&gt; model_2_dense_9_Gelu_mul_1_cast = gelu(mode = model_2_dense_9_Gelu_mul_1_mode_0, x = model_2_dense_9_BiasAdd_cast);
            tensor&lt;int32, [2]&gt; model_2_dense_10_Tensordot_stackx = const()[name = tensor&lt;string, []&gt;("model_2_dense_10_Tensordot_stackx"), val = tensor&lt;int32, [2]&gt;([-1, 20])];
            tensor&lt;fp16, [?, 20]&gt; model_2_dense_10_Tensordot_Reshape_cast = reshape(shape = model_2_dense_10_Tensordot_stackx, x = model_2_dense_9_Gelu_mul_1_cast);
            tensor&lt;bool, []&gt; model_2_dense_10_Tensordot_MatMul_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("model_2_dense_10_Tensordot_MatMul_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; model_2_dense_10_Tensordot_MatMul_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("model_2_dense_10_Tensordot_MatMul_transpose_y_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [20, 20]&gt; model_2_dense_10_Tensordot_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_dense_10_Tensordot_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [20, 20]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4336320)))];
            tensor&lt;fp16, [?, 20]&gt; model_2_dense_10_Tensordot_MatMul_cast = matmul(transpose_x = model_2_dense_10_Tensordot_MatMul_transpose_x_0, transpose_y = model_2_dense_10_Tensordot_MatMul_transpose_y_0, x = model_2_dense_10_Tensordot_Reshape_cast, y = model_2_dense_10_Tensordot_ReadVariableOp_to_fp16);
            tensor&lt;int32, [3]&gt; model_2_dense_10_Tensordot_concat_1x = const()[name = tensor&lt;string, []&gt;("model_2_dense_10_Tensordot_concat_1x"), val = tensor&lt;int32, [3]&gt;([-1, 24, 20])];
            tensor&lt;fp16, [?, 24, 20]&gt; model_2_dense_10_Tensordot_cast = reshape(shape = model_2_dense_10_Tensordot_concat_1x, x = model_2_dense_10_Tensordot_MatMul_cast);
            tensor&lt;fp16, [20]&gt; model_2_dense_10_BiasAdd_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_dense_10_BiasAdd_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [20]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4337216)))];
            tensor&lt;fp16, [?, 24, 20]&gt; model_2_dense_10_BiasAdd_cast = add(x = model_2_dense_10_Tensordot_cast, y = model_2_dense_10_BiasAdd_ReadVariableOp_to_fp16);
            tensor&lt;string, []&gt; model_2_dense_10_Gelu_mul_1_mode_0 = const()[name = tensor&lt;string, []&gt;("model_2_dense_10_Gelu_mul_1_mode_0"), val = tensor&lt;string, []&gt;("EXACT")];
            tensor&lt;fp16, [?, 24, 20]&gt; model_2_dense_10_Gelu_mul_1_cast = gelu(mode = model_2_dense_10_Gelu_mul_1_mode_0, x = model_2_dense_10_BiasAdd_cast);
            tensor&lt;int32, [2]&gt; model_2_dense_11_Tensordot_stackx = const()[name = tensor&lt;string, []&gt;("model_2_dense_11_Tensordot_stackx"), val = tensor&lt;int32, [2]&gt;([-1, 20])];
            tensor&lt;fp16, [?, 20]&gt; model_2_dense_11_Tensordot_Reshape_cast = reshape(shape = model_2_dense_11_Tensordot_stackx, x = model_2_dense_10_Gelu_mul_1_cast);
            tensor&lt;bool, []&gt; model_2_dense_11_Tensordot_MatMul_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("model_2_dense_11_Tensordot_MatMul_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; model_2_dense_11_Tensordot_MatMul_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("model_2_dense_11_Tensordot_MatMul_transpose_y_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [20, 1]&gt; model_2_dense_11_Tensordot_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_dense_11_Tensordot_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [20, 1]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4337344)))];
            tensor&lt;fp16, [?, 1]&gt; model_2_dense_11_Tensordot_MatMul_cast = matmul(transpose_x = model_2_dense_11_Tensordot_MatMul_transpose_x_0, transpose_y = model_2_dense_11_Tensordot_MatMul_transpose_y_0, x = model_2_dense_11_Tensordot_Reshape_cast, y = model_2_dense_11_Tensordot_ReadVariableOp_to_fp16);
            tensor&lt;int32, [3]&gt; model_2_dense_11_Tensordot_concat_1x = const()[name = tensor&lt;string, []&gt;("model_2_dense_11_Tensordot_concat_1x"), val = tensor&lt;int32, [3]&gt;([-1, 24, 1])];
            tensor&lt;fp16, [?, 24, 1]&gt; model_2_dense_11_Tensordot_cast = reshape(shape = model_2_dense_11_Tensordot_concat_1x, x = model_2_dense_11_Tensordot_MatMul_cast);
            tensor&lt;fp16, [1]&gt; model_2_dense_11_BiasAdd_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_dense_11_BiasAdd_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [1]&gt;([0x1.dcp-1])];
            tensor&lt;fp16, [?, 24, 1]&gt; model_2_dense_11_BiasAdd_cast = add(x = model_2_dense_11_Tensordot_cast, y = model_2_dense_11_BiasAdd_ReadVariableOp_to_fp16);
            tensor&lt;fp16, [?, 24, 128]&gt; model_2_tf_math_multiply_Mul_cast = mul(x = model_2_embedding_5_embedding_lookup_cast, y = model_2_dense_11_BiasAdd_cast);
            tensor&lt;int32, [1]&gt; model_2_tf_math_reduce_mean_2_Mean_axes_0 = const()[name = tensor&lt;string, []&gt;("model_2_tf_math_reduce_mean_2_Mean_axes_0"), val = tensor&lt;int32, [1]&gt;([-2])];
            tensor&lt;bool, []&gt; model_2_tf_math_reduce_mean_2_Mean_keep_dims_0 = const()[name = tensor&lt;string, []&gt;("model_2_tf_math_reduce_mean_2_Mean_keep_dims_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [?, 128]&gt; model_2_tf_math_reduce_mean_2_Mean_cast = reduce_mean(axes = model_2_tf_math_reduce_mean_2_Mean_axes_0, keep_dims = model_2_tf_math_reduce_mean_2_Mean_keep_dims_0, x = model_2_tf_math_multiply_Mul_cast);
            tensor&lt;fp16, [?, 128]&gt; model_2_tf___operators___add_5_AddV2_cast = add(x = model_2_tf___operators___add_4_AddV2_cast, y = model_2_tf_math_reduce_mean_2_Mean_cast);
            tensor&lt;fp16, [?, 128]&gt; model_2_tf___operators___add_6_AddV2_cast = add(x = model_2_tf___operators___add_5_AddV2_cast, y = model_2_dense_14_BiasAdd_cast);
            tensor&lt;fp16, [?, 128]&gt; model_2_tf___operators___add_7_AddV2_cast = add(x = model_2_tf___operators___add_6_AddV2_cast, y = model_2_embedding_6_embedding_lookup_cast);
            tensor&lt;fp16, [?, 128]&gt; model_2_tf___operators___add_8_AddV2_cast = add(x = model_2_tf___operators___add_7_AddV2_cast, y = model_2_embedding_7_embedding_lookup_cast);
            tensor&lt;string, []&gt; user_hist_enc_to_fp16_dtype_0 = const()[name = tensor&lt;string, []&gt;("user_hist_enc_to_fp16_dtype_0"), val = tensor&lt;string, []&gt;("fp16")];
            tensor&lt;fp16, [?, 128]&gt; cast_27 = cast(dtype = user_hist_enc_to_fp16_dtype_0, x = user_hist_enc);
            tensor&lt;fp16, [?, 128]&gt; model_2_tf_math_multiply_4_Mul_cast = mul(x = cast_27, y = model_2_tf___operators___add_8_AddV2_cast);
            tensor&lt;fp16, [128, 128]&gt; transpose_5_cast_to_fp32_to_fp16 = const()[name = tensor&lt;string, []&gt;("transpose_5_cast_to_fp32_to_fp16"), val = tensor&lt;fp16, [128, 128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4337472)))];
            tensor&lt;fp16, [128]&gt; model_2_dense_15_BiasAdd_bias_0_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_dense_15_BiasAdd_bias_0_to_fp16"), val = tensor&lt;fp16, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4370304)))];
            tensor&lt;fp16, [?, 128]&gt; model_2_dense_15_BiasAdd_cast = linear(bias = model_2_dense_15_BiasAdd_bias_0_to_fp16, weight = transpose_5_cast_to_fp32_to_fp16, x = model_2_tf_math_multiply_4_Mul_cast);
            tensor&lt;string, []&gt; model_2_dense_15_Gelu_mul_1_mode_0 = const()[name = tensor&lt;string, []&gt;("model_2_dense_15_Gelu_mul_1_mode_0"), val = tensor&lt;string, []&gt;("EXACT")];
            tensor&lt;fp16, [?, 128]&gt; model_2_dense_15_Gelu_mul_1_cast = gelu(mode = model_2_dense_15_Gelu_mul_1_mode_0, x = model_2_dense_15_BiasAdd_cast);
            tensor&lt;fp16, [128, 128]&gt; transpose_6_cast_to_fp32_to_fp16 = const()[name = tensor&lt;string, []&gt;("transpose_6_cast_to_fp32_to_fp16"), val = tensor&lt;fp16, [128, 128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4370624)))];
            tensor&lt;fp16, [128]&gt; model_2_dense_16_BiasAdd_bias_0_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_dense_16_BiasAdd_bias_0_to_fp16"), val = tensor&lt;fp16, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4403456)))];
            tensor&lt;fp16, [?, 128]&gt; model_2_dense_16_BiasAdd_cast = linear(bias = model_2_dense_16_BiasAdd_bias_0_to_fp16, weight = transpose_6_cast_to_fp32_to_fp16, x = model_2_dense_15_Gelu_mul_1_cast);
            tensor&lt;string, []&gt; model_2_dense_16_Gelu_mul_1_mode_0 = const()[name = tensor&lt;string, []&gt;("model_2_dense_16_Gelu_mul_1_mode_0"), val = tensor&lt;string, []&gt;("EXACT")];
            tensor&lt;fp16, [?, 128]&gt; model_2_dense_16_Gelu_mul_1_cast = gelu(mode = model_2_dense_16_Gelu_mul_1_mode_0, x = model_2_dense_16_BiasAdd_cast);
            tensor&lt;fp16, [1, 128]&gt; transpose_7_cast_to_fp32_to_fp16 = const()[name = tensor&lt;string, []&gt;("transpose_7_cast_to_fp32_to_fp16"), val = tensor&lt;fp16, [1, 128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4403776)))];
            tensor&lt;fp16, [1]&gt; model_2_dense_17_BiasAdd_bias_0_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_dense_17_BiasAdd_bias_0_to_fp16"), val = tensor&lt;fp16, [1]&gt;([-0x1.cacp-2])];
            tensor&lt;fp16, [?, 1]&gt; model_2_dense_17_BiasAdd_cast = linear(bias = model_2_dense_17_BiasAdd_bias_0_to_fp16, weight = transpose_7_cast_to_fp32_to_fp16, x = model_2_dense_16_Gelu_mul_1_cast);
            tensor&lt;fp16, [?, 1]&gt; model_2_label_Sigmoid_cast = sigmoid(x = model_2_dense_17_BiasAdd_cast);
            tensor&lt;string, []&gt; model_2_label_Sigmoid_cast_to_fp32_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_2_label_Sigmoid_cast_to_fp32_dtype_0"), val = tensor&lt;string, []&gt;("fp32")];
            tensor&lt;fp32, [?, 1]&gt; Identity = cast(dtype = model_2_label_Sigmoid_cast_to_fp32_dtype_0, x = model_2_label_Sigmoid_cast);
        } -&gt; (Identity);
}</code></pre>
                    </div>
                </div>
            
                <div class="file-section" id="file-12">
                    <div class="file-header">
                        <h2>ðŸ“„ model.mil 13</h2>
                        <div class="file-meta">Size: 14.07 KB</div>
                    </div>
                    <div class="file-content">
                        <pre><code>program(1.0)
[buildInfo = dict&lt;tensor&lt;string, []&gt;, tensor&lt;string, []&gt;&gt;({{"coremlc-component-MIL", "3404.16.1"}, {"coremlc-version", "3404.24.1"}})]
{
    func main&lt;ios16&gt;(tensor&lt;fp32, [?, 64]&gt; input_10, tensor&lt;fp32, [?, 1]&gt; input_5, tensor&lt;int32, [?, 1]&gt; input_6, tensor&lt;fp32, [?, 64]&gt; input_7, tensor&lt;int32, [?, 24]&gt; input_9) [FlexibleShapeInformation = tuple&lt;tuple&lt;tensor&lt;string, []&gt;, dict&lt;tensor&lt;string, []&gt;, tensor&lt;int32, [?]&gt;&gt;&gt;, tuple&lt;tensor&lt;string, []&gt;, dict&lt;tensor&lt;string, []&gt;, list&lt;tensor&lt;int32, [2]&gt;, ?&gt;&gt;&gt;&gt;((("DefaultShapes", {{"input_10", [1, 64]}, {"input_5", [1, 1]}, {"input_6", [1, 1]}, {"input_7", [1, 64]}, {"input_9", [1, 24]}}), ("RangeDims", {{"input_10", [[1, -1], [64, 64]]}, {"input_5", [[1, -1], [1, 1]]}, {"input_6", [[1, -1], [1, 1]]}, {"input_7", [[1, -1], [64, 64]]}, {"input_9", [[1, -1], [24, 24]]}})))] {
            tensor&lt;int32, [1]&gt; model_2_multiply_ExpandDims_axes_0 = const()[name = tensor&lt;string, []&gt;("model_2_multiply_ExpandDims_axes_0"), val = tensor&lt;int32, [1]&gt;([1])];
            tensor&lt;string, []&gt; input_7_to_fp16_dtype_0 = const()[name = tensor&lt;string, []&gt;("input_7_to_fp16_dtype_0"), val = tensor&lt;string, []&gt;("fp16")];
            tensor&lt;fp16, [?, 64]&gt; cast_34 = cast(dtype = input_7_to_fp16_dtype_0, x = input_7);
            tensor&lt;fp16, [?, 1, 64]&gt; model_2_multiply_ExpandDims_cast = expand_dims(axes = model_2_multiply_ExpandDims_axes_0, x = cast_34);
            tensor&lt;int32, []&gt; model_2_embedding_embedding_lookup_axis_0 = const()[name = tensor&lt;string, []&gt;("model_2_embedding_embedding_lookup_axis_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, []&gt; model_2_embedding_embedding_lookup_batch_dims_0 = const()[name = tensor&lt;string, []&gt;("model_2_embedding_embedding_lookup_batch_dims_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;fp16, [2364, 64]&gt; model_2_embedding_embedding_lookup_973_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_embedding_embedding_lookup_973_to_fp16"), val = tensor&lt;fp16, [2364, 64]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(64)))];
            tensor&lt;fp16, [?, 1, 64]&gt; model_2_embedding_embedding_lookup_cast = gather(axis = model_2_embedding_embedding_lookup_axis_0, batch_dims = model_2_embedding_embedding_lookup_batch_dims_0, indices = input_6, x = model_2_embedding_embedding_lookup_973_to_fp16);
            tensor&lt;fp16, []&gt; model_2_1025_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_1025_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1p+0)];
            tensor&lt;string, []&gt; input_5_to_fp16_dtype_0 = const()[name = tensor&lt;string, []&gt;("input_5_to_fp16_dtype_0"), val = tensor&lt;string, []&gt;("fp16")];
            tensor&lt;fp16, [?, 1]&gt; cast_31 = cast(dtype = input_5_to_fp16_dtype_0, x = input_5);
            tensor&lt;fp16, [?, 1]&gt; model_2_tf_math_subtract_1_Sub_cast = sub(x = model_2_1025_to_fp16, y = cast_31);
            tensor&lt;int32, [1]&gt; model_2_multiply_1_ExpandDims_axes_0 = const()[name = tensor&lt;string, []&gt;("model_2_multiply_1_ExpandDims_axes_0"), val = tensor&lt;int32, [1]&gt;([1])];
            tensor&lt;string, []&gt; input_10_to_fp16_dtype_0 = const()[name = tensor&lt;string, []&gt;("input_10_to_fp16_dtype_0"), val = tensor&lt;string, []&gt;("fp16")];
            tensor&lt;fp16, [?, 64]&gt; cast_29 = cast(dtype = input_10_to_fp16_dtype_0, x = input_10);
            tensor&lt;fp16, [?, 1, 64]&gt; model_2_multiply_1_ExpandDims_cast = expand_dims(axes = model_2_multiply_1_ExpandDims_axes_0, x = cast_29);
            tensor&lt;int32, []&gt; model_2_embedding_1_embedding_lookup_axis_0 = const()[name = tensor&lt;string, []&gt;("model_2_embedding_1_embedding_lookup_axis_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, []&gt; model_2_embedding_1_embedding_lookup_batch_dims_0 = const()[name = tensor&lt;string, []&gt;("model_2_embedding_1_embedding_lookup_batch_dims_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;fp16, [11581, 64]&gt; model_2_embedding_1_embedding_lookup_966_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_embedding_1_embedding_lookup_966_to_fp16"), val = tensor&lt;fp16, [11581, 64]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(302720)))];
            tensor&lt;fp16, [?, 24, 64]&gt; model_2_embedding_1_embedding_lookup_cast = gather(axis = model_2_embedding_1_embedding_lookup_axis_0, batch_dims = model_2_embedding_1_embedding_lookup_batch_dims_0, indices = input_9, x = model_2_embedding_1_embedding_lookup_966_to_fp16);
            tensor&lt;fp16, [?, 1, 64]&gt; model_2_tf_math_l2_normalize_1_l2_normalize_Square_cast = mul(x = model_2_embedding_embedding_lookup_cast, y = model_2_embedding_embedding_lookup_cast);
            tensor&lt;int32, [1]&gt; model_2_tf_math_reduce_mean_3_Mean_axes_0 = const()[name = tensor&lt;string, []&gt;("model_2_tf_math_reduce_mean_3_Mean_axes_0"), val = tensor&lt;int32, [1]&gt;([1])];
            tensor&lt;bool, []&gt; model_2_tf_math_reduce_mean_3_Mean_keep_dims_0 = const()[name = tensor&lt;string, []&gt;("model_2_tf_math_reduce_mean_3_Mean_keep_dims_0"), val = tensor&lt;bool, []&gt;(true)];
            tensor&lt;fp16, [?, 1, 64]&gt; model_2_tf_math_reduce_mean_3_Mean_cast = reduce_mean(axes = model_2_tf_math_reduce_mean_3_Mean_axes_0, keep_dims = model_2_tf_math_reduce_mean_3_Mean_keep_dims_0, x = model_2_embedding_1_embedding_lookup_cast);
            tensor&lt;int32, [1]&gt; model_2_tf_math_l2_normalize_1_l2_normalize_Sum_axes_0 = const()[name = tensor&lt;string, []&gt;("model_2_tf_math_l2_normalize_1_l2_normalize_Sum_axes_0"), val = tensor&lt;int32, [1]&gt;([-1])];
            tensor&lt;bool, []&gt; model_2_tf_math_l2_normalize_1_l2_normalize_Sum_keep_dims_0 = const()[name = tensor&lt;string, []&gt;("model_2_tf_math_l2_normalize_1_l2_normalize_Sum_keep_dims_0"), val = tensor&lt;bool, []&gt;(true)];
            tensor&lt;fp16, [?, 1, 1]&gt; model_2_tf_math_l2_normalize_1_l2_normalize_Sum_cast = reduce_sum(axes = model_2_tf_math_l2_normalize_1_l2_normalize_Sum_axes_0, keep_dims = model_2_tf_math_l2_normalize_1_l2_normalize_Sum_keep_dims_0, x = model_2_tf_math_l2_normalize_1_l2_normalize_Square_cast);
            tensor&lt;fp16, [?, 1, 64]&gt; model_2_tf_math_l2_normalize_3_l2_normalize_Square_cast = mul(x = model_2_tf_math_reduce_mean_3_Mean_cast, y = model_2_tf_math_reduce_mean_3_Mean_cast);
            tensor&lt;fp16, []&gt; model_2_tf_math_l2_normalize_1_l2_normalize_Maximum_y_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_tf_math_l2_normalize_1_l2_normalize_Maximum_y_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1p-24)];
            tensor&lt;fp16, [?, 1, 1]&gt; model_2_tf_math_l2_normalize_1_l2_normalize_Maximum_cast = maximum(x = model_2_tf_math_l2_normalize_1_l2_normalize_Sum_cast, y = model_2_tf_math_l2_normalize_1_l2_normalize_Maximum_y_to_fp16);
            tensor&lt;int32, [1]&gt; model_2_tf_math_l2_normalize_3_l2_normalize_Sum_axes_0 = const()[name = tensor&lt;string, []&gt;("model_2_tf_math_l2_normalize_3_l2_normalize_Sum_axes_0"), val = tensor&lt;int32, [1]&gt;([-1])];
            tensor&lt;bool, []&gt; model_2_tf_math_l2_normalize_3_l2_normalize_Sum_keep_dims_0 = const()[name = tensor&lt;string, []&gt;("model_2_tf_math_l2_normalize_3_l2_normalize_Sum_keep_dims_0"), val = tensor&lt;bool, []&gt;(true)];
            tensor&lt;fp16, [?, 1, 1]&gt; model_2_tf_math_l2_normalize_3_l2_normalize_Sum_cast = reduce_sum(axes = model_2_tf_math_l2_normalize_3_l2_normalize_Sum_axes_0, keep_dims = model_2_tf_math_l2_normalize_3_l2_normalize_Sum_keep_dims_0, x = model_2_tf_math_l2_normalize_3_l2_normalize_Square_cast);
            tensor&lt;fp16, []&gt; model_2_tf_math_l2_normalize_1_l2_normalize_Rsqrt_epsilon_0_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_tf_math_l2_normalize_1_l2_normalize_Rsqrt_epsilon_0_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1p-24)];
            tensor&lt;fp16, [?, 1, 1]&gt; model_2_tf_math_l2_normalize_1_l2_normalize_Rsqrt_cast = rsqrt(epsilon = model_2_tf_math_l2_normalize_1_l2_normalize_Rsqrt_epsilon_0_to_fp16, x = model_2_tf_math_l2_normalize_1_l2_normalize_Maximum_cast);
            tensor&lt;fp16, []&gt; model_2_tf_math_l2_normalize_3_l2_normalize_Maximum_y_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_tf_math_l2_normalize_3_l2_normalize_Maximum_y_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1p-24)];
            tensor&lt;fp16, [?, 1, 1]&gt; model_2_tf_math_l2_normalize_3_l2_normalize_Maximum_cast = maximum(x = model_2_tf_math_l2_normalize_3_l2_normalize_Sum_cast, y = model_2_tf_math_l2_normalize_3_l2_normalize_Maximum_y_to_fp16);
            tensor&lt;fp16, [?, 1, 64]&gt; model_2_tf_math_l2_normalize_1_l2_normalize_cast = mul(x = model_2_embedding_embedding_lookup_cast, y = model_2_tf_math_l2_normalize_1_l2_normalize_Rsqrt_cast);
            tensor&lt;fp16, []&gt; model_2_tf_math_l2_normalize_3_l2_normalize_Rsqrt_epsilon_0_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_tf_math_l2_normalize_3_l2_normalize_Rsqrt_epsilon_0_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1p-24)];
            tensor&lt;fp16, [?, 1, 1]&gt; model_2_tf_math_l2_normalize_3_l2_normalize_Rsqrt_cast = rsqrt(epsilon = model_2_tf_math_l2_normalize_3_l2_normalize_Rsqrt_epsilon_0_to_fp16, x = model_2_tf_math_l2_normalize_3_l2_normalize_Maximum_cast);
            tensor&lt;fp16, [?, 1, 64]&gt; model_2_multiply_mul_cast = mul(x = model_2_multiply_ExpandDims_cast, y = model_2_tf_math_l2_normalize_1_l2_normalize_cast);
            tensor&lt;fp16, [?, 1, 64]&gt; model_2_tf_math_l2_normalize_3_l2_normalize_cast = mul(x = model_2_tf_math_reduce_mean_3_Mean_cast, y = model_2_tf_math_l2_normalize_3_l2_normalize_Rsqrt_cast);
            tensor&lt;int32, [1]&gt; model_2_tf_math_reduce_sum_Sum_axes_0 = const()[name = tensor&lt;string, []&gt;("model_2_tf_math_reduce_sum_Sum_axes_0"), val = tensor&lt;int32, [1]&gt;([-1])];
            tensor&lt;bool, []&gt; model_2_tf_math_reduce_sum_Sum_keep_dims_0 = const()[name = tensor&lt;string, []&gt;("model_2_tf_math_reduce_sum_Sum_keep_dims_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [?, 1]&gt; model_2_tf_math_reduce_sum_Sum_cast = reduce_sum(axes = model_2_tf_math_reduce_sum_Sum_axes_0, keep_dims = model_2_tf_math_reduce_sum_Sum_keep_dims_0, x = model_2_multiply_mul_cast);
            tensor&lt;fp16, [?, 1, 64]&gt; model_2_multiply_1_mul_cast = mul(x = model_2_multiply_1_ExpandDims_cast, y = model_2_tf_math_l2_normalize_3_l2_normalize_cast);
            tensor&lt;int32, [1]&gt; model_2_tf_math_reduce_sum_1_Sum_axes_0 = const()[name = tensor&lt;string, []&gt;("model_2_tf_math_reduce_sum_1_Sum_axes_0"), val = tensor&lt;int32, [1]&gt;([-1])];
            tensor&lt;bool, []&gt; model_2_tf_math_reduce_sum_1_Sum_keep_dims_0 = const()[name = tensor&lt;string, []&gt;("model_2_tf_math_reduce_sum_1_Sum_keep_dims_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [?, 1]&gt; model_2_tf_math_reduce_sum_1_Sum_cast = reduce_sum(axes = model_2_tf_math_reduce_sum_1_Sum_axes_0, keep_dims = model_2_tf_math_reduce_sum_1_Sum_keep_dims_0, x = model_2_multiply_1_mul_cast);
            tensor&lt;fp16, []&gt; model_2_1009_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_1009_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1p+1)];
            tensor&lt;fp16, [?, 1]&gt; model_2_tf_math_multiply_Mul_cast = mul(x = model_2_1009_to_fp16, y = model_2_tf_math_reduce_sum_Sum_cast);
            tensor&lt;fp16, []&gt; model_2_tf_clip_by_value_clip_by_value_Minimum_y_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_tf_clip_by_value_clip_by_value_Minimum_y_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1p+0)];
            tensor&lt;fp16, [?, 1]&gt; model_2_tf_clip_by_value_clip_by_value_Minimum_cast = minimum(x = model_2_tf_math_multiply_Mul_cast, y = model_2_tf_clip_by_value_clip_by_value_Minimum_y_to_fp16);
            tensor&lt;fp16, []&gt; model_2_1006_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_1006_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1.cccp+0)];
            tensor&lt;fp16, [?, 1]&gt; model_2_tf_math_multiply_2_Mul_cast = mul(x = model_2_1006_to_fp16, y = model_2_tf_math_reduce_sum_1_Sum_cast);
            tensor&lt;fp16, []&gt; model_2_tf_clip_by_value_clip_by_value_y_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_tf_clip_by_value_clip_by_value_y_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1.8p-1)];
            tensor&lt;fp16, [?, 1]&gt; model_2_tf_clip_by_value_clip_by_value_cast = maximum(x = model_2_tf_clip_by_value_clip_by_value_Minimum_cast, y = model_2_tf_clip_by_value_clip_by_value_y_to_fp16);
            tensor&lt;fp16, []&gt; model_2_tf_clip_by_value_1_clip_by_value_Minimum_y_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_tf_clip_by_value_1_clip_by_value_Minimum_y_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1p+0)];
            tensor&lt;fp16, [?, 1]&gt; model_2_tf_clip_by_value_1_clip_by_value_Minimum_cast = minimum(x = model_2_tf_math_multiply_2_Mul_cast, y = model_2_tf_clip_by_value_1_clip_by_value_Minimum_y_to_fp16);
            tensor&lt;fp16, [?, 1]&gt; model_2_tf_math_multiply_1_Mul_cast = mul(x = cast_31, y = model_2_tf_clip_by_value_clip_by_value_cast);
            tensor&lt;fp16, []&gt; model_2_tf_clip_by_value_1_clip_by_value_y_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_tf_clip_by_value_1_clip_by_value_y_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1.8p-1)];
            tensor&lt;fp16, [?, 1]&gt; model_2_tf_clip_by_value_1_clip_by_value_cast = maximum(x = model_2_tf_clip_by_value_1_clip_by_value_Minimum_cast, y = model_2_tf_clip_by_value_1_clip_by_value_y_to_fp16);
            tensor&lt;fp16, [?, 1]&gt; model_2_tf___operators___add_AddV2_cast = add(x = model_2_tf_math_multiply_1_Mul_cast, y = model_2_tf_math_subtract_1_Sub_cast);
            tensor&lt;fp16, [?, 1]&gt; model_2_tf_math_multiply_3_Mul_cast = mul(x = cast_31, y = model_2_tf_clip_by_value_1_clip_by_value_cast);
            tensor&lt;fp16, [?, 1]&gt; model_2_tf___operators___add_1_AddV2_cast = add(x = model_2_tf_math_multiply_3_Mul_cast, y = model_2_tf_math_subtract_1_Sub_cast);
            tensor&lt;fp16, [?, 1]&gt; model_2_multiply_2_mul_cast = mul(x = model_2_tf___operators___add_AddV2_cast, y = model_2_tf___operators___add_1_AddV2_cast);
            tensor&lt;fp16, []&gt; model_2_tf_math_maximum_Maximum_y_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_tf_math_maximum_Maximum_y_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1.8p-1)];
            tensor&lt;fp16, [?, 1]&gt; model_2_tf_math_maximum_Maximum_cast = maximum(x = model_2_multiply_2_mul_cast, y = model_2_tf_math_maximum_Maximum_y_to_fp16);
            tensor&lt;string, []&gt; model_2_tf_math_maximum_Maximum_cast_to_fp32_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_2_tf_math_maximum_Maximum_cast_to_fp32_dtype_0"), val = tensor&lt;string, []&gt;("fp32")];
            tensor&lt;fp32, [?, 1]&gt; Identity = cast(dtype = model_2_tf_math_maximum_Maximum_cast_to_fp32_dtype_0, x = model_2_tf_math_maximum_Maximum_cast);
        } -&gt; (Identity);
}</code></pre>
                    </div>
                </div>
            
                <div class="file-section" id="file-13">
                    <div class="file-header">
                        <h2>ðŸ“„ model.mil 19</h2>
                        <div class="file-meta">Size: 65.21 KB</div>
                    </div>
                    <div class="file-content">
                        <pre><code>program(1.0)
[buildInfo = dict&lt;tensor&lt;string, []&gt;, tensor&lt;string, []&gt;&gt;({{"coremlc-component-MIL", "3404.16.1"}, {"coremlc-version", "3404.24.1"}})]
{
    func main&lt;ios16&gt;(tensor&lt;fp16, [1, 192, 1, 768]&gt; hist_article_encoding, tensor&lt;int32, [1, 192]&gt; hist_channel_id, tensor&lt;int32, [1, 192, 24]&gt; hist_topic_ids) {
            tensor&lt;int32, [1]&gt; model_1_tf_compat_v1_squeeze_Squeeze_axes_0 = const()[name = tensor&lt;string, []&gt;("model_1_tf_compat_v1_squeeze_Squeeze_axes_0"), val = tensor&lt;int32, [1]&gt;([-2])];
            tensor&lt;fp16, [1, 192, 768]&gt; model_1_tf_compat_v1_squeeze_Squeeze_cast = squeeze(axes = model_1_tf_compat_v1_squeeze_Squeeze_axes_0, x = hist_article_encoding);
            tensor&lt;int32, []&gt; model_1_embedding_1_embedding_lookup_axis_0 = const()[name = tensor&lt;string, []&gt;("model_1_embedding_1_embedding_lookup_axis_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, []&gt; model_1_embedding_1_embedding_lookup_batch_dims_0 = const()[name = tensor&lt;string, []&gt;("model_1_embedding_1_embedding_lookup_batch_dims_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;fp16, [2012, 128]&gt; model_1_embedding_1_embedding_lookup_6588_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_embedding_1_embedding_lookup_6588_to_fp16"), val = tensor&lt;fp16, [2012, 128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(64)))];
            tensor&lt;fp16, [1, 192, 128]&gt; model_1_embedding_1_embedding_lookup_cast = gather(axis = model_1_embedding_1_embedding_lookup_axis_0, batch_dims = model_1_embedding_1_embedding_lookup_batch_dims_0, indices = hist_channel_id, x = model_1_embedding_1_embedding_lookup_6588_to_fp16);
            tensor&lt;int32, []&gt; model_1_embedding_2_embedding_lookup_axis_0 = const()[name = tensor&lt;string, []&gt;("model_1_embedding_2_embedding_lookup_axis_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, []&gt; model_1_embedding_2_embedding_lookup_batch_dims_0 = const()[name = tensor&lt;string, []&gt;("model_1_embedding_2_embedding_lookup_batch_dims_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;fp16, [13980, 128]&gt; model_1_embedding_2_embedding_lookup_6557_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_embedding_2_embedding_lookup_6557_to_fp16"), val = tensor&lt;fp16, [13980, 128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(515200)))];
            tensor&lt;fp16, [1, 192, 24, 128]&gt; model_1_embedding_2_embedding_lookup_cast = gather(axis = model_1_embedding_2_embedding_lookup_axis_0, batch_dims = model_1_embedding_2_embedding_lookup_batch_dims_0, indices = hist_topic_ids, x = model_1_embedding_2_embedding_lookup_6557_to_fp16);
            tensor&lt;int32, [1]&gt; model_1_tf_math_reduce_mean_Mean_axes_0 = const()[name = tensor&lt;string, []&gt;("model_1_tf_math_reduce_mean_Mean_axes_0"), val = tensor&lt;int32, [1]&gt;([-2])];
            tensor&lt;bool, []&gt; model_1_tf_math_reduce_mean_Mean_keep_dims_0 = const()[name = tensor&lt;string, []&gt;("model_1_tf_math_reduce_mean_Mean_keep_dims_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [1, 192, 128]&gt; model_1_tf_math_reduce_mean_Mean_cast = reduce_mean(axes = model_1_tf_math_reduce_mean_Mean_axes_0, keep_dims = model_1_tf_math_reduce_mean_Mean_keep_dims_0, x = model_1_embedding_2_embedding_lookup_cast);
            tensor&lt;int32, [3]&gt; model_1_dense_Tensordot_concat_1 = const()[name = tensor&lt;string, []&gt;("model_1_dense_Tensordot_concat_1"), val = tensor&lt;int32, [3]&gt;([1, 192, 128])];
            tensor&lt;int32, [2]&gt; model_1_dense_Tensordot_stack = const()[name = tensor&lt;string, []&gt;("model_1_dense_Tensordot_stack"), val = tensor&lt;int32, [2]&gt;([192, 768])];
            tensor&lt;fp16, [192, 768]&gt; model_1_dense_Tensordot_Reshape_cast = reshape(shape = model_1_dense_Tensordot_stack, x = model_1_tf_compat_v1_squeeze_Squeeze_cast);
            tensor&lt;bool, []&gt; model_1_dense_Tensordot_MatMul_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("model_1_dense_Tensordot_MatMul_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; model_1_dense_Tensordot_MatMul_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("model_1_dense_Tensordot_MatMul_transpose_y_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [768, 128]&gt; model_1_dense_Tensordot_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_dense_Tensordot_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [768, 128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4094144)))];
            tensor&lt;fp16, [192, 128]&gt; model_1_dense_Tensordot_MatMul_cast = matmul(transpose_x = model_1_dense_Tensordot_MatMul_transpose_x_0, transpose_y = model_1_dense_Tensordot_MatMul_transpose_y_0, x = model_1_dense_Tensordot_Reshape_cast, y = model_1_dense_Tensordot_ReadVariableOp_to_fp16);
            tensor&lt;fp16, [1, 192, 128]&gt; model_1_dense_Tensordot_cast = reshape(shape = model_1_dense_Tensordot_concat_1, x = model_1_dense_Tensordot_MatMul_cast);
            tensor&lt;fp16, [128]&gt; model_1_dense_BiasAdd_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_dense_BiasAdd_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4290816)))];
            tensor&lt;fp16, [1, 192, 128]&gt; model_1_dense_BiasAdd_cast = add(x = model_1_dense_Tensordot_cast, y = model_1_dense_BiasAdd_ReadVariableOp_to_fp16);
            tensor&lt;string, []&gt; model_1_dense_Gelu_mul_1_mode_0 = const()[name = tensor&lt;string, []&gt;("model_1_dense_Gelu_mul_1_mode_0"), val = tensor&lt;string, []&gt;("EXACT")];
            tensor&lt;fp16, [1, 192, 128]&gt; model_1_dense_Gelu_mul_1_cast = gelu(mode = model_1_dense_Gelu_mul_1_mode_0, x = model_1_dense_BiasAdd_cast);
            tensor&lt;int32, [3]&gt; model_1_dense_1_Tensordot_concat_1 = const()[name = tensor&lt;string, []&gt;("model_1_dense_1_Tensordot_concat_1"), val = tensor&lt;int32, [3]&gt;([1, 192, 128])];
            tensor&lt;int32, [2]&gt; model_1_dense_1_Tensordot_stack = const()[name = tensor&lt;string, []&gt;("model_1_dense_1_Tensordot_stack"), val = tensor&lt;int32, [2]&gt;([192, 128])];
            tensor&lt;fp16, [192, 128]&gt; model_1_dense_1_Tensordot_Reshape_cast = reshape(shape = model_1_dense_1_Tensordot_stack, x = model_1_dense_Gelu_mul_1_cast);
            tensor&lt;bool, []&gt; model_1_dense_1_Tensordot_MatMul_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("model_1_dense_1_Tensordot_MatMul_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; model_1_dense_1_Tensordot_MatMul_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("model_1_dense_1_Tensordot_MatMul_transpose_y_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [128, 128]&gt; model_1_dense_1_Tensordot_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_dense_1_Tensordot_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [128, 128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4291136)))];
            tensor&lt;fp16, [192, 128]&gt; model_1_dense_1_Tensordot_MatMul_cast = matmul(transpose_x = model_1_dense_1_Tensordot_MatMul_transpose_x_0, transpose_y = model_1_dense_1_Tensordot_MatMul_transpose_y_0, x = model_1_dense_1_Tensordot_Reshape_cast, y = model_1_dense_1_Tensordot_ReadVariableOp_to_fp16);
            tensor&lt;fp16, [1, 192, 128]&gt; model_1_dense_1_Tensordot_cast = reshape(shape = model_1_dense_1_Tensordot_concat_1, x = model_1_dense_1_Tensordot_MatMul_cast);
            tensor&lt;fp16, [128]&gt; model_1_dense_1_BiasAdd_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_dense_1_BiasAdd_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4323968)))];
            tensor&lt;fp16, [1, 192, 128]&gt; model_1_dense_1_BiasAdd_cast = add(x = model_1_dense_1_Tensordot_cast, y = model_1_dense_1_BiasAdd_ReadVariableOp_to_fp16);
            tensor&lt;fp16, [1, 192, 128]&gt; model_1_tf___operators___add_AddV2_cast = add(x = model_1_dense_1_BiasAdd_cast, y = model_1_embedding_1_embedding_lookup_cast);
            tensor&lt;fp16, [1, 192, 128]&gt; model_1_tf___operators___add_1_AddV2_cast = add(x = model_1_tf___operators___add_AddV2_cast, y = model_1_tf_math_reduce_mean_Mean_cast);
            tensor&lt;fp16, [192, 128]&gt; model_1_p_emb_StatefulPartitionedCall_StatefulPartitionedCall_embedding_3_embedding_lookup_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_p_emb_StatefulPartitionedCall_StatefulPartitionedCall_embedding_3_embedding_lookup_to_fp16"), val = tensor&lt;fp16, [192, 128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4324288)))];
            tensor&lt;fp16, [1, 192, 128]&gt; model_1_p_emb_StatefulPartitionedCall_StatefulPartitionedCall_add_cast = add(x = model_1_p_emb_StatefulPartitionedCall_StatefulPartitionedCall_embedding_3_embedding_lookup_to_fp16, y = model_1_tf___operators___add_1_AddV2_cast);
            tensor&lt;int32, [1]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_batchnorm_add_1_axes_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_batchnorm_add_1_axes_0"), val = tensor&lt;int32, [1]&gt;([2])];
            tensor&lt;fp16, [128]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_batchnorm_mul_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_batchnorm_mul_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4373504)))];
            tensor&lt;fp16, [128]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_batchnorm_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_batchnorm_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4373824)))];
            tensor&lt;fp16, []&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_batchnorm_add_y_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_batchnorm_add_y_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1.1p-20)];
            tensor&lt;fp16, [1, 192, 128]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_batchnorm_add_1_cast = layer_norm(axes = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_batchnorm_add_1_axes_0, beta = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_batchnorm_ReadVariableOp_to_fp16, epsilon = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_batchnorm_add_y_to_fp16, gamma = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_batchnorm_mul_ReadVariableOp_to_fp16, x = model_1_p_emb_StatefulPartitionedCall_StatefulPartitionedCall_add_cast);
            tensor&lt;int32, [2]&gt; reshape_0_shape_0 = const()[name = tensor&lt;string, []&gt;("reshape_0_shape_0"), val = tensor&lt;int32, [2]&gt;([192, 128])];
            tensor&lt;fp16, [192, 128]&gt; reshape_0_cast = reshape(shape = reshape_0_shape_0, x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_batchnorm_add_1_cast);
            tensor&lt;bool, []&gt; matmul_0_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("matmul_0_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; matmul_0_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("matmul_0_transpose_y_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [128, 256]&gt; reshape_1_to_fp16 = const()[name = tensor&lt;string, []&gt;("reshape_1_to_fp16"), val = tensor&lt;fp16, [128, 256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4374144)))];
            tensor&lt;fp16, [192, 256]&gt; matmul_0_cast = matmul(transpose_x = matmul_0_transpose_x_0, transpose_y = matmul_0_transpose_y_0, x = reshape_0_cast, y = reshape_1_to_fp16);
            tensor&lt;int32, [4]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_key_einsum_Einsum_shape_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_key_einsum_Einsum_shape_0"), val = tensor&lt;int32, [4]&gt;([1, 192, 4, 64])];
            tensor&lt;fp16, [1, 192, 4, 64]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_key_einsum_Einsum_cast = reshape(shape = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_key_einsum_Einsum_shape_0, x = matmul_0_cast);
            tensor&lt;bool, []&gt; matmul_1_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("matmul_1_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; matmul_1_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("matmul_1_transpose_y_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [128, 256]&gt; reshape_3_to_fp16 = const()[name = tensor&lt;string, []&gt;("reshape_3_to_fp16"), val = tensor&lt;fp16, [128, 256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4439744)))];
            tensor&lt;fp16, [192, 256]&gt; matmul_1_cast = matmul(transpose_x = matmul_1_transpose_x_0, transpose_y = matmul_1_transpose_y_0, x = reshape_0_cast, y = reshape_3_to_fp16);
            tensor&lt;int32, [4]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_query_einsum_Einsum_shape_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_query_einsum_Einsum_shape_0"), val = tensor&lt;int32, [4]&gt;([1, 192, 4, 64])];
            tensor&lt;fp16, [1, 192, 4, 64]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_query_einsum_Einsum_cast = reshape(shape = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_query_einsum_Einsum_shape_0, x = matmul_1_cast);
            tensor&lt;bool, []&gt; matmul_2_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("matmul_2_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; matmul_2_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("matmul_2_transpose_y_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [128, 256]&gt; reshape_5_to_fp16 = const()[name = tensor&lt;string, []&gt;("reshape_5_to_fp16"), val = tensor&lt;fp16, [128, 256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4505344)))];
            tensor&lt;fp16, [192, 256]&gt; matmul_2_cast = matmul(transpose_x = matmul_2_transpose_x_0, transpose_y = matmul_2_transpose_y_0, x = reshape_0_cast, y = reshape_5_to_fp16);
            tensor&lt;int32, [4]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_value_einsum_Einsum_shape_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_value_einsum_Einsum_shape_0"), val = tensor&lt;int32, [4]&gt;([1, 192, 4, 64])];
            tensor&lt;fp16, [1, 192, 4, 64]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_value_einsum_Einsum_cast = reshape(shape = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_value_einsum_Einsum_shape_0, x = matmul_2_cast);
            tensor&lt;fp16, [4, 64]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_key_add_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_key_add_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [4, 64]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4570944)))];
            tensor&lt;fp16, [1, 192, 4, 64]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_key_add_cast = add(x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_key_einsum_Einsum_cast, y = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_key_add_ReadVariableOp_to_fp16);
            tensor&lt;fp16, [4, 64]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_query_add_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_query_add_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [4, 64]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4571520)))];
            tensor&lt;fp16, [1, 192, 4, 64]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_query_add_cast = add(x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_query_einsum_Einsum_cast, y = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_query_add_ReadVariableOp_to_fp16);
            tensor&lt;fp16, [4, 64]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_value_add_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_value_add_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [4, 64]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4572096)))];
            tensor&lt;fp16, [1, 192, 4, 64]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_value_add_cast = add(x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_value_einsum_Einsum_cast, y = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_value_add_ReadVariableOp_to_fp16);
            tensor&lt;fp16, []&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_Mul_y_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_Mul_y_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1p-3)];
            tensor&lt;fp16, [1, 192, 4, 64]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_Mul_cast = mul(x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_query_add_cast, y = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_Mul_y_to_fp16);
            tensor&lt;bool, []&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_einsum_Einsum_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_einsum_Einsum_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_einsum_Einsum_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_einsum_Einsum_transpose_y_0"), val = tensor&lt;bool, []&gt;(true)];
            tensor&lt;int32, [4]&gt; transpose_12_perm_0 = const()[name = tensor&lt;string, []&gt;("transpose_12_perm_0"), val = tensor&lt;int32, [4]&gt;([0, 2, 1, 3])];
            tensor&lt;int32, [4]&gt; transpose_13_perm_0 = const()[name = tensor&lt;string, []&gt;("transpose_13_perm_0"), val = tensor&lt;int32, [4]&gt;([0, 2, 1, 3])];
            tensor&lt;fp16, [1, 4, 192, 64]&gt; transpose_22 = transpose(perm = transpose_13_perm_0, x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_key_add_cast);
            tensor&lt;fp16, [1, 4, 192, 64]&gt; transpose_23 = transpose(perm = transpose_12_perm_0, x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_Mul_cast);
            tensor&lt;fp16, [1, 4, 192, 192]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_einsum_Einsum_cast = matmul(transpose_x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_einsum_Einsum_transpose_x_0, transpose_y = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_einsum_Einsum_transpose_y_0, x = transpose_23, y = transpose_22);
            tensor&lt;int32, []&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_softmax_Softmax_axis_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_softmax_Softmax_axis_0"), val = tensor&lt;int32, []&gt;(-1)];
            tensor&lt;fp16, [1, 4, 192, 192]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_softmax_Softmax_cast = softmax(axis = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_softmax_Softmax_axis_0, x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_einsum_Einsum_cast);
            tensor&lt;int32, [4]&gt; transpose_2_perm_0 = const()[name = tensor&lt;string, []&gt;("transpose_2_perm_0"), val = tensor&lt;int32, [4]&gt;([0, 2, 1, 3])];
            tensor&lt;bool, []&gt; matmul_3_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("matmul_3_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; matmul_3_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("matmul_3_transpose_y_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [1, 4, 192, 64]&gt; transpose_21 = transpose(perm = transpose_2_perm_0, x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_value_add_cast);
            tensor&lt;fp16, [1, 4, 192, 64]&gt; matmul_3_cast = matmul(transpose_x = matmul_3_transpose_x_0, transpose_y = matmul_3_transpose_y_0, x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_softmax_Softmax_cast, y = transpose_21);
            tensor&lt;int32, [4]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_einsum_1_Einsum_perm_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_einsum_1_Einsum_perm_0"), val = tensor&lt;int32, [4]&gt;([0, 2, 1, 3])];
            tensor&lt;int32, [3]&gt; reshape_6_shape_0 = const()[name = tensor&lt;string, []&gt;("reshape_6_shape_0"), val = tensor&lt;int32, [3]&gt;([1, 192, 256])];
            tensor&lt;fp16, [1, 192, 4, 64]&gt; transpose_20 = transpose(perm = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_einsum_1_Einsum_perm_0, x = matmul_3_cast);
            tensor&lt;fp16, [1, 192, 256]&gt; reshape_6_cast = reshape(shape = reshape_6_shape_0, x = transpose_20);
            tensor&lt;fp16, [128, 256]&gt; transpose_6_cast_to_fp32_to_fp16 = const()[name = tensor&lt;string, []&gt;("transpose_6_cast_to_fp32_to_fp16"), val = tensor&lt;fp16, [128, 256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4572672)))];
            tensor&lt;fp16, [128]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_attention_output_add_bias_0_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_attention_output_add_bias_0_to_fp16"), val = tensor&lt;fp16, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4638272)))];
            tensor&lt;fp16, [1, 192, 128]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_attention_output_add_cast = linear(bias = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_attention_output_add_bias_0_to_fp16, weight = transpose_6_cast_to_fp32_to_fp16, x = reshape_6_cast);
            tensor&lt;fp16, [1, 192, 128]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_add_cast = add(x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_batchnorm_add_1_cast, y = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_attention_output_add_cast);
            tensor&lt;int32, [1]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_1_batchnorm_add_1_axes_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_1_batchnorm_add_1_axes_0"), val = tensor&lt;int32, [1]&gt;([2])];
            tensor&lt;fp16, [128]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_1_batchnorm_mul_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_1_batchnorm_mul_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4638592)))];
            tensor&lt;fp16, [128]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_1_batchnorm_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_1_batchnorm_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4638912)))];
            tensor&lt;fp16, []&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_1_batchnorm_add_y_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_1_batchnorm_add_y_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1.1p-20)];
            tensor&lt;fp16, [1, 192, 128]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_1_batchnorm_add_1_cast = layer_norm(axes = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_1_batchnorm_add_1_axes_0, beta = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_1_batchnorm_ReadVariableOp_to_fp16, epsilon = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_1_batchnorm_add_y_to_fp16, gamma = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_1_batchnorm_mul_ReadVariableOp_to_fp16, x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_add_cast);
            tensor&lt;int32, [3]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Tensordot_concat_1 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Tensordot_concat_1"), val = tensor&lt;int32, [3]&gt;([1, 192, 256])];
            tensor&lt;int32, [2]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Tensordot_stack = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Tensordot_stack"), val = tensor&lt;int32, [2]&gt;([192, 128])];
            tensor&lt;fp16, [192, 128]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Tensordot_Reshape_cast = reshape(shape = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Tensordot_stack, x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_1_batchnorm_add_1_cast);
            tensor&lt;bool, []&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Tensordot_MatMul_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Tensordot_MatMul_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Tensordot_MatMul_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Tensordot_MatMul_transpose_y_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [128, 256]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Tensordot_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Tensordot_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [128, 256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4639232)))];
            tensor&lt;fp16, [192, 256]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Tensordot_MatMul_cast = matmul(transpose_x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Tensordot_MatMul_transpose_x_0, transpose_y = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Tensordot_MatMul_transpose_y_0, x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Tensordot_Reshape_cast, y = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Tensordot_ReadVariableOp_to_fp16);
            tensor&lt;fp16, [1, 192, 256]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Tensordot_cast = reshape(shape = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Tensordot_concat_1, x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Tensordot_MatMul_cast);
            tensor&lt;fp16, [256]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_BiasAdd_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_BiasAdd_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4704832)))];
            tensor&lt;fp16, [1, 192, 256]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_BiasAdd_cast = add(x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Tensordot_cast, y = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_BiasAdd_ReadVariableOp_to_fp16);
            tensor&lt;string, []&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Gelu_mul_1_mode_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Gelu_mul_1_mode_0"), val = tensor&lt;string, []&gt;("EXACT")];
            tensor&lt;fp16, [1, 192, 256]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Gelu_mul_1_cast = gelu(mode = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Gelu_mul_1_mode_0, x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_BiasAdd_cast);
            tensor&lt;int32, [3]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_Tensordot_concat_1 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_Tensordot_concat_1"), val = tensor&lt;int32, [3]&gt;([1, 192, 128])];
            tensor&lt;int32, [2]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_Tensordot_stack = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_Tensordot_stack"), val = tensor&lt;int32, [2]&gt;([192, 256])];
            tensor&lt;fp16, [192, 256]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_Tensordot_Reshape_cast = reshape(shape = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_Tensordot_stack, x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Gelu_mul_1_cast);
            tensor&lt;bool, []&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_Tensordot_MatMul_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_Tensordot_MatMul_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_Tensordot_MatMul_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_Tensordot_MatMul_transpose_y_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [256, 128]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_Tensordot_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_Tensordot_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [256, 128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4705408)))];
            tensor&lt;fp16, [192, 128]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_Tensordot_MatMul_cast = matmul(transpose_x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_Tensordot_MatMul_transpose_x_0, transpose_y = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_Tensordot_MatMul_transpose_y_0, x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_Tensordot_Reshape_cast, y = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_Tensordot_ReadVariableOp_to_fp16);
            tensor&lt;fp16, [1, 192, 128]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_Tensordot_cast = reshape(shape = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_Tensordot_concat_1, x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_Tensordot_MatMul_cast);
            tensor&lt;fp16, [128]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_BiasAdd_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_BiasAdd_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4771008)))];
            tensor&lt;fp16, [1, 192, 128]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_BiasAdd_cast = add(x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_Tensordot_cast, y = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_BiasAdd_ReadVariableOp_to_fp16);
            tensor&lt;fp16, [1, 192, 128]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_add_1_cast = add(x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_1_batchnorm_add_1_cast, y = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_BiasAdd_cast);
            tensor&lt;int32, [1]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_2_batchnorm_add_1_axes_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_2_batchnorm_add_1_axes_0"), val = tensor&lt;int32, [1]&gt;([2])];
            tensor&lt;fp16, [128]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_2_batchnorm_mul_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_2_batchnorm_mul_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4771328)))];
            tensor&lt;fp16, [128]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_2_batchnorm_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_2_batchnorm_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4771648)))];
            tensor&lt;fp16, []&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_2_batchnorm_add_y_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_2_batchnorm_add_y_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1.1p-20)];
            tensor&lt;fp16, [1, 192, 128]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_2_batchnorm_add_1_cast = layer_norm(axes = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_2_batchnorm_add_1_axes_0, beta = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_2_batchnorm_ReadVariableOp_to_fp16, epsilon = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_2_batchnorm_add_y_to_fp16, gamma = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_2_batchnorm_mul_ReadVariableOp_to_fp16, x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_add_1_cast);
            tensor&lt;int32, [2]&gt; reshape_8_shape_0 = const()[name = tensor&lt;string, []&gt;("reshape_8_shape_0"), val = tensor&lt;int32, [2]&gt;([192, 128])];
            tensor&lt;fp16, [192, 128]&gt; reshape_8_cast = reshape(shape = reshape_8_shape_0, x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_2_batchnorm_add_1_cast);
            tensor&lt;bool, []&gt; matmul_4_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("matmul_4_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; matmul_4_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("matmul_4_transpose_y_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [128, 256]&gt; reshape_9_to_fp16 = const()[name = tensor&lt;string, []&gt;("reshape_9_to_fp16"), val = tensor&lt;fp16, [128, 256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4771968)))];
            tensor&lt;fp16, [192, 256]&gt; matmul_4_cast = matmul(transpose_x = matmul_4_transpose_x_0, transpose_y = matmul_4_transpose_y_0, x = reshape_8_cast, y = reshape_9_to_fp16);
            tensor&lt;int32, [4]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_key_einsum_Einsum_shape_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_key_einsum_Einsum_shape_0"), val = tensor&lt;int32, [4]&gt;([1, 192, 4, 64])];
            tensor&lt;fp16, [1, 192, 4, 64]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_key_einsum_Einsum_cast = reshape(shape = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_key_einsum_Einsum_shape_0, x = matmul_4_cast);
            tensor&lt;bool, []&gt; matmul_5_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("matmul_5_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; matmul_5_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("matmul_5_transpose_y_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [128, 256]&gt; reshape_11_to_fp16 = const()[name = tensor&lt;string, []&gt;("reshape_11_to_fp16"), val = tensor&lt;fp16, [128, 256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4837568)))];
            tensor&lt;fp16, [192, 256]&gt; matmul_5_cast = matmul(transpose_x = matmul_5_transpose_x_0, transpose_y = matmul_5_transpose_y_0, x = reshape_8_cast, y = reshape_11_to_fp16);
            tensor&lt;int32, [4]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_query_einsum_Einsum_shape_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_query_einsum_Einsum_shape_0"), val = tensor&lt;int32, [4]&gt;([1, 192, 4, 64])];
            tensor&lt;fp16, [1, 192, 4, 64]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_query_einsum_Einsum_cast = reshape(shape = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_query_einsum_Einsum_shape_0, x = matmul_5_cast);
            tensor&lt;bool, []&gt; matmul_6_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("matmul_6_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; matmul_6_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("matmul_6_transpose_y_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [128, 256]&gt; reshape_13_to_fp16 = const()[name = tensor&lt;string, []&gt;("reshape_13_to_fp16"), val = tensor&lt;fp16, [128, 256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4903168)))];
            tensor&lt;fp16, [192, 256]&gt; matmul_6_cast = matmul(transpose_x = matmul_6_transpose_x_0, transpose_y = matmul_6_transpose_y_0, x = reshape_8_cast, y = reshape_13_to_fp16);
            tensor&lt;int32, [4]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_value_einsum_Einsum_shape_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_value_einsum_Einsum_shape_0"), val = tensor&lt;int32, [4]&gt;([1, 192, 4, 64])];
            tensor&lt;fp16, [1, 192, 4, 64]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_value_einsum_Einsum_cast = reshape(shape = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_value_einsum_Einsum_shape_0, x = matmul_6_cast);
            tensor&lt;fp16, [4, 64]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_key_add_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_key_add_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [4, 64]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4968768)))];
            tensor&lt;fp16, [1, 192, 4, 64]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_key_add_cast = add(x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_key_einsum_Einsum_cast, y = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_key_add_ReadVariableOp_to_fp16);
            tensor&lt;fp16, [4, 64]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_query_add_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_query_add_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [4, 64]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4969344)))];
            tensor&lt;fp16, [1, 192, 4, 64]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_query_add_cast = add(x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_query_einsum_Einsum_cast, y = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_query_add_ReadVariableOp_to_fp16);
            tensor&lt;fp16, [4, 64]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_value_add_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_value_add_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [4, 64]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4969920)))];
            tensor&lt;fp16, [1, 192, 4, 64]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_value_add_cast = add(x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_value_einsum_Einsum_cast, y = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_value_add_ReadVariableOp_to_fp16);
            tensor&lt;fp16, []&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_Mul_y_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_Mul_y_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1p-3)];
            tensor&lt;fp16, [1, 192, 4, 64]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_Mul_cast = mul(x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_query_add_cast, y = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_Mul_y_to_fp16);
            tensor&lt;bool, []&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_einsum_Einsum_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_einsum_Einsum_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_einsum_Einsum_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_einsum_Einsum_transpose_y_0"), val = tensor&lt;bool, []&gt;(true)];
            tensor&lt;int32, [4]&gt; transpose_14_perm_0 = const()[name = tensor&lt;string, []&gt;("transpose_14_perm_0"), val = tensor&lt;int32, [4]&gt;([0, 2, 1, 3])];
            tensor&lt;int32, [4]&gt; transpose_15_perm_0 = const()[name = tensor&lt;string, []&gt;("transpose_15_perm_0"), val = tensor&lt;int32, [4]&gt;([0, 2, 1, 3])];
            tensor&lt;fp16, [1, 4, 192, 64]&gt; transpose_18 = transpose(perm = transpose_15_perm_0, x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_key_add_cast);
            tensor&lt;fp16, [1, 4, 192, 64]&gt; transpose_19 = transpose(perm = transpose_14_perm_0, x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_Mul_cast);
            tensor&lt;fp16, [1, 4, 192, 192]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_einsum_Einsum_cast = matmul(transpose_x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_einsum_Einsum_transpose_x_0, transpose_y = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_einsum_Einsum_transpose_y_0, x = transpose_19, y = transpose_18);
            tensor&lt;int32, []&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_softmax_Softmax_axis_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_softmax_Softmax_axis_0"), val = tensor&lt;int32, []&gt;(-1)];
            tensor&lt;fp16, [1, 4, 192, 192]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_softmax_Softmax_cast = softmax(axis = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_softmax_Softmax_axis_0, x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_einsum_Einsum_cast);
            tensor&lt;int32, [4]&gt; transpose_5_perm_0 = const()[name = tensor&lt;string, []&gt;("transpose_5_perm_0"), val = tensor&lt;int32, [4]&gt;([0, 2, 1, 3])];
            tensor&lt;bool, []&gt; matmul_7_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("matmul_7_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; matmul_7_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("matmul_7_transpose_y_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [1, 4, 192, 64]&gt; transpose_17 = transpose(perm = transpose_5_perm_0, x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_value_add_cast);
            tensor&lt;fp16, [1, 4, 192, 64]&gt; matmul_7_cast = matmul(transpose_x = matmul_7_transpose_x_0, transpose_y = matmul_7_transpose_y_0, x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_softmax_Softmax_cast, y = transpose_17);
            tensor&lt;int32, [4]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_einsum_1_Einsum_perm_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_einsum_1_Einsum_perm_0"), val = tensor&lt;int32, [4]&gt;([0, 2, 1, 3])];
            tensor&lt;int32, [3]&gt; reshape_14_shape_0 = const()[name = tensor&lt;string, []&gt;("reshape_14_shape_0"), val = tensor&lt;int32, [3]&gt;([1, 192, 256])];
            tensor&lt;fp16, [1, 192, 4, 64]&gt; transpose_16 = transpose(perm = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_einsum_1_Einsum_perm_0, x = matmul_7_cast);
            tensor&lt;fp16, [1, 192, 256]&gt; reshape_14_cast = reshape(shape = reshape_14_shape_0, x = transpose_16);
            tensor&lt;fp16, [128, 256]&gt; transpose_7_cast_to_fp32_to_fp16 = const()[name = tensor&lt;string, []&gt;("transpose_7_cast_to_fp32_to_fp16"), val = tensor&lt;fp16, [128, 256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4970496)))];
            tensor&lt;fp16, [128]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_attention_output_add_bias_0_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_attention_output_add_bias_0_to_fp16"), val = tensor&lt;fp16, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(5036096)))];
            tensor&lt;fp16, [1, 192, 128]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_attention_output_add_cast = linear(bias = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_attention_output_add_bias_0_to_fp16, weight = transpose_7_cast_to_fp32_to_fp16, x = reshape_14_cast);
            tensor&lt;fp16, [1, 192, 128]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_add_cast = add(x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_2_batchnorm_add_1_cast, y = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_attention_output_add_cast);
            tensor&lt;int32, [1]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_3_batchnorm_add_1_axes_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_3_batchnorm_add_1_axes_0"), val = tensor&lt;int32, [1]&gt;([2])];
            tensor&lt;fp16, [128]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_3_batchnorm_mul_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_3_batchnorm_mul_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(5036416)))];
            tensor&lt;fp16, [128]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_3_batchnorm_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_3_batchnorm_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(5036736)))];
            tensor&lt;fp16, []&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_3_batchnorm_add_y_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_3_batchnorm_add_y_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1.1p-20)];
            tensor&lt;fp16, [1, 192, 128]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_3_batchnorm_add_1_cast = layer_norm(axes = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_3_batchnorm_add_1_axes_0, beta = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_3_batchnorm_ReadVariableOp_to_fp16, epsilon = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_3_batchnorm_add_y_to_fp16, gamma = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_3_batchnorm_mul_ReadVariableOp_to_fp16, x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_add_cast);
            tensor&lt;int32, [3]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Tensordot_concat_1 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Tensordot_concat_1"), val = tensor&lt;int32, [3]&gt;([1, 192, 256])];
            tensor&lt;int32, [2]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Tensordot_stack = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Tensordot_stack"), val = tensor&lt;int32, [2]&gt;([192, 128])];
            tensor&lt;fp16, [192, 128]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Tensordot_Reshape_cast = reshape(shape = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Tensordot_stack, x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_3_batchnorm_add_1_cast);
            tensor&lt;bool, []&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Tensordot_MatMul_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Tensordot_MatMul_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Tensordot_MatMul_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Tensordot_MatMul_transpose_y_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [128, 256]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Tensordot_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Tensordot_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [128, 256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(5037056)))];
            tensor&lt;fp16, [192, 256]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Tensordot_MatMul_cast = matmul(transpose_x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Tensordot_MatMul_transpose_x_0, transpose_y = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Tensordot_MatMul_transpose_y_0, x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Tensordot_Reshape_cast, y = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Tensordot_ReadVariableOp_to_fp16);
            tensor&lt;fp16, [1, 192, 256]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Tensordot_cast = reshape(shape = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Tensordot_concat_1, x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Tensordot_MatMul_cast);
            tensor&lt;fp16, [256]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_BiasAdd_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_BiasAdd_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(5102656)))];
            tensor&lt;fp16, [1, 192, 256]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_BiasAdd_cast = add(x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Tensordot_cast, y = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_BiasAdd_ReadVariableOp_to_fp16);
            tensor&lt;string, []&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Gelu_mul_1_mode_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Gelu_mul_1_mode_0"), val = tensor&lt;string, []&gt;("EXACT")];
            tensor&lt;fp16, [1, 192, 256]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Gelu_mul_1_cast = gelu(mode = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Gelu_mul_1_mode_0, x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_BiasAdd_cast);
            tensor&lt;int32, [3]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_Tensordot_concat_1 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_Tensordot_concat_1"), val = tensor&lt;int32, [3]&gt;([1, 192, 128])];
            tensor&lt;int32, [2]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_Tensordot_stack = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_Tensordot_stack"), val = tensor&lt;int32, [2]&gt;([192, 256])];
            tensor&lt;fp16, [192, 256]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_Tensordot_Reshape_cast = reshape(shape = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_Tensordot_stack, x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Gelu_mul_1_cast);
            tensor&lt;bool, []&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_Tensordot_MatMul_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_Tensordot_MatMul_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_Tensordot_MatMul_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_Tensordot_MatMul_transpose_y_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [256, 128]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_Tensordot_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_Tensordot_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [256, 128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(5103232)))];
            tensor&lt;fp16, [192, 128]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_Tensordot_MatMul_cast = matmul(transpose_x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_Tensordot_MatMul_transpose_x_0, transpose_y = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_Tensordot_MatMul_transpose_y_0, x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_Tensordot_Reshape_cast, y = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_Tensordot_ReadVariableOp_to_fp16);
            tensor&lt;fp16, [1, 192, 128]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_Tensordot_cast = reshape(shape = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_Tensordot_concat_1, x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_Tensordot_MatMul_cast);
            tensor&lt;fp16, [128]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_BiasAdd_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_BiasAdd_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(5168832)))];
            tensor&lt;fp16, [1, 192, 128]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_BiasAdd_cast = add(x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_Tensordot_cast, y = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_BiasAdd_ReadVariableOp_to_fp16);
            tensor&lt;fp16, [1, 192, 128]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_add_1_cast = add(x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_3_batchnorm_add_1_cast, y = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_BiasAdd_cast);
            tensor&lt;int32, [1]&gt; model_1_tf_math_reduce_mean_1_Mean_axes_0 = const()[name = tensor&lt;string, []&gt;("model_1_tf_math_reduce_mean_1_Mean_axes_0"), val = tensor&lt;int32, [1]&gt;([-2])];
            tensor&lt;bool, []&gt; model_1_tf_math_reduce_mean_1_Mean_keep_dims_0 = const()[name = tensor&lt;string, []&gt;("model_1_tf_math_reduce_mean_1_Mean_keep_dims_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [1, 128]&gt; model_1_tf_math_reduce_mean_1_Mean_cast = reduce_mean(axes = model_1_tf_math_reduce_mean_1_Mean_axes_0, keep_dims = model_1_tf_math_reduce_mean_1_Mean_keep_dims_0, x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_add_1_cast);
            tensor&lt;string, []&gt; model_1_tf_math_reduce_mean_1_Mean_cast_to_fp32_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_1_tf_math_reduce_mean_1_Mean_cast_to_fp32_dtype_0"), val = tensor&lt;string, []&gt;("fp32")];
            tensor&lt;fp32, [1, 128]&gt; Identity = cast(dtype = model_1_tf_math_reduce_mean_1_Mean_cast_to_fp32_dtype_0, x = model_1_tf_math_reduce_mean_1_Mean_cast);
        } -&gt; (Identity);
}</code></pre>
                    </div>
                </div>
            
                <div class="file-section" id="file-14">
                    <div class="file-header">
                        <h2>ðŸ“„ model.mil 17</h2>
                        <div class="file-meta">Size: 17.86 KB</div>
                    </div>
                    <div class="file-content">
                        <pre><code>program(1.0)
[buildInfo = dict&lt;tensor&lt;string, []&gt;, tensor&lt;string, []&gt;&gt;({{"coremlc-component-MIL", "3404.16.1"}, {"coremlc-version", "3404.24.1"}})]
{
    func main&lt;ios16&gt;(tensor&lt;fp16, [1, 384, 768]&gt; input_1, tensor&lt;int32, [1, 384, 1]&gt; input_2, tensor&lt;int32, [1, 384, 24]&gt; input_3, tensor&lt;int32, [1, 384, 1]&gt; input_4) {
            tensor&lt;string, []&gt; input_1_dtype_0 = const()[name = tensor&lt;string, []&gt;("input_1_dtype_0"), val = tensor&lt;string, []&gt;("fp32")];
            tensor&lt;int32, []&gt; model_extend_layer_concat_1_axis = const()[name = tensor&lt;string, []&gt;("model_extend_layer_concat_1_axis"), val = tensor&lt;int32, []&gt;(1)];
            tensor&lt;int32, []&gt; model_extend_layer_concat_axis = const()[name = tensor&lt;string, []&gt;("model_extend_layer_concat_axis"), val = tensor&lt;int32, []&gt;(1)];
            tensor&lt;int32, []&gt; model_tf_math_equal_Equal_y = const()[name = tensor&lt;string, []&gt;("model_tf_math_equal_Equal_y"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, []&gt; model_tf_math_equal_1_Equal_y = const()[name = tensor&lt;string, []&gt;("model_tf_math_equal_1_Equal_y"), val = tensor&lt;int32, []&gt;(2)];
            tensor&lt;int32, [192]&gt; model_304 = const()[name = tensor&lt;string, []&gt;("model_304"), val = tensor&lt;int32, [192]&gt;([384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384])];
            tensor&lt;int32, []&gt; model_tf_concat_concat_axis = const()[name = tensor&lt;string, []&gt;("model_tf_concat_concat_axis"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, []&gt; model_extend_layer_concat_3_axis = const()[name = tensor&lt;string, []&gt;("model_extend_layer_concat_3_axis"), val = tensor&lt;int32, []&gt;(1)];
            tensor&lt;int32, []&gt; model_extend_layer_concat_2_axis = const()[name = tensor&lt;string, []&gt;("model_extend_layer_concat_2_axis"), val = tensor&lt;int32, []&gt;(1)];
            tensor&lt;int32, []&gt; model_tf_math_equal_3_Equal_y = const()[name = tensor&lt;string, []&gt;("model_tf_math_equal_3_Equal_y"), val = tensor&lt;int32, []&gt;(3)];
            tensor&lt;int32, [192]&gt; model_308 = const()[name = tensor&lt;string, []&gt;("model_308"), val = tensor&lt;int32, [192]&gt;([384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384])];
            tensor&lt;int32, []&gt; model_tf_concat_1_concat_axis = const()[name = tensor&lt;string, []&gt;("model_tf_concat_1_concat_axis"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, [1, 1, 1]&gt; model_extend_layer_zeros = const()[name = tensor&lt;string, []&gt;("model_extend_layer_zeros"), val = tensor&lt;int32, [1, 1, 1]&gt;([[[0]]])];
            tensor&lt;int32, [1, 1, 1]&gt; model_extend_layer_zeros_3 = const()[name = tensor&lt;string, []&gt;("model_extend_layer_zeros_3"), val = tensor&lt;int32, [1, 1, 1]&gt;([[[0]]])];
            tensor&lt;int32, [1, 1, 24]&gt; model_extend_layer_zeros_2 = const()[name = tensor&lt;string, []&gt;("model_extend_layer_zeros_2"), val = tensor&lt;int32, [1, 1, 24]&gt;([[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]])];
            tensor&lt;fp32, [1, 1, 768]&gt; model_extend_layer_zeros_1 = const()[name = tensor&lt;string, []&gt;("model_extend_layer_zeros_1"), val = tensor&lt;fp32, [1, 1, 768]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(64)))];
            tensor&lt;bool, []&gt; model_extend_layer_concat_interleave_0 = const()[name = tensor&lt;string, []&gt;("model_extend_layer_concat_interleave_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;int32, [1, 385, 1]&gt; model_extend_layer_concat = concat(axis = model_extend_layer_concat_axis, interleave = model_extend_layer_concat_interleave_0, values = (input_4, model_extend_layer_zeros));
            tensor&lt;bool, []&gt; model_extend_layer_concat_3_interleave_0 = const()[name = tensor&lt;string, []&gt;("model_extend_layer_concat_3_interleave_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;int32, [1, 385, 1]&gt; model_extend_layer_concat_3 = concat(axis = model_extend_layer_concat_3_axis, interleave = model_extend_layer_concat_3_interleave_0, values = (input_2, model_extend_layer_zeros_3));
            tensor&lt;bool, []&gt; model_extend_layer_concat_2_interleave_0 = const()[name = tensor&lt;string, []&gt;("model_extend_layer_concat_2_interleave_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;int32, [1, 385, 24]&gt; model_extend_layer_concat_2 = concat(axis = model_extend_layer_concat_2_axis, interleave = model_extend_layer_concat_2_interleave_0, values = (input_3, model_extend_layer_zeros_2));
            tensor&lt;bool, []&gt; model_extend_layer_concat_1_interleave_0 = const()[name = tensor&lt;string, []&gt;("model_extend_layer_concat_1_interleave_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp32, [1, 384, 768]&gt; cast_1 = cast(dtype = input_1_dtype_0, x = input_1);
            tensor&lt;fp32, [1, 385, 768]&gt; model_extend_layer_concat_1 = concat(axis = model_extend_layer_concat_1_axis, interleave = model_extend_layer_concat_1_interleave_0, values = (cast_1, model_extend_layer_zeros_1));
            tensor&lt;int32, [3]&gt; model_tf___operators___getitem_strided_slice_begin_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_strided_slice_begin_0"), val = tensor&lt;int32, [3]&gt;([0, 0, 0])];
            tensor&lt;int32, [3]&gt; model_tf___operators___getitem_strided_slice_end_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_strided_slice_end_0"), val = tensor&lt;int32, [3]&gt;([1, 0, 1])];
            tensor&lt;int32, [3]&gt; model_tf___operators___getitem_strided_slice_stride_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_strided_slice_stride_0"), val = tensor&lt;int32, [3]&gt;([1, 1, 1])];
            tensor&lt;bool, [3]&gt; model_tf___operators___getitem_strided_slice_begin_mask_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_strided_slice_begin_mask_0"), val = tensor&lt;bool, [3]&gt;([false, true, false])];
            tensor&lt;bool, [3]&gt; model_tf___operators___getitem_strided_slice_end_mask_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_strided_slice_end_mask_0"), val = tensor&lt;bool, [3]&gt;([false, true, false])];
            tensor&lt;bool, [3]&gt; model_tf___operators___getitem_strided_slice_squeeze_mask_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_strided_slice_squeeze_mask_0"), val = tensor&lt;bool, [3]&gt;([true, false, true])];
            tensor&lt;int32, [385]&gt; model_tf___operators___getitem_strided_slice = slice_by_index(begin = model_tf___operators___getitem_strided_slice_begin_0, begin_mask = model_tf___operators___getitem_strided_slice_begin_mask_0, end = model_tf___operators___getitem_strided_slice_end_0, end_mask = model_tf___operators___getitem_strided_slice_end_mask_0, squeeze_mask = model_tf___operators___getitem_strided_slice_squeeze_mask_0, stride = model_tf___operators___getitem_strided_slice_stride_0, x = model_extend_layer_concat);
            tensor&lt;bool, [385]&gt; model_tf_math_equal_Equal = equal(x = model_tf___operators___getitem_strided_slice, y = model_tf_math_equal_Equal_y);
            tensor&lt;bool, [385]&gt; model_tf_math_equal_1_Equal = equal(x = model_tf___operators___getitem_strided_slice, y = model_tf_math_equal_1_Equal_y);
            tensor&lt;bool, [385]&gt; model_tf_math_equal_3_Equal = equal(x = model_tf___operators___getitem_strided_slice, y = model_tf_math_equal_3_Equal_y);
            tensor&lt;bool, [385]&gt; model_tf_math_logical_or_LogicalOr = logical_or(x = model_tf_math_equal_Equal, y = model_tf_math_equal_1_Equal);
            tensor&lt;bool, [385]&gt; model_tf_math_logical_or_1_LogicalOr = logical_or(x = model_tf_math_equal_Equal, y = model_tf_math_equal_3_Equal);
            tensor&lt;int32, [?, 1]&gt; model_tf_where_Where = non_zero(x = model_tf_math_logical_or_LogicalOr);
            tensor&lt;int32, [?, 1]&gt; model_tf_where_1_Where = non_zero(x = model_tf_math_logical_or_1_LogicalOr);
            tensor&lt;int32, [1]&gt; model_tf_compat_v1_squeeze_Squeeze_axes_0 = const()[name = tensor&lt;string, []&gt;("model_tf_compat_v1_squeeze_Squeeze_axes_0"), val = tensor&lt;int32, [1]&gt;([-1])];
            tensor&lt;int32, [?]&gt; model_tf_compat_v1_squeeze_Squeeze = squeeze(axes = model_tf_compat_v1_squeeze_Squeeze_axes_0, x = model_tf_where_Where);
            tensor&lt;int32, [1]&gt; model_tf_compat_v1_squeeze_1_Squeeze_axes_0 = const()[name = tensor&lt;string, []&gt;("model_tf_compat_v1_squeeze_1_Squeeze_axes_0"), val = tensor&lt;int32, [1]&gt;([-1])];
            tensor&lt;int32, [?]&gt; model_tf_compat_v1_squeeze_1_Squeeze = squeeze(axes = model_tf_compat_v1_squeeze_1_Squeeze_axes_0, x = model_tf_where_1_Where);
            tensor&lt;bool, []&gt; model_tf_concat_concat_interleave_0 = const()[name = tensor&lt;string, []&gt;("model_tf_concat_concat_interleave_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;int32, [?]&gt; model_tf_concat_concat = concat(axis = model_tf_concat_concat_axis, interleave = model_tf_concat_concat_interleave_0, values = (model_tf_compat_v1_squeeze_Squeeze, model_304));
            tensor&lt;bool, []&gt; model_tf_concat_1_concat_interleave_0 = const()[name = tensor&lt;string, []&gt;("model_tf_concat_1_concat_interleave_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;int32, [?]&gt; model_tf_concat_1_concat = concat(axis = model_tf_concat_1_concat_axis, interleave = model_tf_concat_1_concat_interleave_0, values = (model_tf_compat_v1_squeeze_1_Squeeze, model_308));
            tensor&lt;int32, [1]&gt; model_tf___operators___getitem_2_strided_slice_begin_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_2_strided_slice_begin_0"), val = tensor&lt;int32, [1]&gt;([0])];
            tensor&lt;int32, [1]&gt; model_tf___operators___getitem_2_strided_slice_end_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_2_strided_slice_end_0"), val = tensor&lt;int32, [1]&gt;([192])];
            tensor&lt;int32, [1]&gt; model_tf___operators___getitem_2_strided_slice_stride_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_2_strided_slice_stride_0"), val = tensor&lt;int32, [1]&gt;([1])];
            tensor&lt;bool, [1]&gt; model_tf___operators___getitem_2_strided_slice_begin_mask_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_2_strided_slice_begin_mask_0"), val = tensor&lt;bool, [1]&gt;([true])];
            tensor&lt;bool, [1]&gt; model_tf___operators___getitem_2_strided_slice_end_mask_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_2_strided_slice_end_mask_0"), val = tensor&lt;bool, [1]&gt;([false])];
            tensor&lt;bool, [1]&gt; model_tf___operators___getitem_2_strided_slice_squeeze_mask_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_2_strided_slice_squeeze_mask_0"), val = tensor&lt;bool, [1]&gt;([false])];
            tensor&lt;int32, [192]&gt; model_tf___operators___getitem_2_strided_slice = slice_by_index(begin = model_tf___operators___getitem_2_strided_slice_begin_0, begin_mask = model_tf___operators___getitem_2_strided_slice_begin_mask_0, end = model_tf___operators___getitem_2_strided_slice_end_0, end_mask = model_tf___operators___getitem_2_strided_slice_end_mask_0, squeeze_mask = model_tf___operators___getitem_2_strided_slice_squeeze_mask_0, stride = model_tf___operators___getitem_2_strided_slice_stride_0, x = model_tf_concat_concat);
            tensor&lt;int32, [1]&gt; model_tf___operators___getitem_5_strided_slice_begin_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_5_strided_slice_begin_0"), val = tensor&lt;int32, [1]&gt;([0])];
            tensor&lt;int32, [1]&gt; model_tf___operators___getitem_5_strided_slice_end_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_5_strided_slice_end_0"), val = tensor&lt;int32, [1]&gt;([192])];
            tensor&lt;int32, [1]&gt; model_tf___operators___getitem_5_strided_slice_stride_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_5_strided_slice_stride_0"), val = tensor&lt;int32, [1]&gt;([1])];
            tensor&lt;bool, [1]&gt; model_tf___operators___getitem_5_strided_slice_begin_mask_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_5_strided_slice_begin_mask_0"), val = tensor&lt;bool, [1]&gt;([true])];
            tensor&lt;bool, [1]&gt; model_tf___operators___getitem_5_strided_slice_end_mask_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_5_strided_slice_end_mask_0"), val = tensor&lt;bool, [1]&gt;([false])];
            tensor&lt;bool, [1]&gt; model_tf___operators___getitem_5_strided_slice_squeeze_mask_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_5_strided_slice_squeeze_mask_0"), val = tensor&lt;bool, [1]&gt;([false])];
            tensor&lt;int32, [192]&gt; model_tf___operators___getitem_5_strided_slice = slice_by_index(begin = model_tf___operators___getitem_5_strided_slice_begin_0, begin_mask = model_tf___operators___getitem_5_strided_slice_begin_mask_0, end = model_tf___operators___getitem_5_strided_slice_end_0, end_mask = model_tf___operators___getitem_5_strided_slice_end_mask_0, squeeze_mask = model_tf___operators___getitem_5_strided_slice_squeeze_mask_0, stride = model_tf___operators___getitem_5_strided_slice_stride_0, x = model_tf_concat_1_concat);
            tensor&lt;int32, []&gt; model_tf_compat_v1_gather_GatherV2_axis_0 = const()[name = tensor&lt;string, []&gt;("model_tf_compat_v1_gather_GatherV2_axis_0"), val = tensor&lt;int32, []&gt;(1)];
            tensor&lt;int32, []&gt; model_tf_compat_v1_gather_GatherV2_batch_dims_0 = const()[name = tensor&lt;string, []&gt;("model_tf_compat_v1_gather_GatherV2_batch_dims_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;fp32, [1, 192, 768]&gt; model_tf_compat_v1_gather_GatherV2 = gather(axis = model_tf_compat_v1_gather_GatherV2_axis_0, batch_dims = model_tf_compat_v1_gather_GatherV2_batch_dims_0, indices = model_tf___operators___getitem_2_strided_slice, x = model_extend_layer_concat_1);
            tensor&lt;int32, []&gt; model_tf_compat_v1_gather_1_GatherV2_axis_0 = const()[name = tensor&lt;string, []&gt;("model_tf_compat_v1_gather_1_GatherV2_axis_0"), val = tensor&lt;int32, []&gt;(1)];
            tensor&lt;int32, []&gt; model_tf_compat_v1_gather_1_GatherV2_batch_dims_0 = const()[name = tensor&lt;string, []&gt;("model_tf_compat_v1_gather_1_GatherV2_batch_dims_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, [1, 192, 1]&gt; Identity_1 = gather(axis = model_tf_compat_v1_gather_1_GatherV2_axis_0, batch_dims = model_tf_compat_v1_gather_1_GatherV2_batch_dims_0, indices = model_tf___operators___getitem_2_strided_slice, x = model_extend_layer_concat_3);
            tensor&lt;int32, []&gt; model_tf_compat_v1_gather_2_GatherV2_axis_0 = const()[name = tensor&lt;string, []&gt;("model_tf_compat_v1_gather_2_GatherV2_axis_0"), val = tensor&lt;int32, []&gt;(1)];
            tensor&lt;int32, []&gt; model_tf_compat_v1_gather_2_GatherV2_batch_dims_0 = const()[name = tensor&lt;string, []&gt;("model_tf_compat_v1_gather_2_GatherV2_batch_dims_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, [1, 192, 24]&gt; Identity_2 = gather(axis = model_tf_compat_v1_gather_2_GatherV2_axis_0, batch_dims = model_tf_compat_v1_gather_2_GatherV2_batch_dims_0, indices = model_tf___operators___getitem_2_strided_slice, x = model_extend_layer_concat_2);
            tensor&lt;int32, []&gt; model_tf_compat_v1_gather_3_GatherV2_axis_0 = const()[name = tensor&lt;string, []&gt;("model_tf_compat_v1_gather_3_GatherV2_axis_0"), val = tensor&lt;int32, []&gt;(1)];
            tensor&lt;int32, []&gt; model_tf_compat_v1_gather_3_GatherV2_batch_dims_0 = const()[name = tensor&lt;string, []&gt;("model_tf_compat_v1_gather_3_GatherV2_batch_dims_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, [1, 192, 1]&gt; Identity_3 = gather(axis = model_tf_compat_v1_gather_3_GatherV2_axis_0, batch_dims = model_tf_compat_v1_gather_3_GatherV2_batch_dims_0, indices = model_tf___operators___getitem_5_strided_slice, x = model_extend_layer_concat_3);
            tensor&lt;int32, []&gt; model_tf_compat_v1_gather_4_GatherV2_axis_0 = const()[name = tensor&lt;string, []&gt;("model_tf_compat_v1_gather_4_GatherV2_axis_0"), val = tensor&lt;int32, []&gt;(1)];
            tensor&lt;int32, []&gt; model_tf_compat_v1_gather_4_GatherV2_batch_dims_0 = const()[name = tensor&lt;string, []&gt;("model_tf_compat_v1_gather_4_GatherV2_batch_dims_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, [1, 192, 24]&gt; Identity_4 = gather(axis = model_tf_compat_v1_gather_4_GatherV2_axis_0, batch_dims = model_tf_compat_v1_gather_4_GatherV2_batch_dims_0, indices = model_tf___operators___getitem_5_strided_slice, x = model_extend_layer_concat_2);
            tensor&lt;string, []&gt; model_tf_cast_3_Cast_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_tf_cast_3_Cast_dtype_0"), val = tensor&lt;string, []&gt;("fp16")];
            tensor&lt;fp16, [1, 192, 768]&gt; Identity = cast(dtype = model_tf_cast_3_Cast_dtype_0, x = model_tf_compat_v1_gather_GatherV2);
        } -&gt; (Identity, Identity_1, Identity_2, Identity_3, Identity_4);
}</code></pre>
                    </div>
                </div>
            
                <div class="file-section" id="file-15">
                    <div class="file-header">
                        <h2>ðŸ“„ model.mil 24</h2>
                        <div class="file-meta">Size: 65.21 KB</div>
                    </div>
                    <div class="file-content">
                        <pre><code>program(1.0)
[buildInfo = dict&lt;tensor&lt;string, []&gt;, tensor&lt;string, []&gt;&gt;({{"coremlc-component-MIL", "3404.16.1"}, {"coremlc-version", "3404.24.1"}})]
{
    func main&lt;ios16&gt;(tensor&lt;fp16, [1, 192, 1, 768]&gt; hist_article_encoding, tensor&lt;int32, [1, 192]&gt; hist_channel_id, tensor&lt;int32, [1, 192, 24]&gt; hist_topic_ids) {
            tensor&lt;int32, [1]&gt; model_1_tf_compat_v1_squeeze_Squeeze_axes_0 = const()[name = tensor&lt;string, []&gt;("model_1_tf_compat_v1_squeeze_Squeeze_axes_0"), val = tensor&lt;int32, [1]&gt;([-2])];
            tensor&lt;fp16, [1, 192, 768]&gt; model_1_tf_compat_v1_squeeze_Squeeze_cast = squeeze(axes = model_1_tf_compat_v1_squeeze_Squeeze_axes_0, x = hist_article_encoding);
            tensor&lt;int32, []&gt; model_1_embedding_1_embedding_lookup_axis_0 = const()[name = tensor&lt;string, []&gt;("model_1_embedding_1_embedding_lookup_axis_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, []&gt; model_1_embedding_1_embedding_lookup_batch_dims_0 = const()[name = tensor&lt;string, []&gt;("model_1_embedding_1_embedding_lookup_batch_dims_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;fp16, [2012, 128]&gt; model_1_embedding_1_embedding_lookup_6588_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_embedding_1_embedding_lookup_6588_to_fp16"), val = tensor&lt;fp16, [2012, 128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(64)))];
            tensor&lt;fp16, [1, 192, 128]&gt; model_1_embedding_1_embedding_lookup_cast = gather(axis = model_1_embedding_1_embedding_lookup_axis_0, batch_dims = model_1_embedding_1_embedding_lookup_batch_dims_0, indices = hist_channel_id, x = model_1_embedding_1_embedding_lookup_6588_to_fp16);
            tensor&lt;int32, []&gt; model_1_embedding_2_embedding_lookup_axis_0 = const()[name = tensor&lt;string, []&gt;("model_1_embedding_2_embedding_lookup_axis_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, []&gt; model_1_embedding_2_embedding_lookup_batch_dims_0 = const()[name = tensor&lt;string, []&gt;("model_1_embedding_2_embedding_lookup_batch_dims_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;fp16, [13980, 128]&gt; model_1_embedding_2_embedding_lookup_6557_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_embedding_2_embedding_lookup_6557_to_fp16"), val = tensor&lt;fp16, [13980, 128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(515200)))];
            tensor&lt;fp16, [1, 192, 24, 128]&gt; model_1_embedding_2_embedding_lookup_cast = gather(axis = model_1_embedding_2_embedding_lookup_axis_0, batch_dims = model_1_embedding_2_embedding_lookup_batch_dims_0, indices = hist_topic_ids, x = model_1_embedding_2_embedding_lookup_6557_to_fp16);
            tensor&lt;int32, [1]&gt; model_1_tf_math_reduce_mean_Mean_axes_0 = const()[name = tensor&lt;string, []&gt;("model_1_tf_math_reduce_mean_Mean_axes_0"), val = tensor&lt;int32, [1]&gt;([-2])];
            tensor&lt;bool, []&gt; model_1_tf_math_reduce_mean_Mean_keep_dims_0 = const()[name = tensor&lt;string, []&gt;("model_1_tf_math_reduce_mean_Mean_keep_dims_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [1, 192, 128]&gt; model_1_tf_math_reduce_mean_Mean_cast = reduce_mean(axes = model_1_tf_math_reduce_mean_Mean_axes_0, keep_dims = model_1_tf_math_reduce_mean_Mean_keep_dims_0, x = model_1_embedding_2_embedding_lookup_cast);
            tensor&lt;int32, [3]&gt; model_1_dense_Tensordot_concat_1 = const()[name = tensor&lt;string, []&gt;("model_1_dense_Tensordot_concat_1"), val = tensor&lt;int32, [3]&gt;([1, 192, 128])];
            tensor&lt;int32, [2]&gt; model_1_dense_Tensordot_stack = const()[name = tensor&lt;string, []&gt;("model_1_dense_Tensordot_stack"), val = tensor&lt;int32, [2]&gt;([192, 768])];
            tensor&lt;fp16, [192, 768]&gt; model_1_dense_Tensordot_Reshape_cast = reshape(shape = model_1_dense_Tensordot_stack, x = model_1_tf_compat_v1_squeeze_Squeeze_cast);
            tensor&lt;bool, []&gt; model_1_dense_Tensordot_MatMul_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("model_1_dense_Tensordot_MatMul_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; model_1_dense_Tensordot_MatMul_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("model_1_dense_Tensordot_MatMul_transpose_y_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [768, 128]&gt; model_1_dense_Tensordot_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_dense_Tensordot_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [768, 128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4094144)))];
            tensor&lt;fp16, [192, 128]&gt; model_1_dense_Tensordot_MatMul_cast = matmul(transpose_x = model_1_dense_Tensordot_MatMul_transpose_x_0, transpose_y = model_1_dense_Tensordot_MatMul_transpose_y_0, x = model_1_dense_Tensordot_Reshape_cast, y = model_1_dense_Tensordot_ReadVariableOp_to_fp16);
            tensor&lt;fp16, [1, 192, 128]&gt; model_1_dense_Tensordot_cast = reshape(shape = model_1_dense_Tensordot_concat_1, x = model_1_dense_Tensordot_MatMul_cast);
            tensor&lt;fp16, [128]&gt; model_1_dense_BiasAdd_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_dense_BiasAdd_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4290816)))];
            tensor&lt;fp16, [1, 192, 128]&gt; model_1_dense_BiasAdd_cast = add(x = model_1_dense_Tensordot_cast, y = model_1_dense_BiasAdd_ReadVariableOp_to_fp16);
            tensor&lt;string, []&gt; model_1_dense_Gelu_mul_1_mode_0 = const()[name = tensor&lt;string, []&gt;("model_1_dense_Gelu_mul_1_mode_0"), val = tensor&lt;string, []&gt;("EXACT")];
            tensor&lt;fp16, [1, 192, 128]&gt; model_1_dense_Gelu_mul_1_cast = gelu(mode = model_1_dense_Gelu_mul_1_mode_0, x = model_1_dense_BiasAdd_cast);
            tensor&lt;int32, [3]&gt; model_1_dense_1_Tensordot_concat_1 = const()[name = tensor&lt;string, []&gt;("model_1_dense_1_Tensordot_concat_1"), val = tensor&lt;int32, [3]&gt;([1, 192, 128])];
            tensor&lt;int32, [2]&gt; model_1_dense_1_Tensordot_stack = const()[name = tensor&lt;string, []&gt;("model_1_dense_1_Tensordot_stack"), val = tensor&lt;int32, [2]&gt;([192, 128])];
            tensor&lt;fp16, [192, 128]&gt; model_1_dense_1_Tensordot_Reshape_cast = reshape(shape = model_1_dense_1_Tensordot_stack, x = model_1_dense_Gelu_mul_1_cast);
            tensor&lt;bool, []&gt; model_1_dense_1_Tensordot_MatMul_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("model_1_dense_1_Tensordot_MatMul_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; model_1_dense_1_Tensordot_MatMul_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("model_1_dense_1_Tensordot_MatMul_transpose_y_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [128, 128]&gt; model_1_dense_1_Tensordot_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_dense_1_Tensordot_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [128, 128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4291136)))];
            tensor&lt;fp16, [192, 128]&gt; model_1_dense_1_Tensordot_MatMul_cast = matmul(transpose_x = model_1_dense_1_Tensordot_MatMul_transpose_x_0, transpose_y = model_1_dense_1_Tensordot_MatMul_transpose_y_0, x = model_1_dense_1_Tensordot_Reshape_cast, y = model_1_dense_1_Tensordot_ReadVariableOp_to_fp16);
            tensor&lt;fp16, [1, 192, 128]&gt; model_1_dense_1_Tensordot_cast = reshape(shape = model_1_dense_1_Tensordot_concat_1, x = model_1_dense_1_Tensordot_MatMul_cast);
            tensor&lt;fp16, [128]&gt; model_1_dense_1_BiasAdd_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_dense_1_BiasAdd_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4323968)))];
            tensor&lt;fp16, [1, 192, 128]&gt; model_1_dense_1_BiasAdd_cast = add(x = model_1_dense_1_Tensordot_cast, y = model_1_dense_1_BiasAdd_ReadVariableOp_to_fp16);
            tensor&lt;fp16, [1, 192, 128]&gt; model_1_tf___operators___add_AddV2_cast = add(x = model_1_dense_1_BiasAdd_cast, y = model_1_embedding_1_embedding_lookup_cast);
            tensor&lt;fp16, [1, 192, 128]&gt; model_1_tf___operators___add_1_AddV2_cast = add(x = model_1_tf___operators___add_AddV2_cast, y = model_1_tf_math_reduce_mean_Mean_cast);
            tensor&lt;fp16, [192, 128]&gt; model_1_p_emb_StatefulPartitionedCall_StatefulPartitionedCall_embedding_3_embedding_lookup_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_p_emb_StatefulPartitionedCall_StatefulPartitionedCall_embedding_3_embedding_lookup_to_fp16"), val = tensor&lt;fp16, [192, 128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4324288)))];
            tensor&lt;fp16, [1, 192, 128]&gt; model_1_p_emb_StatefulPartitionedCall_StatefulPartitionedCall_add_cast = add(x = model_1_p_emb_StatefulPartitionedCall_StatefulPartitionedCall_embedding_3_embedding_lookup_to_fp16, y = model_1_tf___operators___add_1_AddV2_cast);
            tensor&lt;int32, [1]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_batchnorm_add_1_axes_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_batchnorm_add_1_axes_0"), val = tensor&lt;int32, [1]&gt;([2])];
            tensor&lt;fp16, [128]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_batchnorm_mul_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_batchnorm_mul_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4373504)))];
            tensor&lt;fp16, [128]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_batchnorm_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_batchnorm_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4373824)))];
            tensor&lt;fp16, []&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_batchnorm_add_y_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_batchnorm_add_y_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1.1p-20)];
            tensor&lt;fp16, [1, 192, 128]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_batchnorm_add_1_cast = layer_norm(axes = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_batchnorm_add_1_axes_0, beta = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_batchnorm_ReadVariableOp_to_fp16, epsilon = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_batchnorm_add_y_to_fp16, gamma = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_batchnorm_mul_ReadVariableOp_to_fp16, x = model_1_p_emb_StatefulPartitionedCall_StatefulPartitionedCall_add_cast);
            tensor&lt;int32, [2]&gt; reshape_0_shape_0 = const()[name = tensor&lt;string, []&gt;("reshape_0_shape_0"), val = tensor&lt;int32, [2]&gt;([192, 128])];
            tensor&lt;fp16, [192, 128]&gt; reshape_0_cast = reshape(shape = reshape_0_shape_0, x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_batchnorm_add_1_cast);
            tensor&lt;bool, []&gt; matmul_0_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("matmul_0_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; matmul_0_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("matmul_0_transpose_y_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [128, 256]&gt; reshape_1_to_fp16 = const()[name = tensor&lt;string, []&gt;("reshape_1_to_fp16"), val = tensor&lt;fp16, [128, 256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4374144)))];
            tensor&lt;fp16, [192, 256]&gt; matmul_0_cast = matmul(transpose_x = matmul_0_transpose_x_0, transpose_y = matmul_0_transpose_y_0, x = reshape_0_cast, y = reshape_1_to_fp16);
            tensor&lt;int32, [4]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_key_einsum_Einsum_shape_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_key_einsum_Einsum_shape_0"), val = tensor&lt;int32, [4]&gt;([1, 192, 4, 64])];
            tensor&lt;fp16, [1, 192, 4, 64]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_key_einsum_Einsum_cast = reshape(shape = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_key_einsum_Einsum_shape_0, x = matmul_0_cast);
            tensor&lt;bool, []&gt; matmul_1_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("matmul_1_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; matmul_1_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("matmul_1_transpose_y_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [128, 256]&gt; reshape_3_to_fp16 = const()[name = tensor&lt;string, []&gt;("reshape_3_to_fp16"), val = tensor&lt;fp16, [128, 256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4439744)))];
            tensor&lt;fp16, [192, 256]&gt; matmul_1_cast = matmul(transpose_x = matmul_1_transpose_x_0, transpose_y = matmul_1_transpose_y_0, x = reshape_0_cast, y = reshape_3_to_fp16);
            tensor&lt;int32, [4]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_query_einsum_Einsum_shape_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_query_einsum_Einsum_shape_0"), val = tensor&lt;int32, [4]&gt;([1, 192, 4, 64])];
            tensor&lt;fp16, [1, 192, 4, 64]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_query_einsum_Einsum_cast = reshape(shape = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_query_einsum_Einsum_shape_0, x = matmul_1_cast);
            tensor&lt;bool, []&gt; matmul_2_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("matmul_2_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; matmul_2_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("matmul_2_transpose_y_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [128, 256]&gt; reshape_5_to_fp16 = const()[name = tensor&lt;string, []&gt;("reshape_5_to_fp16"), val = tensor&lt;fp16, [128, 256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4505344)))];
            tensor&lt;fp16, [192, 256]&gt; matmul_2_cast = matmul(transpose_x = matmul_2_transpose_x_0, transpose_y = matmul_2_transpose_y_0, x = reshape_0_cast, y = reshape_5_to_fp16);
            tensor&lt;int32, [4]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_value_einsum_Einsum_shape_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_value_einsum_Einsum_shape_0"), val = tensor&lt;int32, [4]&gt;([1, 192, 4, 64])];
            tensor&lt;fp16, [1, 192, 4, 64]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_value_einsum_Einsum_cast = reshape(shape = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_value_einsum_Einsum_shape_0, x = matmul_2_cast);
            tensor&lt;fp16, [4, 64]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_key_add_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_key_add_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [4, 64]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4570944)))];
            tensor&lt;fp16, [1, 192, 4, 64]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_key_add_cast = add(x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_key_einsum_Einsum_cast, y = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_key_add_ReadVariableOp_to_fp16);
            tensor&lt;fp16, [4, 64]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_query_add_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_query_add_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [4, 64]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4571520)))];
            tensor&lt;fp16, [1, 192, 4, 64]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_query_add_cast = add(x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_query_einsum_Einsum_cast, y = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_query_add_ReadVariableOp_to_fp16);
            tensor&lt;fp16, [4, 64]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_value_add_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_value_add_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [4, 64]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4572096)))];
            tensor&lt;fp16, [1, 192, 4, 64]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_value_add_cast = add(x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_value_einsum_Einsum_cast, y = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_value_add_ReadVariableOp_to_fp16);
            tensor&lt;fp16, []&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_Mul_y_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_Mul_y_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1p-3)];
            tensor&lt;fp16, [1, 192, 4, 64]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_Mul_cast = mul(x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_query_add_cast, y = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_Mul_y_to_fp16);
            tensor&lt;bool, []&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_einsum_Einsum_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_einsum_Einsum_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_einsum_Einsum_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_einsum_Einsum_transpose_y_0"), val = tensor&lt;bool, []&gt;(true)];
            tensor&lt;int32, [4]&gt; transpose_12_perm_0 = const()[name = tensor&lt;string, []&gt;("transpose_12_perm_0"), val = tensor&lt;int32, [4]&gt;([0, 2, 1, 3])];
            tensor&lt;int32, [4]&gt; transpose_13_perm_0 = const()[name = tensor&lt;string, []&gt;("transpose_13_perm_0"), val = tensor&lt;int32, [4]&gt;([0, 2, 1, 3])];
            tensor&lt;fp16, [1, 4, 192, 64]&gt; transpose_22 = transpose(perm = transpose_13_perm_0, x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_key_add_cast);
            tensor&lt;fp16, [1, 4, 192, 64]&gt; transpose_23 = transpose(perm = transpose_12_perm_0, x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_Mul_cast);
            tensor&lt;fp16, [1, 4, 192, 192]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_einsum_Einsum_cast = matmul(transpose_x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_einsum_Einsum_transpose_x_0, transpose_y = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_einsum_Einsum_transpose_y_0, x = transpose_23, y = transpose_22);
            tensor&lt;int32, []&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_softmax_Softmax_axis_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_softmax_Softmax_axis_0"), val = tensor&lt;int32, []&gt;(-1)];
            tensor&lt;fp16, [1, 4, 192, 192]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_softmax_Softmax_cast = softmax(axis = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_softmax_Softmax_axis_0, x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_einsum_Einsum_cast);
            tensor&lt;int32, [4]&gt; transpose_2_perm_0 = const()[name = tensor&lt;string, []&gt;("transpose_2_perm_0"), val = tensor&lt;int32, [4]&gt;([0, 2, 1, 3])];
            tensor&lt;bool, []&gt; matmul_3_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("matmul_3_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; matmul_3_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("matmul_3_transpose_y_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [1, 4, 192, 64]&gt; transpose_21 = transpose(perm = transpose_2_perm_0, x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_value_add_cast);
            tensor&lt;fp16, [1, 4, 192, 64]&gt; matmul_3_cast = matmul(transpose_x = matmul_3_transpose_x_0, transpose_y = matmul_3_transpose_y_0, x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_softmax_Softmax_cast, y = transpose_21);
            tensor&lt;int32, [4]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_einsum_1_Einsum_perm_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_einsum_1_Einsum_perm_0"), val = tensor&lt;int32, [4]&gt;([0, 2, 1, 3])];
            tensor&lt;int32, [3]&gt; reshape_6_shape_0 = const()[name = tensor&lt;string, []&gt;("reshape_6_shape_0"), val = tensor&lt;int32, [3]&gt;([1, 192, 256])];
            tensor&lt;fp16, [1, 192, 4, 64]&gt; transpose_20 = transpose(perm = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_einsum_1_Einsum_perm_0, x = matmul_3_cast);
            tensor&lt;fp16, [1, 192, 256]&gt; reshape_6_cast = reshape(shape = reshape_6_shape_0, x = transpose_20);
            tensor&lt;fp16, [128, 256]&gt; transpose_6_cast_to_fp32_to_fp16 = const()[name = tensor&lt;string, []&gt;("transpose_6_cast_to_fp32_to_fp16"), val = tensor&lt;fp16, [128, 256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4572672)))];
            tensor&lt;fp16, [128]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_attention_output_add_bias_0_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_attention_output_add_bias_0_to_fp16"), val = tensor&lt;fp16, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4638272)))];
            tensor&lt;fp16, [1, 192, 128]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_attention_output_add_cast = linear(bias = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_attention_output_add_bias_0_to_fp16, weight = transpose_6_cast_to_fp32_to_fp16, x = reshape_6_cast);
            tensor&lt;fp16, [1, 192, 128]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_add_cast = add(x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_batchnorm_add_1_cast, y = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_attention_output_add_cast);
            tensor&lt;int32, [1]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_1_batchnorm_add_1_axes_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_1_batchnorm_add_1_axes_0"), val = tensor&lt;int32, [1]&gt;([2])];
            tensor&lt;fp16, [128]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_1_batchnorm_mul_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_1_batchnorm_mul_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4638592)))];
            tensor&lt;fp16, [128]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_1_batchnorm_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_1_batchnorm_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4638912)))];
            tensor&lt;fp16, []&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_1_batchnorm_add_y_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_1_batchnorm_add_y_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1.1p-20)];
            tensor&lt;fp16, [1, 192, 128]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_1_batchnorm_add_1_cast = layer_norm(axes = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_1_batchnorm_add_1_axes_0, beta = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_1_batchnorm_ReadVariableOp_to_fp16, epsilon = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_1_batchnorm_add_y_to_fp16, gamma = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_1_batchnorm_mul_ReadVariableOp_to_fp16, x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_add_cast);
            tensor&lt;int32, [3]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Tensordot_concat_1 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Tensordot_concat_1"), val = tensor&lt;int32, [3]&gt;([1, 192, 256])];
            tensor&lt;int32, [2]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Tensordot_stack = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Tensordot_stack"), val = tensor&lt;int32, [2]&gt;([192, 128])];
            tensor&lt;fp16, [192, 128]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Tensordot_Reshape_cast = reshape(shape = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Tensordot_stack, x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_1_batchnorm_add_1_cast);
            tensor&lt;bool, []&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Tensordot_MatMul_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Tensordot_MatMul_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Tensordot_MatMul_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Tensordot_MatMul_transpose_y_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [128, 256]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Tensordot_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Tensordot_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [128, 256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4639232)))];
            tensor&lt;fp16, [192, 256]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Tensordot_MatMul_cast = matmul(transpose_x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Tensordot_MatMul_transpose_x_0, transpose_y = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Tensordot_MatMul_transpose_y_0, x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Tensordot_Reshape_cast, y = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Tensordot_ReadVariableOp_to_fp16);
            tensor&lt;fp16, [1, 192, 256]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Tensordot_cast = reshape(shape = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Tensordot_concat_1, x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Tensordot_MatMul_cast);
            tensor&lt;fp16, [256]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_BiasAdd_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_BiasAdd_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4704832)))];
            tensor&lt;fp16, [1, 192, 256]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_BiasAdd_cast = add(x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Tensordot_cast, y = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_BiasAdd_ReadVariableOp_to_fp16);
            tensor&lt;string, []&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Gelu_mul_1_mode_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Gelu_mul_1_mode_0"), val = tensor&lt;string, []&gt;("EXACT")];
            tensor&lt;fp16, [1, 192, 256]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Gelu_mul_1_cast = gelu(mode = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Gelu_mul_1_mode_0, x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_BiasAdd_cast);
            tensor&lt;int32, [3]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_Tensordot_concat_1 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_Tensordot_concat_1"), val = tensor&lt;int32, [3]&gt;([1, 192, 128])];
            tensor&lt;int32, [2]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_Tensordot_stack = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_Tensordot_stack"), val = tensor&lt;int32, [2]&gt;([192, 256])];
            tensor&lt;fp16, [192, 256]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_Tensordot_Reshape_cast = reshape(shape = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_Tensordot_stack, x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Gelu_mul_1_cast);
            tensor&lt;bool, []&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_Tensordot_MatMul_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_Tensordot_MatMul_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_Tensordot_MatMul_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_Tensordot_MatMul_transpose_y_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [256, 128]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_Tensordot_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_Tensordot_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [256, 128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4705408)))];
            tensor&lt;fp16, [192, 128]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_Tensordot_MatMul_cast = matmul(transpose_x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_Tensordot_MatMul_transpose_x_0, transpose_y = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_Tensordot_MatMul_transpose_y_0, x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_Tensordot_Reshape_cast, y = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_Tensordot_ReadVariableOp_to_fp16);
            tensor&lt;fp16, [1, 192, 128]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_Tensordot_cast = reshape(shape = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_Tensordot_concat_1, x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_Tensordot_MatMul_cast);
            tensor&lt;fp16, [128]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_BiasAdd_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_BiasAdd_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4771008)))];
            tensor&lt;fp16, [1, 192, 128]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_BiasAdd_cast = add(x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_Tensordot_cast, y = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_BiasAdd_ReadVariableOp_to_fp16);
            tensor&lt;fp16, [1, 192, 128]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_add_1_cast = add(x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_1_batchnorm_add_1_cast, y = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_BiasAdd_cast);
            tensor&lt;int32, [1]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_2_batchnorm_add_1_axes_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_2_batchnorm_add_1_axes_0"), val = tensor&lt;int32, [1]&gt;([2])];
            tensor&lt;fp16, [128]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_2_batchnorm_mul_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_2_batchnorm_mul_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4771328)))];
            tensor&lt;fp16, [128]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_2_batchnorm_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_2_batchnorm_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4771648)))];
            tensor&lt;fp16, []&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_2_batchnorm_add_y_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_2_batchnorm_add_y_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1.1p-20)];
            tensor&lt;fp16, [1, 192, 128]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_2_batchnorm_add_1_cast = layer_norm(axes = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_2_batchnorm_add_1_axes_0, beta = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_2_batchnorm_ReadVariableOp_to_fp16, epsilon = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_2_batchnorm_add_y_to_fp16, gamma = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_2_batchnorm_mul_ReadVariableOp_to_fp16, x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_add_1_cast);
            tensor&lt;int32, [2]&gt; reshape_8_shape_0 = const()[name = tensor&lt;string, []&gt;("reshape_8_shape_0"), val = tensor&lt;int32, [2]&gt;([192, 128])];
            tensor&lt;fp16, [192, 128]&gt; reshape_8_cast = reshape(shape = reshape_8_shape_0, x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_2_batchnorm_add_1_cast);
            tensor&lt;bool, []&gt; matmul_4_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("matmul_4_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; matmul_4_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("matmul_4_transpose_y_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [128, 256]&gt; reshape_9_to_fp16 = const()[name = tensor&lt;string, []&gt;("reshape_9_to_fp16"), val = tensor&lt;fp16, [128, 256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4771968)))];
            tensor&lt;fp16, [192, 256]&gt; matmul_4_cast = matmul(transpose_x = matmul_4_transpose_x_0, transpose_y = matmul_4_transpose_y_0, x = reshape_8_cast, y = reshape_9_to_fp16);
            tensor&lt;int32, [4]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_key_einsum_Einsum_shape_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_key_einsum_Einsum_shape_0"), val = tensor&lt;int32, [4]&gt;([1, 192, 4, 64])];
            tensor&lt;fp16, [1, 192, 4, 64]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_key_einsum_Einsum_cast = reshape(shape = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_key_einsum_Einsum_shape_0, x = matmul_4_cast);
            tensor&lt;bool, []&gt; matmul_5_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("matmul_5_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; matmul_5_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("matmul_5_transpose_y_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [128, 256]&gt; reshape_11_to_fp16 = const()[name = tensor&lt;string, []&gt;("reshape_11_to_fp16"), val = tensor&lt;fp16, [128, 256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4837568)))];
            tensor&lt;fp16, [192, 256]&gt; matmul_5_cast = matmul(transpose_x = matmul_5_transpose_x_0, transpose_y = matmul_5_transpose_y_0, x = reshape_8_cast, y = reshape_11_to_fp16);
            tensor&lt;int32, [4]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_query_einsum_Einsum_shape_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_query_einsum_Einsum_shape_0"), val = tensor&lt;int32, [4]&gt;([1, 192, 4, 64])];
            tensor&lt;fp16, [1, 192, 4, 64]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_query_einsum_Einsum_cast = reshape(shape = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_query_einsum_Einsum_shape_0, x = matmul_5_cast);
            tensor&lt;bool, []&gt; matmul_6_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("matmul_6_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; matmul_6_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("matmul_6_transpose_y_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [128, 256]&gt; reshape_13_to_fp16 = const()[name = tensor&lt;string, []&gt;("reshape_13_to_fp16"), val = tensor&lt;fp16, [128, 256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4903168)))];
            tensor&lt;fp16, [192, 256]&gt; matmul_6_cast = matmul(transpose_x = matmul_6_transpose_x_0, transpose_y = matmul_6_transpose_y_0, x = reshape_8_cast, y = reshape_13_to_fp16);
            tensor&lt;int32, [4]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_value_einsum_Einsum_shape_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_value_einsum_Einsum_shape_0"), val = tensor&lt;int32, [4]&gt;([1, 192, 4, 64])];
            tensor&lt;fp16, [1, 192, 4, 64]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_value_einsum_Einsum_cast = reshape(shape = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_value_einsum_Einsum_shape_0, x = matmul_6_cast);
            tensor&lt;fp16, [4, 64]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_key_add_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_key_add_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [4, 64]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4968768)))];
            tensor&lt;fp16, [1, 192, 4, 64]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_key_add_cast = add(x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_key_einsum_Einsum_cast, y = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_key_add_ReadVariableOp_to_fp16);
            tensor&lt;fp16, [4, 64]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_query_add_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_query_add_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [4, 64]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4969344)))];
            tensor&lt;fp16, [1, 192, 4, 64]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_query_add_cast = add(x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_query_einsum_Einsum_cast, y = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_query_add_ReadVariableOp_to_fp16);
            tensor&lt;fp16, [4, 64]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_value_add_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_value_add_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [4, 64]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4969920)))];
            tensor&lt;fp16, [1, 192, 4, 64]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_value_add_cast = add(x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_value_einsum_Einsum_cast, y = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_value_add_ReadVariableOp_to_fp16);
            tensor&lt;fp16, []&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_Mul_y_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_Mul_y_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1p-3)];
            tensor&lt;fp16, [1, 192, 4, 64]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_Mul_cast = mul(x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_query_add_cast, y = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_Mul_y_to_fp16);
            tensor&lt;bool, []&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_einsum_Einsum_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_einsum_Einsum_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_einsum_Einsum_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_einsum_Einsum_transpose_y_0"), val = tensor&lt;bool, []&gt;(true)];
            tensor&lt;int32, [4]&gt; transpose_14_perm_0 = const()[name = tensor&lt;string, []&gt;("transpose_14_perm_0"), val = tensor&lt;int32, [4]&gt;([0, 2, 1, 3])];
            tensor&lt;int32, [4]&gt; transpose_15_perm_0 = const()[name = tensor&lt;string, []&gt;("transpose_15_perm_0"), val = tensor&lt;int32, [4]&gt;([0, 2, 1, 3])];
            tensor&lt;fp16, [1, 4, 192, 64]&gt; transpose_18 = transpose(perm = transpose_15_perm_0, x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_key_add_cast);
            tensor&lt;fp16, [1, 4, 192, 64]&gt; transpose_19 = transpose(perm = transpose_14_perm_0, x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_Mul_cast);
            tensor&lt;fp16, [1, 4, 192, 192]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_einsum_Einsum_cast = matmul(transpose_x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_einsum_Einsum_transpose_x_0, transpose_y = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_einsum_Einsum_transpose_y_0, x = transpose_19, y = transpose_18);
            tensor&lt;int32, []&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_softmax_Softmax_axis_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_softmax_Softmax_axis_0"), val = tensor&lt;int32, []&gt;(-1)];
            tensor&lt;fp16, [1, 4, 192, 192]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_softmax_Softmax_cast = softmax(axis = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_softmax_Softmax_axis_0, x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_einsum_Einsum_cast);
            tensor&lt;int32, [4]&gt; transpose_5_perm_0 = const()[name = tensor&lt;string, []&gt;("transpose_5_perm_0"), val = tensor&lt;int32, [4]&gt;([0, 2, 1, 3])];
            tensor&lt;bool, []&gt; matmul_7_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("matmul_7_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; matmul_7_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("matmul_7_transpose_y_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [1, 4, 192, 64]&gt; transpose_17 = transpose(perm = transpose_5_perm_0, x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_value_add_cast);
            tensor&lt;fp16, [1, 4, 192, 64]&gt; matmul_7_cast = matmul(transpose_x = matmul_7_transpose_x_0, transpose_y = matmul_7_transpose_y_0, x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_softmax_Softmax_cast, y = transpose_17);
            tensor&lt;int32, [4]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_einsum_1_Einsum_perm_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_einsum_1_Einsum_perm_0"), val = tensor&lt;int32, [4]&gt;([0, 2, 1, 3])];
            tensor&lt;int32, [3]&gt; reshape_14_shape_0 = const()[name = tensor&lt;string, []&gt;("reshape_14_shape_0"), val = tensor&lt;int32, [3]&gt;([1, 192, 256])];
            tensor&lt;fp16, [1, 192, 4, 64]&gt; transpose_16 = transpose(perm = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_einsum_1_Einsum_perm_0, x = matmul_7_cast);
            tensor&lt;fp16, [1, 192, 256]&gt; reshape_14_cast = reshape(shape = reshape_14_shape_0, x = transpose_16);
            tensor&lt;fp16, [128, 256]&gt; transpose_7_cast_to_fp32_to_fp16 = const()[name = tensor&lt;string, []&gt;("transpose_7_cast_to_fp32_to_fp16"), val = tensor&lt;fp16, [128, 256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4970496)))];
            tensor&lt;fp16, [128]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_attention_output_add_bias_0_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_attention_output_add_bias_0_to_fp16"), val = tensor&lt;fp16, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(5036096)))];
            tensor&lt;fp16, [1, 192, 128]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_attention_output_add_cast = linear(bias = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_attention_output_add_bias_0_to_fp16, weight = transpose_7_cast_to_fp32_to_fp16, x = reshape_14_cast);
            tensor&lt;fp16, [1, 192, 128]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_add_cast = add(x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_2_batchnorm_add_1_cast, y = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_attention_output_add_cast);
            tensor&lt;int32, [1]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_3_batchnorm_add_1_axes_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_3_batchnorm_add_1_axes_0"), val = tensor&lt;int32, [1]&gt;([2])];
            tensor&lt;fp16, [128]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_3_batchnorm_mul_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_3_batchnorm_mul_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(5036416)))];
            tensor&lt;fp16, [128]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_3_batchnorm_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_3_batchnorm_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(5036736)))];
            tensor&lt;fp16, []&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_3_batchnorm_add_y_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_3_batchnorm_add_y_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1.1p-20)];
            tensor&lt;fp16, [1, 192, 128]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_3_batchnorm_add_1_cast = layer_norm(axes = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_3_batchnorm_add_1_axes_0, beta = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_3_batchnorm_ReadVariableOp_to_fp16, epsilon = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_3_batchnorm_add_y_to_fp16, gamma = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_3_batchnorm_mul_ReadVariableOp_to_fp16, x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_add_cast);
            tensor&lt;int32, [3]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Tensordot_concat_1 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Tensordot_concat_1"), val = tensor&lt;int32, [3]&gt;([1, 192, 256])];
            tensor&lt;int32, [2]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Tensordot_stack = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Tensordot_stack"), val = tensor&lt;int32, [2]&gt;([192, 128])];
            tensor&lt;fp16, [192, 128]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Tensordot_Reshape_cast = reshape(shape = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Tensordot_stack, x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_3_batchnorm_add_1_cast);
            tensor&lt;bool, []&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Tensordot_MatMul_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Tensordot_MatMul_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Tensordot_MatMul_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Tensordot_MatMul_transpose_y_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [128, 256]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Tensordot_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Tensordot_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [128, 256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(5037056)))];
            tensor&lt;fp16, [192, 256]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Tensordot_MatMul_cast = matmul(transpose_x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Tensordot_MatMul_transpose_x_0, transpose_y = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Tensordot_MatMul_transpose_y_0, x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Tensordot_Reshape_cast, y = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Tensordot_ReadVariableOp_to_fp16);
            tensor&lt;fp16, [1, 192, 256]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Tensordot_cast = reshape(shape = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Tensordot_concat_1, x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Tensordot_MatMul_cast);
            tensor&lt;fp16, [256]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_BiasAdd_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_BiasAdd_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(5102656)))];
            tensor&lt;fp16, [1, 192, 256]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_BiasAdd_cast = add(x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Tensordot_cast, y = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_BiasAdd_ReadVariableOp_to_fp16);
            tensor&lt;string, []&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Gelu_mul_1_mode_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Gelu_mul_1_mode_0"), val = tensor&lt;string, []&gt;("EXACT")];
            tensor&lt;fp16, [1, 192, 256]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Gelu_mul_1_cast = gelu(mode = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Gelu_mul_1_mode_0, x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_BiasAdd_cast);
            tensor&lt;int32, [3]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_Tensordot_concat_1 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_Tensordot_concat_1"), val = tensor&lt;int32, [3]&gt;([1, 192, 128])];
            tensor&lt;int32, [2]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_Tensordot_stack = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_Tensordot_stack"), val = tensor&lt;int32, [2]&gt;([192, 256])];
            tensor&lt;fp16, [192, 256]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_Tensordot_Reshape_cast = reshape(shape = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_Tensordot_stack, x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Gelu_mul_1_cast);
            tensor&lt;bool, []&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_Tensordot_MatMul_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_Tensordot_MatMul_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_Tensordot_MatMul_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_Tensordot_MatMul_transpose_y_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [256, 128]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_Tensordot_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_Tensordot_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [256, 128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(5103232)))];
            tensor&lt;fp16, [192, 128]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_Tensordot_MatMul_cast = matmul(transpose_x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_Tensordot_MatMul_transpose_x_0, transpose_y = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_Tensordot_MatMul_transpose_y_0, x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_Tensordot_Reshape_cast, y = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_Tensordot_ReadVariableOp_to_fp16);
            tensor&lt;fp16, [1, 192, 128]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_Tensordot_cast = reshape(shape = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_Tensordot_concat_1, x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_Tensordot_MatMul_cast);
            tensor&lt;fp16, [128]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_BiasAdd_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_BiasAdd_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(5168832)))];
            tensor&lt;fp16, [1, 192, 128]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_BiasAdd_cast = add(x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_Tensordot_cast, y = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_BiasAdd_ReadVariableOp_to_fp16);
            tensor&lt;fp16, [1, 192, 128]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_add_1_cast = add(x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_3_batchnorm_add_1_cast, y = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_BiasAdd_cast);
            tensor&lt;int32, [1]&gt; model_1_tf_math_reduce_mean_1_Mean_axes_0 = const()[name = tensor&lt;string, []&gt;("model_1_tf_math_reduce_mean_1_Mean_axes_0"), val = tensor&lt;int32, [1]&gt;([-2])];
            tensor&lt;bool, []&gt; model_1_tf_math_reduce_mean_1_Mean_keep_dims_0 = const()[name = tensor&lt;string, []&gt;("model_1_tf_math_reduce_mean_1_Mean_keep_dims_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [1, 128]&gt; model_1_tf_math_reduce_mean_1_Mean_cast = reduce_mean(axes = model_1_tf_math_reduce_mean_1_Mean_axes_0, keep_dims = model_1_tf_math_reduce_mean_1_Mean_keep_dims_0, x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_add_1_cast);
            tensor&lt;string, []&gt; model_1_tf_math_reduce_mean_1_Mean_cast_to_fp32_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_1_tf_math_reduce_mean_1_Mean_cast_to_fp32_dtype_0"), val = tensor&lt;string, []&gt;("fp32")];
            tensor&lt;fp32, [1, 128]&gt; Identity = cast(dtype = model_1_tf_math_reduce_mean_1_Mean_cast_to_fp32_dtype_0, x = model_1_tf_math_reduce_mean_1_Mean_cast);
        } -&gt; (Identity);
}</code></pre>
                    </div>
                </div>
            
                <div class="file-section" id="file-16">
                    <div class="file-header">
                        <h2>ðŸ“„ model.mil 23</h2>
                        <div class="file-meta">Size: 11.35 KB</div>
                    </div>
                    <div class="file-content">
                        <pre><code>program(1.0)
[buildInfo = dict&lt;tensor&lt;string, []&gt;, tensor&lt;string, []&gt;&gt;({{"coremlc-component-MIL", "3404.16.1"}, {"coremlc-version", "3404.24.1"}})]
{
    func main&lt;ios16&gt;(tensor&lt;int32, [1, 192, 1]&gt; input_4, tensor&lt;int32, [1, 192, 24]&gt; input_8) {
            tensor&lt;int32, []&gt; model_1_tf_math_count_nonzero_count_nonzero_zeros = const()[name = tensor&lt;string, []&gt;("model_1_tf_math_count_nonzero_count_nonzero_zeros"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, []&gt; model_1_embedding_embedding_lookup_axis_0 = const()[name = tensor&lt;string, []&gt;("model_1_embedding_embedding_lookup_axis_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, []&gt; model_1_embedding_embedding_lookup_batch_dims_0 = const()[name = tensor&lt;string, []&gt;("model_1_embedding_embedding_lookup_batch_dims_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;fp16, [2364, 64]&gt; model_1_embedding_embedding_lookup_757_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_embedding_embedding_lookup_757_to_fp16"), val = tensor&lt;fp16, [2364, 64]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(64)))];
            tensor&lt;fp16, [1, 192, 1, 64]&gt; model_1_embedding_embedding_lookup_cast = gather(axis = model_1_embedding_embedding_lookup_axis_0, batch_dims = model_1_embedding_embedding_lookup_batch_dims_0, indices = input_4, x = model_1_embedding_embedding_lookup_757_to_fp16);
            tensor&lt;int32, []&gt; model_1_embedding_1_embedding_lookup_axis_0 = const()[name = tensor&lt;string, []&gt;("model_1_embedding_1_embedding_lookup_axis_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, []&gt; model_1_embedding_1_embedding_lookup_batch_dims_0 = const()[name = tensor&lt;string, []&gt;("model_1_embedding_1_embedding_lookup_batch_dims_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;fp16, [11581, 64]&gt; model_1_embedding_1_embedding_lookup_748_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_embedding_1_embedding_lookup_748_to_fp16"), val = tensor&lt;fp16, [11581, 64]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(302720)))];
            tensor&lt;fp16, [1, 192, 24, 64]&gt; model_1_embedding_1_embedding_lookup_cast = gather(axis = model_1_embedding_1_embedding_lookup_axis_0, batch_dims = model_1_embedding_1_embedding_lookup_batch_dims_0, indices = input_8, x = model_1_embedding_1_embedding_lookup_748_to_fp16);
            tensor&lt;bool, [1, 192, 1]&gt; model_1_tf_math_count_nonzero_count_nonzero_NotEqual = not_equal(x = input_4, y = model_1_tf_math_count_nonzero_count_nonzero_zeros);
            tensor&lt;string, []&gt; model_1_tf_math_count_nonzero_count_nonzero_Cast_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_1_tf_math_count_nonzero_count_nonzero_Cast_dtype_0"), val = tensor&lt;string, []&gt;("int32")];
            tensor&lt;int32, [1]&gt; model_1_tf_math_count_nonzero_count_nonzero_Sum_axes_0 = const()[name = tensor&lt;string, []&gt;("model_1_tf_math_count_nonzero_count_nonzero_Sum_axes_0"), val = tensor&lt;int32, [1]&gt;([1])];
            tensor&lt;bool, []&gt; model_1_tf_math_count_nonzero_count_nonzero_Sum_keep_dims_0 = const()[name = tensor&lt;string, []&gt;("model_1_tf_math_count_nonzero_count_nonzero_Sum_keep_dims_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;int32, [1, 192, 1]&gt; cast_19 = cast(dtype = model_1_tf_math_count_nonzero_count_nonzero_Cast_dtype_0, x = model_1_tf_math_count_nonzero_count_nonzero_NotEqual);
            tensor&lt;int32, [1, 1]&gt; model_1_tf_math_count_nonzero_count_nonzero_Sum = reduce_sum(axes = model_1_tf_math_count_nonzero_count_nonzero_Sum_axes_0, keep_dims = model_1_tf_math_count_nonzero_count_nonzero_Sum_keep_dims_0, x = cast_19);
            tensor&lt;int32, [1]&gt; model_1_tf_math_reduce_mean_Mean_axes_0 = const()[name = tensor&lt;string, []&gt;("model_1_tf_math_reduce_mean_Mean_axes_0"), val = tensor&lt;int32, [1]&gt;([1])];
            tensor&lt;bool, []&gt; model_1_tf_math_reduce_mean_Mean_keep_dims_0 = const()[name = tensor&lt;string, []&gt;("model_1_tf_math_reduce_mean_Mean_keep_dims_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [1, 1, 64]&gt; model_1_tf_math_reduce_mean_Mean_cast = reduce_mean(axes = model_1_tf_math_reduce_mean_Mean_axes_0, keep_dims = model_1_tf_math_reduce_mean_Mean_keep_dims_0, x = model_1_embedding_embedding_lookup_cast);
            tensor&lt;int32, [1]&gt; model_1_tf_math_reduce_mean_1_Mean_axes_0 = const()[name = tensor&lt;string, []&gt;("model_1_tf_math_reduce_mean_1_Mean_axes_0"), val = tensor&lt;int32, [1]&gt;([1])];
            tensor&lt;bool, []&gt; model_1_tf_math_reduce_mean_1_Mean_keep_dims_0 = const()[name = tensor&lt;string, []&gt;("model_1_tf_math_reduce_mean_1_Mean_keep_dims_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [1, 24, 64]&gt; model_1_tf_math_reduce_mean_1_Mean_cast = reduce_mean(axes = model_1_tf_math_reduce_mean_1_Mean_axes_0, keep_dims = model_1_tf_math_reduce_mean_1_Mean_keep_dims_0, x = model_1_embedding_1_embedding_lookup_cast);
            tensor&lt;fp16, [1, 1, 64]&gt; model_1_tf_math_l2_normalize_l2_normalize_Square_cast = mul(x = model_1_tf_math_reduce_mean_Mean_cast, y = model_1_tf_math_reduce_mean_Mean_cast);
            tensor&lt;int32, [1]&gt; model_1_tf_math_reduce_mean_2_Mean_axes_0 = const()[name = tensor&lt;string, []&gt;("model_1_tf_math_reduce_mean_2_Mean_axes_0"), val = tensor&lt;int32, [1]&gt;([1])];
            tensor&lt;bool, []&gt; model_1_tf_math_reduce_mean_2_Mean_keep_dims_0 = const()[name = tensor&lt;string, []&gt;("model_1_tf_math_reduce_mean_2_Mean_keep_dims_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [1, 64]&gt; model_1_tf_math_reduce_mean_2_Mean_cast = reduce_mean(axes = model_1_tf_math_reduce_mean_2_Mean_axes_0, keep_dims = model_1_tf_math_reduce_mean_2_Mean_keep_dims_0, x = model_1_tf_math_reduce_mean_1_Mean_cast);
            tensor&lt;string, []&gt; model_1_tf_cast_1_Cast_to_fp16_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_1_tf_cast_1_Cast_to_fp16_dtype_0"), val = tensor&lt;string, []&gt;("fp16")];
            tensor&lt;fp16, []&gt; _inversed_model_1_tf_math_truediv_truediv_y_0_to_fp16 = const()[name = tensor&lt;string, []&gt;("_inversed_model_1_tf_math_truediv_truediv_y_0_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1.554p-8)];
            tensor&lt;fp16, [1, 1]&gt; cast_13 = cast(dtype = model_1_tf_cast_1_Cast_to_fp16_dtype_0, x = model_1_tf_math_count_nonzero_count_nonzero_Sum);
            tensor&lt;fp16, [1, 1]&gt; _inversed_model_1_tf_math_truediv_truediv_cast = mul(x = cast_13, y = _inversed_model_1_tf_math_truediv_truediv_y_0_to_fp16);
            tensor&lt;int32, [1]&gt; model_1_tf_math_l2_normalize_l2_normalize_Sum_axes_0 = const()[name = tensor&lt;string, []&gt;("model_1_tf_math_l2_normalize_l2_normalize_Sum_axes_0"), val = tensor&lt;int32, [1]&gt;([-1])];
            tensor&lt;bool, []&gt; model_1_tf_math_l2_normalize_l2_normalize_Sum_keep_dims_0 = const()[name = tensor&lt;string, []&gt;("model_1_tf_math_l2_normalize_l2_normalize_Sum_keep_dims_0"), val = tensor&lt;bool, []&gt;(true)];
            tensor&lt;fp16, [1, 1, 1]&gt; model_1_tf_math_l2_normalize_l2_normalize_Sum_cast = reduce_sum(axes = model_1_tf_math_l2_normalize_l2_normalize_Sum_axes_0, keep_dims = model_1_tf_math_l2_normalize_l2_normalize_Sum_keep_dims_0, x = model_1_tf_math_l2_normalize_l2_normalize_Square_cast);
            tensor&lt;fp16, [1, 64]&gt; model_1_tf_math_l2_normalize_2_l2_normalize_Square_cast = mul(x = model_1_tf_math_reduce_mean_2_Mean_cast, y = model_1_tf_math_reduce_mean_2_Mean_cast);
            tensor&lt;fp16, [1, 1]&gt; model_1_tf_math_sqrt_Sqrt_cast = sqrt(x = _inversed_model_1_tf_math_truediv_truediv_cast);
            tensor&lt;string, []&gt; model_1_tf_math_sqrt_Sqrt_cast_to_fp32_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_1_tf_math_sqrt_Sqrt_cast_to_fp32_dtype_0"), val = tensor&lt;string, []&gt;("fp32")];
            tensor&lt;fp16, []&gt; model_1_tf_math_l2_normalize_l2_normalize_Maximum_y_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_tf_math_l2_normalize_l2_normalize_Maximum_y_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1p-24)];
            tensor&lt;fp16, [1, 1, 1]&gt; model_1_tf_math_l2_normalize_l2_normalize_Maximum_cast = maximum(x = model_1_tf_math_l2_normalize_l2_normalize_Sum_cast, y = model_1_tf_math_l2_normalize_l2_normalize_Maximum_y_to_fp16);
            tensor&lt;int32, [1]&gt; model_1_tf_math_l2_normalize_2_l2_normalize_Sum_axes_0 = const()[name = tensor&lt;string, []&gt;("model_1_tf_math_l2_normalize_2_l2_normalize_Sum_axes_0"), val = tensor&lt;int32, [1]&gt;([-1])];
            tensor&lt;bool, []&gt; model_1_tf_math_l2_normalize_2_l2_normalize_Sum_keep_dims_0 = const()[name = tensor&lt;string, []&gt;("model_1_tf_math_l2_normalize_2_l2_normalize_Sum_keep_dims_0"), val = tensor&lt;bool, []&gt;(true)];
            tensor&lt;fp16, [1, 1]&gt; model_1_tf_math_l2_normalize_2_l2_normalize_Sum_cast = reduce_sum(axes = model_1_tf_math_l2_normalize_2_l2_normalize_Sum_axes_0, keep_dims = model_1_tf_math_l2_normalize_2_l2_normalize_Sum_keep_dims_0, x = model_1_tf_math_l2_normalize_2_l2_normalize_Square_cast);
            tensor&lt;fp16, []&gt; model_1_tf_math_l2_normalize_l2_normalize_Rsqrt_epsilon_0_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_tf_math_l2_normalize_l2_normalize_Rsqrt_epsilon_0_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1p-24)];
            tensor&lt;fp16, [1, 1, 1]&gt; model_1_tf_math_l2_normalize_l2_normalize_Rsqrt_cast = rsqrt(epsilon = model_1_tf_math_l2_normalize_l2_normalize_Rsqrt_epsilon_0_to_fp16, x = model_1_tf_math_l2_normalize_l2_normalize_Maximum_cast);
            tensor&lt;fp16, []&gt; model_1_tf_math_l2_normalize_2_l2_normalize_Maximum_y_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_tf_math_l2_normalize_2_l2_normalize_Maximum_y_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1p-24)];
            tensor&lt;fp16, [1, 1]&gt; model_1_tf_math_l2_normalize_2_l2_normalize_Maximum_cast = maximum(x = model_1_tf_math_l2_normalize_2_l2_normalize_Sum_cast, y = model_1_tf_math_l2_normalize_2_l2_normalize_Maximum_y_to_fp16);
            tensor&lt;fp16, [1, 1, 64]&gt; model_1_tf_math_l2_normalize_l2_normalize_cast = mul(x = model_1_tf_math_reduce_mean_Mean_cast, y = model_1_tf_math_l2_normalize_l2_normalize_Rsqrt_cast);
            tensor&lt;string, []&gt; model_1_tf_math_l2_normalize_l2_normalize_cast_to_fp32_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_1_tf_math_l2_normalize_l2_normalize_cast_to_fp32_dtype_0"), val = tensor&lt;string, []&gt;("fp32")];
            tensor&lt;fp16, []&gt; model_1_tf_math_l2_normalize_2_l2_normalize_Rsqrt_epsilon_0_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_tf_math_l2_normalize_2_l2_normalize_Rsqrt_epsilon_0_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1p-24)];
            tensor&lt;fp16, [1, 1]&gt; model_1_tf_math_l2_normalize_2_l2_normalize_Rsqrt_cast = rsqrt(epsilon = model_1_tf_math_l2_normalize_2_l2_normalize_Rsqrt_epsilon_0_to_fp16, x = model_1_tf_math_l2_normalize_2_l2_normalize_Maximum_cast);
            tensor&lt;fp16, [1, 64]&gt; model_1_tf_math_l2_normalize_2_l2_normalize_cast = mul(x = model_1_tf_math_reduce_mean_2_Mean_cast, y = model_1_tf_math_l2_normalize_2_l2_normalize_Rsqrt_cast);
            tensor&lt;string, []&gt; model_1_tf_math_l2_normalize_2_l2_normalize_cast_to_fp32_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_1_tf_math_l2_normalize_2_l2_normalize_cast_to_fp32_dtype_0"), val = tensor&lt;string, []&gt;("fp32")];
            tensor&lt;fp32, [1, 64]&gt; Identity_1 = cast(dtype = model_1_tf_math_l2_normalize_2_l2_normalize_cast_to_fp32_dtype_0, x = model_1_tf_math_l2_normalize_2_l2_normalize_cast);
            tensor&lt;fp32, [1, 1, 64]&gt; Identity = cast(dtype = model_1_tf_math_l2_normalize_l2_normalize_cast_to_fp32_dtype_0, x = model_1_tf_math_l2_normalize_l2_normalize_cast);
            tensor&lt;fp32, [1, 1]&gt; Identity_2 = cast(dtype = model_1_tf_math_sqrt_Sqrt_cast_to_fp32_dtype_0, x = model_1_tf_math_sqrt_Sqrt_cast);
        } -&gt; (Identity, Identity_1, Identity_2);
}</code></pre>
                    </div>
                </div>
            
                <div class="file-section" id="file-17">
                    <div class="file-header">
                        <h2>ðŸ“„ model.mil 15</h2>
                        <div class="file-meta">Size: 10.2 KB</div>
                    </div>
                    <div class="file-content">
                        <pre><code>program(1.0)
[buildInfo = dict&lt;tensor&lt;string, []&gt;, tensor&lt;string, []&gt;&gt;({{"coremlc-component-MIL", "3404.16.1"}, {"coremlc-version", "3404.24.1"}})]
{
    func main&lt;ios16&gt;(tensor&lt;int32, [1, 384, 1]&gt; input_1, tensor&lt;int32, [1, 384, 24]&gt; input_2, tensor&lt;int32, [1, 384, 1]&gt; input_3) {
            tensor&lt;int32, []&gt; model_extend_layer_concat_axis = const()[name = tensor&lt;string, []&gt;("model_extend_layer_concat_axis"), val = tensor&lt;int32, []&gt;(1)];
            tensor&lt;int32, []&gt; model_extend_layer_concat_1_axis = const()[name = tensor&lt;string, []&gt;("model_extend_layer_concat_1_axis"), val = tensor&lt;int32, []&gt;(1)];
            tensor&lt;int32, []&gt; model_tf_math_equal_Equal_y = const()[name = tensor&lt;string, []&gt;("model_tf_math_equal_Equal_y"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, []&gt; model_tf_math_equal_1_Equal_y = const()[name = tensor&lt;string, []&gt;("model_tf_math_equal_1_Equal_y"), val = tensor&lt;int32, []&gt;(2)];
            tensor&lt;int32, [192]&gt; model_178 = const()[name = tensor&lt;string, []&gt;("model_178"), val = tensor&lt;int32, [192]&gt;([384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384])];
            tensor&lt;int32, []&gt; model_tf_concat_concat_axis = const()[name = tensor&lt;string, []&gt;("model_tf_concat_concat_axis"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, []&gt; model_extend_layer_concat_2_axis = const()[name = tensor&lt;string, []&gt;("model_extend_layer_concat_2_axis"), val = tensor&lt;int32, []&gt;(1)];
            tensor&lt;int32, [1, 1, 1]&gt; model_extend_layer_zeros = const()[name = tensor&lt;string, []&gt;("model_extend_layer_zeros"), val = tensor&lt;int32, [1, 1, 1]&gt;([[[0]]])];
            tensor&lt;int32, [1, 1, 24]&gt; model_extend_layer_zeros_2 = const()[name = tensor&lt;string, []&gt;("model_extend_layer_zeros_2"), val = tensor&lt;int32, [1, 1, 24]&gt;([[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]])];
            tensor&lt;int32, [1, 1, 1]&gt; model_extend_layer_zeros_1 = const()[name = tensor&lt;string, []&gt;("model_extend_layer_zeros_1"), val = tensor&lt;int32, [1, 1, 1]&gt;([[[0]]])];
            tensor&lt;bool, []&gt; model_extend_layer_concat_interleave_0 = const()[name = tensor&lt;string, []&gt;("model_extend_layer_concat_interleave_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;int32, [1, 385, 1]&gt; model_extend_layer_concat = concat(axis = model_extend_layer_concat_axis, interleave = model_extend_layer_concat_interleave_0, values = (input_1, model_extend_layer_zeros));
            tensor&lt;bool, []&gt; model_extend_layer_concat_2_interleave_0 = const()[name = tensor&lt;string, []&gt;("model_extend_layer_concat_2_interleave_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;int32, [1, 385, 24]&gt; model_extend_layer_concat_2 = concat(axis = model_extend_layer_concat_2_axis, interleave = model_extend_layer_concat_2_interleave_0, values = (input_2, model_extend_layer_zeros_2));
            tensor&lt;bool, []&gt; model_extend_layer_concat_1_interleave_0 = const()[name = tensor&lt;string, []&gt;("model_extend_layer_concat_1_interleave_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;int32, [1, 385, 1]&gt; model_extend_layer_concat_1 = concat(axis = model_extend_layer_concat_1_axis, interleave = model_extend_layer_concat_1_interleave_0, values = (input_3, model_extend_layer_zeros_1));
            tensor&lt;int32, [3]&gt; model_tf___operators___getitem_strided_slice_begin_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_strided_slice_begin_0"), val = tensor&lt;int32, [3]&gt;([0, 0, 0])];
            tensor&lt;int32, [3]&gt; model_tf___operators___getitem_strided_slice_end_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_strided_slice_end_0"), val = tensor&lt;int32, [3]&gt;([1, 0, 1])];
            tensor&lt;int32, [3]&gt; model_tf___operators___getitem_strided_slice_stride_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_strided_slice_stride_0"), val = tensor&lt;int32, [3]&gt;([1, 1, 1])];
            tensor&lt;bool, [3]&gt; model_tf___operators___getitem_strided_slice_begin_mask_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_strided_slice_begin_mask_0"), val = tensor&lt;bool, [3]&gt;([false, true, false])];
            tensor&lt;bool, [3]&gt; model_tf___operators___getitem_strided_slice_end_mask_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_strided_slice_end_mask_0"), val = tensor&lt;bool, [3]&gt;([false, true, false])];
            tensor&lt;bool, [3]&gt; model_tf___operators___getitem_strided_slice_squeeze_mask_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_strided_slice_squeeze_mask_0"), val = tensor&lt;bool, [3]&gt;([true, false, true])];
            tensor&lt;int32, [385]&gt; model_tf___operators___getitem_strided_slice = slice_by_index(begin = model_tf___operators___getitem_strided_slice_begin_0, begin_mask = model_tf___operators___getitem_strided_slice_begin_mask_0, end = model_tf___operators___getitem_strided_slice_end_0, end_mask = model_tf___operators___getitem_strided_slice_end_mask_0, squeeze_mask = model_tf___operators___getitem_strided_slice_squeeze_mask_0, stride = model_tf___operators___getitem_strided_slice_stride_0, x = model_extend_layer_concat_1);
            tensor&lt;bool, [385]&gt; model_tf_math_equal_Equal = equal(x = model_tf___operators___getitem_strided_slice, y = model_tf_math_equal_Equal_y);
            tensor&lt;bool, [385]&gt; model_tf_math_equal_1_Equal = equal(x = model_tf___operators___getitem_strided_slice, y = model_tf_math_equal_1_Equal_y);
            tensor&lt;bool, [385]&gt; model_tf_math_logical_or_LogicalOr = logical_or(x = model_tf_math_equal_Equal, y = model_tf_math_equal_1_Equal);
            tensor&lt;int32, [?, 1]&gt; model_tf_where_Where = non_zero(x = model_tf_math_logical_or_LogicalOr);
            tensor&lt;int32, [1]&gt; model_tf_compat_v1_squeeze_Squeeze_axes_0 = const()[name = tensor&lt;string, []&gt;("model_tf_compat_v1_squeeze_Squeeze_axes_0"), val = tensor&lt;int32, [1]&gt;([-1])];
            tensor&lt;int32, [?]&gt; model_tf_compat_v1_squeeze_Squeeze = squeeze(axes = model_tf_compat_v1_squeeze_Squeeze_axes_0, x = model_tf_where_Where);
            tensor&lt;bool, []&gt; model_tf_concat_concat_interleave_0 = const()[name = tensor&lt;string, []&gt;("model_tf_concat_concat_interleave_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;int32, [?]&gt; model_tf_concat_concat = concat(axis = model_tf_concat_concat_axis, interleave = model_tf_concat_concat_interleave_0, values = (model_tf_compat_v1_squeeze_Squeeze, model_178));
            tensor&lt;int32, [1]&gt; model_tf___operators___getitem_2_strided_slice_begin_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_2_strided_slice_begin_0"), val = tensor&lt;int32, [1]&gt;([0])];
            tensor&lt;int32, [1]&gt; model_tf___operators___getitem_2_strided_slice_end_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_2_strided_slice_end_0"), val = tensor&lt;int32, [1]&gt;([192])];
            tensor&lt;int32, [1]&gt; model_tf___operators___getitem_2_strided_slice_stride_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_2_strided_slice_stride_0"), val = tensor&lt;int32, [1]&gt;([1])];
            tensor&lt;bool, [1]&gt; model_tf___operators___getitem_2_strided_slice_begin_mask_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_2_strided_slice_begin_mask_0"), val = tensor&lt;bool, [1]&gt;([true])];
            tensor&lt;bool, [1]&gt; model_tf___operators___getitem_2_strided_slice_end_mask_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_2_strided_slice_end_mask_0"), val = tensor&lt;bool, [1]&gt;([false])];
            tensor&lt;bool, [1]&gt; model_tf___operators___getitem_2_strided_slice_squeeze_mask_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_2_strided_slice_squeeze_mask_0"), val = tensor&lt;bool, [1]&gt;([false])];
            tensor&lt;int32, [192]&gt; model_tf___operators___getitem_2_strided_slice = slice_by_index(begin = model_tf___operators___getitem_2_strided_slice_begin_0, begin_mask = model_tf___operators___getitem_2_strided_slice_begin_mask_0, end = model_tf___operators___getitem_2_strided_slice_end_0, end_mask = model_tf___operators___getitem_2_strided_slice_end_mask_0, squeeze_mask = model_tf___operators___getitem_2_strided_slice_squeeze_mask_0, stride = model_tf___operators___getitem_2_strided_slice_stride_0, x = model_tf_concat_concat);
            tensor&lt;int32, []&gt; model_tf_compat_v1_gather_GatherV2_axis_0 = const()[name = tensor&lt;string, []&gt;("model_tf_compat_v1_gather_GatherV2_axis_0"), val = tensor&lt;int32, []&gt;(1)];
            tensor&lt;int32, []&gt; model_tf_compat_v1_gather_GatherV2_batch_dims_0 = const()[name = tensor&lt;string, []&gt;("model_tf_compat_v1_gather_GatherV2_batch_dims_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, [1, 192, 1]&gt; Identity = gather(axis = model_tf_compat_v1_gather_GatherV2_axis_0, batch_dims = model_tf_compat_v1_gather_GatherV2_batch_dims_0, indices = model_tf___operators___getitem_2_strided_slice, x = model_extend_layer_concat);
            tensor&lt;int32, []&gt; model_tf_compat_v1_gather_1_GatherV2_axis_0 = const()[name = tensor&lt;string, []&gt;("model_tf_compat_v1_gather_1_GatherV2_axis_0"), val = tensor&lt;int32, []&gt;(1)];
            tensor&lt;int32, []&gt; model_tf_compat_v1_gather_1_GatherV2_batch_dims_0 = const()[name = tensor&lt;string, []&gt;("model_tf_compat_v1_gather_1_GatherV2_batch_dims_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, [1, 192, 24]&gt; Identity_1 = gather(axis = model_tf_compat_v1_gather_1_GatherV2_axis_0, batch_dims = model_tf_compat_v1_gather_1_GatherV2_batch_dims_0, indices = model_tf___operators___getitem_2_strided_slice, x = model_extend_layer_concat_2);
        } -&gt; (Identity, Identity_1);
}</code></pre>
                    </div>
                </div>
            
                <div class="file-section" id="file-18">
                    <div class="file-header">
                        <h2>ðŸ“„ model.mil 11</h2>
                        <div class="file-meta">Size: 17.86 KB</div>
                    </div>
                    <div class="file-content">
                        <pre><code>program(1.0)
[buildInfo = dict&lt;tensor&lt;string, []&gt;, tensor&lt;string, []&gt;&gt;({{"coremlc-component-MIL", "3404.16.1"}, {"coremlc-version", "3404.24.1"}})]
{
    func main&lt;ios16&gt;(tensor&lt;fp16, [1, 384, 768]&gt; input_1, tensor&lt;int32, [1, 384, 1]&gt; input_2, tensor&lt;int32, [1, 384, 24]&gt; input_3, tensor&lt;int32, [1, 384, 1]&gt; input_4) {
            tensor&lt;string, []&gt; input_1_dtype_0 = const()[name = tensor&lt;string, []&gt;("input_1_dtype_0"), val = tensor&lt;string, []&gt;("fp32")];
            tensor&lt;int32, []&gt; model_extend_layer_concat_1_axis = const()[name = tensor&lt;string, []&gt;("model_extend_layer_concat_1_axis"), val = tensor&lt;int32, []&gt;(1)];
            tensor&lt;int32, []&gt; model_extend_layer_concat_axis = const()[name = tensor&lt;string, []&gt;("model_extend_layer_concat_axis"), val = tensor&lt;int32, []&gt;(1)];
            tensor&lt;int32, []&gt; model_tf_math_equal_Equal_y = const()[name = tensor&lt;string, []&gt;("model_tf_math_equal_Equal_y"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, []&gt; model_tf_math_equal_1_Equal_y = const()[name = tensor&lt;string, []&gt;("model_tf_math_equal_1_Equal_y"), val = tensor&lt;int32, []&gt;(2)];
            tensor&lt;int32, [192]&gt; model_304 = const()[name = tensor&lt;string, []&gt;("model_304"), val = tensor&lt;int32, [192]&gt;([384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384])];
            tensor&lt;int32, []&gt; model_tf_concat_concat_axis = const()[name = tensor&lt;string, []&gt;("model_tf_concat_concat_axis"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, []&gt; model_extend_layer_concat_3_axis = const()[name = tensor&lt;string, []&gt;("model_extend_layer_concat_3_axis"), val = tensor&lt;int32, []&gt;(1)];
            tensor&lt;int32, []&gt; model_extend_layer_concat_2_axis = const()[name = tensor&lt;string, []&gt;("model_extend_layer_concat_2_axis"), val = tensor&lt;int32, []&gt;(1)];
            tensor&lt;int32, []&gt; model_tf_math_equal_3_Equal_y = const()[name = tensor&lt;string, []&gt;("model_tf_math_equal_3_Equal_y"), val = tensor&lt;int32, []&gt;(3)];
            tensor&lt;int32, [192]&gt; model_308 = const()[name = tensor&lt;string, []&gt;("model_308"), val = tensor&lt;int32, [192]&gt;([384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384, 384])];
            tensor&lt;int32, []&gt; model_tf_concat_1_concat_axis = const()[name = tensor&lt;string, []&gt;("model_tf_concat_1_concat_axis"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, [1, 1, 1]&gt; model_extend_layer_zeros_3 = const()[name = tensor&lt;string, []&gt;("model_extend_layer_zeros_3"), val = tensor&lt;int32, [1, 1, 1]&gt;([[[0]]])];
            tensor&lt;int32, [1, 1, 24]&gt; model_extend_layer_zeros_2 = const()[name = tensor&lt;string, []&gt;("model_extend_layer_zeros_2"), val = tensor&lt;int32, [1, 1, 24]&gt;([[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]])];
            tensor&lt;int32, [1, 1, 1]&gt; model_extend_layer_zeros = const()[name = tensor&lt;string, []&gt;("model_extend_layer_zeros"), val = tensor&lt;int32, [1, 1, 1]&gt;([[[0]]])];
            tensor&lt;fp32, [1, 1, 768]&gt; model_extend_layer_zeros_1 = const()[name = tensor&lt;string, []&gt;("model_extend_layer_zeros_1"), val = tensor&lt;fp32, [1, 1, 768]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(64)))];
            tensor&lt;bool, []&gt; model_extend_layer_concat_3_interleave_0 = const()[name = tensor&lt;string, []&gt;("model_extend_layer_concat_3_interleave_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;int32, [1, 385, 1]&gt; model_extend_layer_concat_3 = concat(axis = model_extend_layer_concat_3_axis, interleave = model_extend_layer_concat_3_interleave_0, values = (input_2, model_extend_layer_zeros_3));
            tensor&lt;bool, []&gt; model_extend_layer_concat_2_interleave_0 = const()[name = tensor&lt;string, []&gt;("model_extend_layer_concat_2_interleave_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;int32, [1, 385, 24]&gt; model_extend_layer_concat_2 = concat(axis = model_extend_layer_concat_2_axis, interleave = model_extend_layer_concat_2_interleave_0, values = (input_3, model_extend_layer_zeros_2));
            tensor&lt;bool, []&gt; model_extend_layer_concat_interleave_0 = const()[name = tensor&lt;string, []&gt;("model_extend_layer_concat_interleave_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;int32, [1, 385, 1]&gt; model_extend_layer_concat = concat(axis = model_extend_layer_concat_axis, interleave = model_extend_layer_concat_interleave_0, values = (input_4, model_extend_layer_zeros));
            tensor&lt;bool, []&gt; model_extend_layer_concat_1_interleave_0 = const()[name = tensor&lt;string, []&gt;("model_extend_layer_concat_1_interleave_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp32, [1, 384, 768]&gt; cast_1 = cast(dtype = input_1_dtype_0, x = input_1);
            tensor&lt;fp32, [1, 385, 768]&gt; model_extend_layer_concat_1 = concat(axis = model_extend_layer_concat_1_axis, interleave = model_extend_layer_concat_1_interleave_0, values = (cast_1, model_extend_layer_zeros_1));
            tensor&lt;int32, [3]&gt; model_tf___operators___getitem_strided_slice_begin_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_strided_slice_begin_0"), val = tensor&lt;int32, [3]&gt;([0, 0, 0])];
            tensor&lt;int32, [3]&gt; model_tf___operators___getitem_strided_slice_end_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_strided_slice_end_0"), val = tensor&lt;int32, [3]&gt;([1, 0, 1])];
            tensor&lt;int32, [3]&gt; model_tf___operators___getitem_strided_slice_stride_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_strided_slice_stride_0"), val = tensor&lt;int32, [3]&gt;([1, 1, 1])];
            tensor&lt;bool, [3]&gt; model_tf___operators___getitem_strided_slice_begin_mask_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_strided_slice_begin_mask_0"), val = tensor&lt;bool, [3]&gt;([false, true, false])];
            tensor&lt;bool, [3]&gt; model_tf___operators___getitem_strided_slice_end_mask_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_strided_slice_end_mask_0"), val = tensor&lt;bool, [3]&gt;([false, true, false])];
            tensor&lt;bool, [3]&gt; model_tf___operators___getitem_strided_slice_squeeze_mask_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_strided_slice_squeeze_mask_0"), val = tensor&lt;bool, [3]&gt;([true, false, true])];
            tensor&lt;int32, [385]&gt; model_tf___operators___getitem_strided_slice = slice_by_index(begin = model_tf___operators___getitem_strided_slice_begin_0, begin_mask = model_tf___operators___getitem_strided_slice_begin_mask_0, end = model_tf___operators___getitem_strided_slice_end_0, end_mask = model_tf___operators___getitem_strided_slice_end_mask_0, squeeze_mask = model_tf___operators___getitem_strided_slice_squeeze_mask_0, stride = model_tf___operators___getitem_strided_slice_stride_0, x = model_extend_layer_concat);
            tensor&lt;bool, [385]&gt; model_tf_math_equal_Equal = equal(x = model_tf___operators___getitem_strided_slice, y = model_tf_math_equal_Equal_y);
            tensor&lt;bool, [385]&gt; model_tf_math_equal_1_Equal = equal(x = model_tf___operators___getitem_strided_slice, y = model_tf_math_equal_1_Equal_y);
            tensor&lt;bool, [385]&gt; model_tf_math_equal_3_Equal = equal(x = model_tf___operators___getitem_strided_slice, y = model_tf_math_equal_3_Equal_y);
            tensor&lt;bool, [385]&gt; model_tf_math_logical_or_LogicalOr = logical_or(x = model_tf_math_equal_Equal, y = model_tf_math_equal_1_Equal);
            tensor&lt;bool, [385]&gt; model_tf_math_logical_or_1_LogicalOr = logical_or(x = model_tf_math_equal_Equal, y = model_tf_math_equal_3_Equal);
            tensor&lt;int32, [?, 1]&gt; model_tf_where_Where = non_zero(x = model_tf_math_logical_or_LogicalOr);
            tensor&lt;int32, [?, 1]&gt; model_tf_where_1_Where = non_zero(x = model_tf_math_logical_or_1_LogicalOr);
            tensor&lt;int32, [1]&gt; model_tf_compat_v1_squeeze_Squeeze_axes_0 = const()[name = tensor&lt;string, []&gt;("model_tf_compat_v1_squeeze_Squeeze_axes_0"), val = tensor&lt;int32, [1]&gt;([-1])];
            tensor&lt;int32, [?]&gt; model_tf_compat_v1_squeeze_Squeeze = squeeze(axes = model_tf_compat_v1_squeeze_Squeeze_axes_0, x = model_tf_where_Where);
            tensor&lt;int32, [1]&gt; model_tf_compat_v1_squeeze_1_Squeeze_axes_0 = const()[name = tensor&lt;string, []&gt;("model_tf_compat_v1_squeeze_1_Squeeze_axes_0"), val = tensor&lt;int32, [1]&gt;([-1])];
            tensor&lt;int32, [?]&gt; model_tf_compat_v1_squeeze_1_Squeeze = squeeze(axes = model_tf_compat_v1_squeeze_1_Squeeze_axes_0, x = model_tf_where_1_Where);
            tensor&lt;bool, []&gt; model_tf_concat_concat_interleave_0 = const()[name = tensor&lt;string, []&gt;("model_tf_concat_concat_interleave_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;int32, [?]&gt; model_tf_concat_concat = concat(axis = model_tf_concat_concat_axis, interleave = model_tf_concat_concat_interleave_0, values = (model_tf_compat_v1_squeeze_Squeeze, model_304));
            tensor&lt;bool, []&gt; model_tf_concat_1_concat_interleave_0 = const()[name = tensor&lt;string, []&gt;("model_tf_concat_1_concat_interleave_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;int32, [?]&gt; model_tf_concat_1_concat = concat(axis = model_tf_concat_1_concat_axis, interleave = model_tf_concat_1_concat_interleave_0, values = (model_tf_compat_v1_squeeze_1_Squeeze, model_308));
            tensor&lt;int32, [1]&gt; model_tf___operators___getitem_2_strided_slice_begin_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_2_strided_slice_begin_0"), val = tensor&lt;int32, [1]&gt;([0])];
            tensor&lt;int32, [1]&gt; model_tf___operators___getitem_2_strided_slice_end_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_2_strided_slice_end_0"), val = tensor&lt;int32, [1]&gt;([192])];
            tensor&lt;int32, [1]&gt; model_tf___operators___getitem_2_strided_slice_stride_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_2_strided_slice_stride_0"), val = tensor&lt;int32, [1]&gt;([1])];
            tensor&lt;bool, [1]&gt; model_tf___operators___getitem_2_strided_slice_begin_mask_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_2_strided_slice_begin_mask_0"), val = tensor&lt;bool, [1]&gt;([true])];
            tensor&lt;bool, [1]&gt; model_tf___operators___getitem_2_strided_slice_end_mask_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_2_strided_slice_end_mask_0"), val = tensor&lt;bool, [1]&gt;([false])];
            tensor&lt;bool, [1]&gt; model_tf___operators___getitem_2_strided_slice_squeeze_mask_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_2_strided_slice_squeeze_mask_0"), val = tensor&lt;bool, [1]&gt;([false])];
            tensor&lt;int32, [192]&gt; model_tf___operators___getitem_2_strided_slice = slice_by_index(begin = model_tf___operators___getitem_2_strided_slice_begin_0, begin_mask = model_tf___operators___getitem_2_strided_slice_begin_mask_0, end = model_tf___operators___getitem_2_strided_slice_end_0, end_mask = model_tf___operators___getitem_2_strided_slice_end_mask_0, squeeze_mask = model_tf___operators___getitem_2_strided_slice_squeeze_mask_0, stride = model_tf___operators___getitem_2_strided_slice_stride_0, x = model_tf_concat_concat);
            tensor&lt;int32, [1]&gt; model_tf___operators___getitem_5_strided_slice_begin_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_5_strided_slice_begin_0"), val = tensor&lt;int32, [1]&gt;([0])];
            tensor&lt;int32, [1]&gt; model_tf___operators___getitem_5_strided_slice_end_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_5_strided_slice_end_0"), val = tensor&lt;int32, [1]&gt;([192])];
            tensor&lt;int32, [1]&gt; model_tf___operators___getitem_5_strided_slice_stride_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_5_strided_slice_stride_0"), val = tensor&lt;int32, [1]&gt;([1])];
            tensor&lt;bool, [1]&gt; model_tf___operators___getitem_5_strided_slice_begin_mask_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_5_strided_slice_begin_mask_0"), val = tensor&lt;bool, [1]&gt;([true])];
            tensor&lt;bool, [1]&gt; model_tf___operators___getitem_5_strided_slice_end_mask_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_5_strided_slice_end_mask_0"), val = tensor&lt;bool, [1]&gt;([false])];
            tensor&lt;bool, [1]&gt; model_tf___operators___getitem_5_strided_slice_squeeze_mask_0 = const()[name = tensor&lt;string, []&gt;("model_tf___operators___getitem_5_strided_slice_squeeze_mask_0"), val = tensor&lt;bool, [1]&gt;([false])];
            tensor&lt;int32, [192]&gt; model_tf___operators___getitem_5_strided_slice = slice_by_index(begin = model_tf___operators___getitem_5_strided_slice_begin_0, begin_mask = model_tf___operators___getitem_5_strided_slice_begin_mask_0, end = model_tf___operators___getitem_5_strided_slice_end_0, end_mask = model_tf___operators___getitem_5_strided_slice_end_mask_0, squeeze_mask = model_tf___operators___getitem_5_strided_slice_squeeze_mask_0, stride = model_tf___operators___getitem_5_strided_slice_stride_0, x = model_tf_concat_1_concat);
            tensor&lt;int32, []&gt; model_tf_compat_v1_gather_GatherV2_axis_0 = const()[name = tensor&lt;string, []&gt;("model_tf_compat_v1_gather_GatherV2_axis_0"), val = tensor&lt;int32, []&gt;(1)];
            tensor&lt;int32, []&gt; model_tf_compat_v1_gather_GatherV2_batch_dims_0 = const()[name = tensor&lt;string, []&gt;("model_tf_compat_v1_gather_GatherV2_batch_dims_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;fp32, [1, 192, 768]&gt; model_tf_compat_v1_gather_GatherV2 = gather(axis = model_tf_compat_v1_gather_GatherV2_axis_0, batch_dims = model_tf_compat_v1_gather_GatherV2_batch_dims_0, indices = model_tf___operators___getitem_2_strided_slice, x = model_extend_layer_concat_1);
            tensor&lt;int32, []&gt; model_tf_compat_v1_gather_1_GatherV2_axis_0 = const()[name = tensor&lt;string, []&gt;("model_tf_compat_v1_gather_1_GatherV2_axis_0"), val = tensor&lt;int32, []&gt;(1)];
            tensor&lt;int32, []&gt; model_tf_compat_v1_gather_1_GatherV2_batch_dims_0 = const()[name = tensor&lt;string, []&gt;("model_tf_compat_v1_gather_1_GatherV2_batch_dims_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, [1, 192, 1]&gt; Identity_1 = gather(axis = model_tf_compat_v1_gather_1_GatherV2_axis_0, batch_dims = model_tf_compat_v1_gather_1_GatherV2_batch_dims_0, indices = model_tf___operators___getitem_2_strided_slice, x = model_extend_layer_concat_3);
            tensor&lt;int32, []&gt; model_tf_compat_v1_gather_2_GatherV2_axis_0 = const()[name = tensor&lt;string, []&gt;("model_tf_compat_v1_gather_2_GatherV2_axis_0"), val = tensor&lt;int32, []&gt;(1)];
            tensor&lt;int32, []&gt; model_tf_compat_v1_gather_2_GatherV2_batch_dims_0 = const()[name = tensor&lt;string, []&gt;("model_tf_compat_v1_gather_2_GatherV2_batch_dims_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, [1, 192, 24]&gt; Identity_2 = gather(axis = model_tf_compat_v1_gather_2_GatherV2_axis_0, batch_dims = model_tf_compat_v1_gather_2_GatherV2_batch_dims_0, indices = model_tf___operators___getitem_2_strided_slice, x = model_extend_layer_concat_2);
            tensor&lt;int32, []&gt; model_tf_compat_v1_gather_3_GatherV2_axis_0 = const()[name = tensor&lt;string, []&gt;("model_tf_compat_v1_gather_3_GatherV2_axis_0"), val = tensor&lt;int32, []&gt;(1)];
            tensor&lt;int32, []&gt; model_tf_compat_v1_gather_3_GatherV2_batch_dims_0 = const()[name = tensor&lt;string, []&gt;("model_tf_compat_v1_gather_3_GatherV2_batch_dims_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, [1, 192, 1]&gt; Identity_3 = gather(axis = model_tf_compat_v1_gather_3_GatherV2_axis_0, batch_dims = model_tf_compat_v1_gather_3_GatherV2_batch_dims_0, indices = model_tf___operators___getitem_5_strided_slice, x = model_extend_layer_concat_3);
            tensor&lt;int32, []&gt; model_tf_compat_v1_gather_4_GatherV2_axis_0 = const()[name = tensor&lt;string, []&gt;("model_tf_compat_v1_gather_4_GatherV2_axis_0"), val = tensor&lt;int32, []&gt;(1)];
            tensor&lt;int32, []&gt; model_tf_compat_v1_gather_4_GatherV2_batch_dims_0 = const()[name = tensor&lt;string, []&gt;("model_tf_compat_v1_gather_4_GatherV2_batch_dims_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, [1, 192, 24]&gt; Identity_4 = gather(axis = model_tf_compat_v1_gather_4_GatherV2_axis_0, batch_dims = model_tf_compat_v1_gather_4_GatherV2_batch_dims_0, indices = model_tf___operators___getitem_5_strided_slice, x = model_extend_layer_concat_2);
            tensor&lt;string, []&gt; model_tf_cast_3_Cast_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_tf_cast_3_Cast_dtype_0"), val = tensor&lt;string, []&gt;("fp16")];
            tensor&lt;fp16, [1, 192, 768]&gt; Identity = cast(dtype = model_tf_cast_3_Cast_dtype_0, x = model_tf_compat_v1_gather_GatherV2);
        } -&gt; (Identity, Identity_1, Identity_2, Identity_3, Identity_4);
}</code></pre>
                    </div>
                </div>
            
                <div class="file-section" id="file-19">
                    <div class="file-header">
                        <h2>ðŸ“„ model.mil 6</h2>
                        <div class="file-meta">Size: 14.07 KB</div>
                    </div>
                    <div class="file-content">
                        <pre><code>program(1.0)
[buildInfo = dict&lt;tensor&lt;string, []&gt;, tensor&lt;string, []&gt;&gt;({{"coremlc-component-MIL", "3404.16.1"}, {"coremlc-version", "3404.24.1"}})]
{
    func main&lt;ios16&gt;(tensor&lt;fp32, [?, 64]&gt; input_10, tensor&lt;fp32, [?, 1]&gt; input_5, tensor&lt;int32, [?, 1]&gt; input_6, tensor&lt;fp32, [?, 64]&gt; input_7, tensor&lt;int32, [?, 24]&gt; input_9) [FlexibleShapeInformation = tuple&lt;tuple&lt;tensor&lt;string, []&gt;, dict&lt;tensor&lt;string, []&gt;, tensor&lt;int32, [?]&gt;&gt;&gt;, tuple&lt;tensor&lt;string, []&gt;, dict&lt;tensor&lt;string, []&gt;, list&lt;tensor&lt;int32, [2]&gt;, ?&gt;&gt;&gt;&gt;((("DefaultShapes", {{"input_10", [1, 64]}, {"input_5", [1, 1]}, {"input_6", [1, 1]}, {"input_7", [1, 64]}, {"input_9", [1, 24]}}), ("RangeDims", {{"input_10", [[1, -1], [64, 64]]}, {"input_5", [[1, -1], [1, 1]]}, {"input_6", [[1, -1], [1, 1]]}, {"input_7", [[1, -1], [64, 64]]}, {"input_9", [[1, -1], [24, 24]]}})))] {
            tensor&lt;int32, [1]&gt; model_2_multiply_ExpandDims_axes_0 = const()[name = tensor&lt;string, []&gt;("model_2_multiply_ExpandDims_axes_0"), val = tensor&lt;int32, [1]&gt;([1])];
            tensor&lt;string, []&gt; input_7_to_fp16_dtype_0 = const()[name = tensor&lt;string, []&gt;("input_7_to_fp16_dtype_0"), val = tensor&lt;string, []&gt;("fp16")];
            tensor&lt;fp16, [?, 64]&gt; cast_34 = cast(dtype = input_7_to_fp16_dtype_0, x = input_7);
            tensor&lt;fp16, [?, 1, 64]&gt; model_2_multiply_ExpandDims_cast = expand_dims(axes = model_2_multiply_ExpandDims_axes_0, x = cast_34);
            tensor&lt;int32, []&gt; model_2_embedding_embedding_lookup_axis_0 = const()[name = tensor&lt;string, []&gt;("model_2_embedding_embedding_lookup_axis_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, []&gt; model_2_embedding_embedding_lookup_batch_dims_0 = const()[name = tensor&lt;string, []&gt;("model_2_embedding_embedding_lookup_batch_dims_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;fp16, [2364, 64]&gt; model_2_embedding_embedding_lookup_973_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_embedding_embedding_lookup_973_to_fp16"), val = tensor&lt;fp16, [2364, 64]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(64)))];
            tensor&lt;fp16, [?, 1, 64]&gt; model_2_embedding_embedding_lookup_cast = gather(axis = model_2_embedding_embedding_lookup_axis_0, batch_dims = model_2_embedding_embedding_lookup_batch_dims_0, indices = input_6, x = model_2_embedding_embedding_lookup_973_to_fp16);
            tensor&lt;fp16, []&gt; model_2_1025_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_1025_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1p+0)];
            tensor&lt;string, []&gt; input_5_to_fp16_dtype_0 = const()[name = tensor&lt;string, []&gt;("input_5_to_fp16_dtype_0"), val = tensor&lt;string, []&gt;("fp16")];
            tensor&lt;fp16, [?, 1]&gt; cast_31 = cast(dtype = input_5_to_fp16_dtype_0, x = input_5);
            tensor&lt;fp16, [?, 1]&gt; model_2_tf_math_subtract_1_Sub_cast = sub(x = model_2_1025_to_fp16, y = cast_31);
            tensor&lt;int32, [1]&gt; model_2_multiply_1_ExpandDims_axes_0 = const()[name = tensor&lt;string, []&gt;("model_2_multiply_1_ExpandDims_axes_0"), val = tensor&lt;int32, [1]&gt;([1])];
            tensor&lt;string, []&gt; input_10_to_fp16_dtype_0 = const()[name = tensor&lt;string, []&gt;("input_10_to_fp16_dtype_0"), val = tensor&lt;string, []&gt;("fp16")];
            tensor&lt;fp16, [?, 64]&gt; cast_29 = cast(dtype = input_10_to_fp16_dtype_0, x = input_10);
            tensor&lt;fp16, [?, 1, 64]&gt; model_2_multiply_1_ExpandDims_cast = expand_dims(axes = model_2_multiply_1_ExpandDims_axes_0, x = cast_29);
            tensor&lt;int32, []&gt; model_2_embedding_1_embedding_lookup_axis_0 = const()[name = tensor&lt;string, []&gt;("model_2_embedding_1_embedding_lookup_axis_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, []&gt; model_2_embedding_1_embedding_lookup_batch_dims_0 = const()[name = tensor&lt;string, []&gt;("model_2_embedding_1_embedding_lookup_batch_dims_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;fp16, [11581, 64]&gt; model_2_embedding_1_embedding_lookup_966_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_embedding_1_embedding_lookup_966_to_fp16"), val = tensor&lt;fp16, [11581, 64]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(302720)))];
            tensor&lt;fp16, [?, 24, 64]&gt; model_2_embedding_1_embedding_lookup_cast = gather(axis = model_2_embedding_1_embedding_lookup_axis_0, batch_dims = model_2_embedding_1_embedding_lookup_batch_dims_0, indices = input_9, x = model_2_embedding_1_embedding_lookup_966_to_fp16);
            tensor&lt;fp16, [?, 1, 64]&gt; model_2_tf_math_l2_normalize_1_l2_normalize_Square_cast = mul(x = model_2_embedding_embedding_lookup_cast, y = model_2_embedding_embedding_lookup_cast);
            tensor&lt;int32, [1]&gt; model_2_tf_math_reduce_mean_3_Mean_axes_0 = const()[name = tensor&lt;string, []&gt;("model_2_tf_math_reduce_mean_3_Mean_axes_0"), val = tensor&lt;int32, [1]&gt;([1])];
            tensor&lt;bool, []&gt; model_2_tf_math_reduce_mean_3_Mean_keep_dims_0 = const()[name = tensor&lt;string, []&gt;("model_2_tf_math_reduce_mean_3_Mean_keep_dims_0"), val = tensor&lt;bool, []&gt;(true)];
            tensor&lt;fp16, [?, 1, 64]&gt; model_2_tf_math_reduce_mean_3_Mean_cast = reduce_mean(axes = model_2_tf_math_reduce_mean_3_Mean_axes_0, keep_dims = model_2_tf_math_reduce_mean_3_Mean_keep_dims_0, x = model_2_embedding_1_embedding_lookup_cast);
            tensor&lt;int32, [1]&gt; model_2_tf_math_l2_normalize_1_l2_normalize_Sum_axes_0 = const()[name = tensor&lt;string, []&gt;("model_2_tf_math_l2_normalize_1_l2_normalize_Sum_axes_0"), val = tensor&lt;int32, [1]&gt;([-1])];
            tensor&lt;bool, []&gt; model_2_tf_math_l2_normalize_1_l2_normalize_Sum_keep_dims_0 = const()[name = tensor&lt;string, []&gt;("model_2_tf_math_l2_normalize_1_l2_normalize_Sum_keep_dims_0"), val = tensor&lt;bool, []&gt;(true)];
            tensor&lt;fp16, [?, 1, 1]&gt; model_2_tf_math_l2_normalize_1_l2_normalize_Sum_cast = reduce_sum(axes = model_2_tf_math_l2_normalize_1_l2_normalize_Sum_axes_0, keep_dims = model_2_tf_math_l2_normalize_1_l2_normalize_Sum_keep_dims_0, x = model_2_tf_math_l2_normalize_1_l2_normalize_Square_cast);
            tensor&lt;fp16, [?, 1, 64]&gt; model_2_tf_math_l2_normalize_3_l2_normalize_Square_cast = mul(x = model_2_tf_math_reduce_mean_3_Mean_cast, y = model_2_tf_math_reduce_mean_3_Mean_cast);
            tensor&lt;fp16, []&gt; model_2_tf_math_l2_normalize_1_l2_normalize_Maximum_y_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_tf_math_l2_normalize_1_l2_normalize_Maximum_y_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1p-24)];
            tensor&lt;fp16, [?, 1, 1]&gt; model_2_tf_math_l2_normalize_1_l2_normalize_Maximum_cast = maximum(x = model_2_tf_math_l2_normalize_1_l2_normalize_Sum_cast, y = model_2_tf_math_l2_normalize_1_l2_normalize_Maximum_y_to_fp16);
            tensor&lt;int32, [1]&gt; model_2_tf_math_l2_normalize_3_l2_normalize_Sum_axes_0 = const()[name = tensor&lt;string, []&gt;("model_2_tf_math_l2_normalize_3_l2_normalize_Sum_axes_0"), val = tensor&lt;int32, [1]&gt;([-1])];
            tensor&lt;bool, []&gt; model_2_tf_math_l2_normalize_3_l2_normalize_Sum_keep_dims_0 = const()[name = tensor&lt;string, []&gt;("model_2_tf_math_l2_normalize_3_l2_normalize_Sum_keep_dims_0"), val = tensor&lt;bool, []&gt;(true)];
            tensor&lt;fp16, [?, 1, 1]&gt; model_2_tf_math_l2_normalize_3_l2_normalize_Sum_cast = reduce_sum(axes = model_2_tf_math_l2_normalize_3_l2_normalize_Sum_axes_0, keep_dims = model_2_tf_math_l2_normalize_3_l2_normalize_Sum_keep_dims_0, x = model_2_tf_math_l2_normalize_3_l2_normalize_Square_cast);
            tensor&lt;fp16, []&gt; model_2_tf_math_l2_normalize_1_l2_normalize_Rsqrt_epsilon_0_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_tf_math_l2_normalize_1_l2_normalize_Rsqrt_epsilon_0_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1p-24)];
            tensor&lt;fp16, [?, 1, 1]&gt; model_2_tf_math_l2_normalize_1_l2_normalize_Rsqrt_cast = rsqrt(epsilon = model_2_tf_math_l2_normalize_1_l2_normalize_Rsqrt_epsilon_0_to_fp16, x = model_2_tf_math_l2_normalize_1_l2_normalize_Maximum_cast);
            tensor&lt;fp16, []&gt; model_2_tf_math_l2_normalize_3_l2_normalize_Maximum_y_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_tf_math_l2_normalize_3_l2_normalize_Maximum_y_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1p-24)];
            tensor&lt;fp16, [?, 1, 1]&gt; model_2_tf_math_l2_normalize_3_l2_normalize_Maximum_cast = maximum(x = model_2_tf_math_l2_normalize_3_l2_normalize_Sum_cast, y = model_2_tf_math_l2_normalize_3_l2_normalize_Maximum_y_to_fp16);
            tensor&lt;fp16, [?, 1, 64]&gt; model_2_tf_math_l2_normalize_1_l2_normalize_cast = mul(x = model_2_embedding_embedding_lookup_cast, y = model_2_tf_math_l2_normalize_1_l2_normalize_Rsqrt_cast);
            tensor&lt;fp16, []&gt; model_2_tf_math_l2_normalize_3_l2_normalize_Rsqrt_epsilon_0_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_tf_math_l2_normalize_3_l2_normalize_Rsqrt_epsilon_0_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1p-24)];
            tensor&lt;fp16, [?, 1, 1]&gt; model_2_tf_math_l2_normalize_3_l2_normalize_Rsqrt_cast = rsqrt(epsilon = model_2_tf_math_l2_normalize_3_l2_normalize_Rsqrt_epsilon_0_to_fp16, x = model_2_tf_math_l2_normalize_3_l2_normalize_Maximum_cast);
            tensor&lt;fp16, [?, 1, 64]&gt; model_2_multiply_mul_cast = mul(x = model_2_multiply_ExpandDims_cast, y = model_2_tf_math_l2_normalize_1_l2_normalize_cast);
            tensor&lt;fp16, [?, 1, 64]&gt; model_2_tf_math_l2_normalize_3_l2_normalize_cast = mul(x = model_2_tf_math_reduce_mean_3_Mean_cast, y = model_2_tf_math_l2_normalize_3_l2_normalize_Rsqrt_cast);
            tensor&lt;int32, [1]&gt; model_2_tf_math_reduce_sum_Sum_axes_0 = const()[name = tensor&lt;string, []&gt;("model_2_tf_math_reduce_sum_Sum_axes_0"), val = tensor&lt;int32, [1]&gt;([-1])];
            tensor&lt;bool, []&gt; model_2_tf_math_reduce_sum_Sum_keep_dims_0 = const()[name = tensor&lt;string, []&gt;("model_2_tf_math_reduce_sum_Sum_keep_dims_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [?, 1]&gt; model_2_tf_math_reduce_sum_Sum_cast = reduce_sum(axes = model_2_tf_math_reduce_sum_Sum_axes_0, keep_dims = model_2_tf_math_reduce_sum_Sum_keep_dims_0, x = model_2_multiply_mul_cast);
            tensor&lt;fp16, [?, 1, 64]&gt; model_2_multiply_1_mul_cast = mul(x = model_2_multiply_1_ExpandDims_cast, y = model_2_tf_math_l2_normalize_3_l2_normalize_cast);
            tensor&lt;int32, [1]&gt; model_2_tf_math_reduce_sum_1_Sum_axes_0 = const()[name = tensor&lt;string, []&gt;("model_2_tf_math_reduce_sum_1_Sum_axes_0"), val = tensor&lt;int32, [1]&gt;([-1])];
            tensor&lt;bool, []&gt; model_2_tf_math_reduce_sum_1_Sum_keep_dims_0 = const()[name = tensor&lt;string, []&gt;("model_2_tf_math_reduce_sum_1_Sum_keep_dims_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [?, 1]&gt; model_2_tf_math_reduce_sum_1_Sum_cast = reduce_sum(axes = model_2_tf_math_reduce_sum_1_Sum_axes_0, keep_dims = model_2_tf_math_reduce_sum_1_Sum_keep_dims_0, x = model_2_multiply_1_mul_cast);
            tensor&lt;fp16, []&gt; model_2_1009_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_1009_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1p+1)];
            tensor&lt;fp16, [?, 1]&gt; model_2_tf_math_multiply_Mul_cast = mul(x = model_2_1009_to_fp16, y = model_2_tf_math_reduce_sum_Sum_cast);
            tensor&lt;fp16, []&gt; model_2_tf_clip_by_value_clip_by_value_Minimum_y_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_tf_clip_by_value_clip_by_value_Minimum_y_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1p+0)];
            tensor&lt;fp16, [?, 1]&gt; model_2_tf_clip_by_value_clip_by_value_Minimum_cast = minimum(x = model_2_tf_math_multiply_Mul_cast, y = model_2_tf_clip_by_value_clip_by_value_Minimum_y_to_fp16);
            tensor&lt;fp16, []&gt; model_2_1006_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_1006_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1.cccp+0)];
            tensor&lt;fp16, [?, 1]&gt; model_2_tf_math_multiply_2_Mul_cast = mul(x = model_2_1006_to_fp16, y = model_2_tf_math_reduce_sum_1_Sum_cast);
            tensor&lt;fp16, []&gt; model_2_tf_clip_by_value_clip_by_value_y_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_tf_clip_by_value_clip_by_value_y_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1.8p-1)];
            tensor&lt;fp16, [?, 1]&gt; model_2_tf_clip_by_value_clip_by_value_cast = maximum(x = model_2_tf_clip_by_value_clip_by_value_Minimum_cast, y = model_2_tf_clip_by_value_clip_by_value_y_to_fp16);
            tensor&lt;fp16, []&gt; model_2_tf_clip_by_value_1_clip_by_value_Minimum_y_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_tf_clip_by_value_1_clip_by_value_Minimum_y_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1p+0)];
            tensor&lt;fp16, [?, 1]&gt; model_2_tf_clip_by_value_1_clip_by_value_Minimum_cast = minimum(x = model_2_tf_math_multiply_2_Mul_cast, y = model_2_tf_clip_by_value_1_clip_by_value_Minimum_y_to_fp16);
            tensor&lt;fp16, [?, 1]&gt; model_2_tf_math_multiply_1_Mul_cast = mul(x = cast_31, y = model_2_tf_clip_by_value_clip_by_value_cast);
            tensor&lt;fp16, []&gt; model_2_tf_clip_by_value_1_clip_by_value_y_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_tf_clip_by_value_1_clip_by_value_y_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1.8p-1)];
            tensor&lt;fp16, [?, 1]&gt; model_2_tf_clip_by_value_1_clip_by_value_cast = maximum(x = model_2_tf_clip_by_value_1_clip_by_value_Minimum_cast, y = model_2_tf_clip_by_value_1_clip_by_value_y_to_fp16);
            tensor&lt;fp16, [?, 1]&gt; model_2_tf___operators___add_AddV2_cast = add(x = model_2_tf_math_multiply_1_Mul_cast, y = model_2_tf_math_subtract_1_Sub_cast);
            tensor&lt;fp16, [?, 1]&gt; model_2_tf_math_multiply_3_Mul_cast = mul(x = cast_31, y = model_2_tf_clip_by_value_1_clip_by_value_cast);
            tensor&lt;fp16, [?, 1]&gt; model_2_tf___operators___add_1_AddV2_cast = add(x = model_2_tf_math_multiply_3_Mul_cast, y = model_2_tf_math_subtract_1_Sub_cast);
            tensor&lt;fp16, [?, 1]&gt; model_2_multiply_2_mul_cast = mul(x = model_2_tf___operators___add_AddV2_cast, y = model_2_tf___operators___add_1_AddV2_cast);
            tensor&lt;fp16, []&gt; model_2_tf_math_maximum_Maximum_y_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_tf_math_maximum_Maximum_y_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1.8p-1)];
            tensor&lt;fp16, [?, 1]&gt; model_2_tf_math_maximum_Maximum_cast = maximum(x = model_2_multiply_2_mul_cast, y = model_2_tf_math_maximum_Maximum_y_to_fp16);
            tensor&lt;string, []&gt; model_2_tf_math_maximum_Maximum_cast_to_fp32_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_2_tf_math_maximum_Maximum_cast_to_fp32_dtype_0"), val = tensor&lt;string, []&gt;("fp32")];
            tensor&lt;fp32, [?, 1]&gt; Identity = cast(dtype = model_2_tf_math_maximum_Maximum_cast_to_fp32_dtype_0, x = model_2_tf_math_maximum_Maximum_cast);
        } -&gt; (Identity);
}</code></pre>
                    </div>
                </div>
            
                <div class="file-section" id="file-20">
                    <div class="file-header">
                        <h2>ðŸ“„ model.mil 22</h2>
                        <div class="file-meta">Size: 33.46 KB</div>
                    </div>
                    <div class="file-content">
                        <pre><code>program(1.0)
[buildInfo = dict&lt;tensor&lt;string, []&gt;, tensor&lt;string, []&gt;&gt;({{"coremlc-component-MIL", "3404.16.1"}, {"coremlc-version", "3404.24.1"}})]
{
    func main&lt;ios16&gt;(tensor&lt;fp16, [?, 1, 768]&gt; article_encoding, tensor&lt;int32, [?]&gt; channel_id, tensor&lt;int32, [?, 1]&gt; channel_quality, tensor&lt;int32, [?, 1]&gt; channel_relevance, tensor&lt;fp32, [?, 1]&gt; clicks, tensor&lt;fp32, [?, 1]&gt; impressions, tensor&lt;int32, [?]&gt; is_paid, tensor&lt;fp32, [?, 24, 1]&gt; topic_clicks, tensor&lt;int32, [?, 24]&gt; topic_ids, tensor&lt;fp32, [?, 24, 1]&gt; topic_impressions, tensor&lt;int32, [?, 24]&gt; topic_quality, tensor&lt;int32, [?, 24]&gt; topic_relevance, tensor&lt;fp32, [?, 128]&gt; user_hist_enc) [FlexibleShapeInformation = tuple&lt;tuple&lt;tensor&lt;string, []&gt;, dict&lt;tensor&lt;string, []&gt;, tensor&lt;int32, [?]&gt;&gt;&gt;, tuple&lt;tensor&lt;string, []&gt;, dict&lt;tensor&lt;string, []&gt;, list&lt;tensor&lt;int32, [2]&gt;, ?&gt;&gt;&gt;&gt;((("DefaultShapes", {{"article_encoding", [1, 1, 768]}, {"channel_id", [1]}, {"channel_quality", [1, 1]}, {"channel_relevance", [1, 1]}, {"clicks", [1, 1]}, {"impressions", [1, 1]}, {"is_paid", [1]}, {"topic_clicks", [1, 24, 1]}, {"topic_ids", [1, 24]}, {"topic_impressions", [1, 24, 1]}, {"topic_quality", [1, 24]}, {"topic_relevance", [1, 24]}, {"user_hist_enc", [1, 128]}}), ("RangeDims", {{"article_encoding", [[1, -1], [1, 1], [768, 768]]}, {"channel_id", [[1, -1]]}, {"channel_quality", [[1, -1], [1, 1]]}, {"channel_relevance", [[1, -1], [1, 1]]}, {"clicks", [[1, -1], [1, 1]]}, {"impressions", [[1, -1], [1, 1]]}, {"is_paid", [[1, -1]]}, {"topic_clicks", [[1, -1], [24, 24], [1, 1]]}, {"topic_ids", [[1, -1], [24, 24]]}, {"topic_impressions", [[1, -1], [24, 24], [1, 1]]}, {"topic_quality", [[1, -1], [24, 24]]}, {"topic_relevance", [[1, -1], [24, 24]]}, {"user_hist_enc", [[1, -1], [128, 128]]}})))] {
            tensor&lt;int32, []&gt; model_2_tf_math_equal_Equal_y = const()[name = tensor&lt;string, []&gt;("model_2_tf_math_equal_Equal_y"), val = tensor&lt;int32, []&gt;(5)];
            tensor&lt;int32, []&gt; model_2_tf_math_multiply_1_Mul_y = const()[name = tensor&lt;string, []&gt;("model_2_tf_math_multiply_1_Mul_y"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, []&gt; model_2_tf_math_greater_Greater_y = const()[name = tensor&lt;string, []&gt;("model_2_tf_math_greater_Greater_y"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, []&gt; model_2_tf_math_equal_1_Equal_y = const()[name = tensor&lt;string, []&gt;("model_2_tf_math_equal_1_Equal_y"), val = tensor&lt;int32, []&gt;(8)];
            tensor&lt;int32, []&gt; model_2_tf_math_multiply_2_Mul_y = const()[name = tensor&lt;string, []&gt;("model_2_tf_math_multiply_2_Mul_y"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, []&gt; model_2_tf_math_greater_1_Greater_y = const()[name = tensor&lt;string, []&gt;("model_2_tf_math_greater_1_Greater_y"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, [1]&gt; model_2_tf_compat_v1_squeeze_1_Squeeze_axes_0 = const()[name = tensor&lt;string, []&gt;("model_2_tf_compat_v1_squeeze_1_Squeeze_axes_0"), val = tensor&lt;int32, [1]&gt;([-2])];
            tensor&lt;fp16, [?, 768]&gt; model_2_tf_compat_v1_squeeze_1_Squeeze_cast = squeeze(axes = model_2_tf_compat_v1_squeeze_1_Squeeze_axes_0, x = article_encoding);
            tensor&lt;int32, []&gt; model_2_embedding_4_embedding_lookup_axis_0 = const()[name = tensor&lt;string, []&gt;("model_2_embedding_4_embedding_lookup_axis_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, []&gt; model_2_embedding_4_embedding_lookup_batch_dims_0 = const()[name = tensor&lt;string, []&gt;("model_2_embedding_4_embedding_lookup_batch_dims_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;fp16, [2012, 128]&gt; model_2_embedding_4_embedding_lookup_12110_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_embedding_4_embedding_lookup_12110_to_fp16"), val = tensor&lt;fp16, [2012, 128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(64)))];
            tensor&lt;fp16, [?, 128]&gt; model_2_embedding_4_embedding_lookup_cast = gather(axis = model_2_embedding_4_embedding_lookup_axis_0, batch_dims = model_2_embedding_4_embedding_lookup_batch_dims_0, indices = channel_id, x = model_2_embedding_4_embedding_lookup_12110_to_fp16);
            tensor&lt;int32, []&gt; model_2_embedding_5_embedding_lookup_axis_0 = const()[name = tensor&lt;string, []&gt;("model_2_embedding_5_embedding_lookup_axis_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, []&gt; model_2_embedding_5_embedding_lookup_batch_dims_0 = const()[name = tensor&lt;string, []&gt;("model_2_embedding_5_embedding_lookup_batch_dims_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;fp16, [13980, 128]&gt; model_2_embedding_5_embedding_lookup_12039_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_embedding_5_embedding_lookup_12039_to_fp16"), val = tensor&lt;fp16, [13980, 128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(515200)))];
            tensor&lt;fp16, [?, 24, 128]&gt; model_2_embedding_5_embedding_lookup_cast = gather(axis = model_2_embedding_5_embedding_lookup_axis_0, batch_dims = model_2_embedding_5_embedding_lookup_batch_dims_0, indices = topic_ids, x = model_2_embedding_5_embedding_lookup_12039_to_fp16);
            tensor&lt;string, []&gt; topic_impressions_to_fp16_dtype_0 = const()[name = tensor&lt;string, []&gt;("topic_impressions_to_fp16_dtype_0"), val = tensor&lt;string, []&gt;("fp16")];
            tensor&lt;fp16, []&gt; model_2_tf___operators___add_2_y_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_tf___operators___add_2_y_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1p+0)];
            tensor&lt;fp16, [?, 24, 1]&gt; cast_75 = cast(dtype = topic_impressions_to_fp16_dtype_0, x = topic_impressions);
            tensor&lt;fp16, [?, 24, 1]&gt; model_2_tf___operators___add_2_AddV2_cast = add(x = cast_75, y = model_2_tf___operators___add_2_y_to_fp16);
            tensor&lt;string, []&gt; impressions_to_fp16_dtype_0 = const()[name = tensor&lt;string, []&gt;("impressions_to_fp16_dtype_0"), val = tensor&lt;string, []&gt;("fp16")];
            tensor&lt;fp16, []&gt; model_2_tf___operators___add_3_y_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_tf___operators___add_3_y_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1p+0)];
            tensor&lt;fp16, [?, 1]&gt; cast_73 = cast(dtype = impressions_to_fp16_dtype_0, x = impressions);
            tensor&lt;fp16, [?, 1]&gt; model_2_tf___operators___add_3_AddV2_cast = add(x = cast_73, y = model_2_tf___operators___add_3_y_to_fp16);
            tensor&lt;bool, [?, 24]&gt; model_2_tf_math_equal_Equal = equal(x = topic_quality, y = model_2_tf_math_equal_Equal_y);
            tensor&lt;int32, [?, 24]&gt; model_2_tf_math_multiply_1_Mul = mul(x = topic_quality, y = model_2_tf_math_multiply_1_Mul_y);
            tensor&lt;bool, [?, 24]&gt; model_2_tf_math_equal_1_Equal = equal(x = topic_relevance, y = model_2_tf_math_equal_1_Equal_y);
            tensor&lt;int32, [?, 24]&gt; model_2_tf_math_multiply_2_Mul = mul(x = topic_relevance, y = model_2_tf_math_multiply_2_Mul_y);
            tensor&lt;int32, []&gt; model_2_embedding_8_embedding_lookup_axis_0 = const()[name = tensor&lt;string, []&gt;("model_2_embedding_8_embedding_lookup_axis_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, []&gt; model_2_embedding_8_embedding_lookup_batch_dims_0 = const()[name = tensor&lt;string, []&gt;("model_2_embedding_8_embedding_lookup_batch_dims_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;fp16, [2, 128]&gt; model_2_embedding_8_embedding_lookup_12158_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_embedding_8_embedding_lookup_12158_to_fp16"), val = tensor&lt;fp16, [2, 128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4094144)))];
            tensor&lt;fp16, [?, 128]&gt; model_2_embedding_8_embedding_lookup_cast = gather(axis = model_2_embedding_8_embedding_lookup_axis_0, batch_dims = model_2_embedding_8_embedding_lookup_batch_dims_0, indices = is_paid, x = model_2_embedding_8_embedding_lookup_12158_to_fp16);
            tensor&lt;string, []&gt; topic_clicks_to_fp16_dtype_0 = const()[name = tensor&lt;string, []&gt;("topic_clicks_to_fp16_dtype_0"), val = tensor&lt;string, []&gt;("fp16")];
            tensor&lt;fp16, [?, 24, 1]&gt; cast_70 = cast(dtype = topic_clicks_to_fp16_dtype_0, x = topic_clicks);
            tensor&lt;fp16, [?, 24, 1]&gt; model_2_tf_math_truediv_truediv_cast = real_div(x = cast_70, y = model_2_tf___operators___add_2_AddV2_cast);
            tensor&lt;string, []&gt; clicks_to_fp16_dtype_0 = const()[name = tensor&lt;string, []&gt;("clicks_to_fp16_dtype_0"), val = tensor&lt;string, []&gt;("fp16")];
            tensor&lt;fp16, [?, 1]&gt; cast_68 = cast(dtype = clicks_to_fp16_dtype_0, x = clicks);
            tensor&lt;fp16, [?, 1]&gt; model_2_tf_math_truediv_1_truediv_cast = real_div(x = cast_68, y = model_2_tf___operators___add_3_AddV2_cast);
            tensor&lt;int32, [?, 24]&gt; model_2_tf_where_SelectV2 = select(a = model_2_tf_math_multiply_1_Mul, b = topic_quality, cond = model_2_tf_math_equal_Equal);
            tensor&lt;int32, [?, 24]&gt; model_2_tf_where_2_SelectV2 = select(a = model_2_tf_math_multiply_2_Mul, b = topic_relevance, cond = model_2_tf_math_equal_1_Equal);
            tensor&lt;fp16, [128, 768]&gt; transpose_0_cast_to_fp32_to_fp16 = const()[name = tensor&lt;string, []&gt;("transpose_0_cast_to_fp32_to_fp16"), val = tensor&lt;fp16, [128, 768]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4094720)))];
            tensor&lt;fp16, [128]&gt; model_2_dense_6_BiasAdd_bias_0_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_dense_6_BiasAdd_bias_0_to_fp16"), val = tensor&lt;fp16, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4291392)))];
            tensor&lt;fp16, [?, 128]&gt; model_2_dense_6_BiasAdd_cast = linear(bias = model_2_dense_6_BiasAdd_bias_0_to_fp16, weight = transpose_0_cast_to_fp32_to_fp16, x = model_2_tf_compat_v1_squeeze_1_Squeeze_cast);
            tensor&lt;fp16, []&gt; model_2_tf_math_minimum_Minimum_y_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_tf_math_minimum_Minimum_y_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1p+0)];
            tensor&lt;fp16, [?, 24, 1]&gt; model_2_tf_math_minimum_Minimum_cast = minimum(x = model_2_tf_math_truediv_truediv_cast, y = model_2_tf_math_minimum_Minimum_y_to_fp16);
            tensor&lt;fp16, []&gt; model_2_tf_math_minimum_1_Minimum_y_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_tf_math_minimum_1_Minimum_y_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1p+0)];
            tensor&lt;fp16, [?, 1]&gt; model_2_tf_math_minimum_1_Minimum_cast = minimum(x = model_2_tf_math_truediv_1_truediv_cast, y = model_2_tf_math_minimum_1_Minimum_y_to_fp16);
            tensor&lt;int32, [1]&gt; model_2_tf_math_reduce_max_Max_axes_0 = const()[name = tensor&lt;string, []&gt;("model_2_tf_math_reduce_max_Max_axes_0"), val = tensor&lt;int32, [1]&gt;([-1])];
            tensor&lt;bool, []&gt; model_2_tf_math_reduce_max_Max_keep_dims_0 = const()[name = tensor&lt;string, []&gt;("model_2_tf_math_reduce_max_Max_keep_dims_0"), val = tensor&lt;bool, []&gt;(true)];
            tensor&lt;int32, [?, 1]&gt; model_2_tf_math_reduce_max_Max = reduce_max(axes = model_2_tf_math_reduce_max_Max_axes_0, keep_dims = model_2_tf_math_reduce_max_Max_keep_dims_0, x = model_2_tf_where_SelectV2);
            tensor&lt;int32, [1]&gt; model_2_tf_math_reduce_max_1_Max_axes_0 = const()[name = tensor&lt;string, []&gt;("model_2_tf_math_reduce_max_1_Max_axes_0"), val = tensor&lt;int32, [1]&gt;([-1])];
            tensor&lt;bool, []&gt; model_2_tf_math_reduce_max_1_Max_keep_dims_0 = const()[name = tensor&lt;string, []&gt;("model_2_tf_math_reduce_max_1_Max_keep_dims_0"), val = tensor&lt;bool, []&gt;(true)];
            tensor&lt;int32, [?, 1]&gt; model_2_tf_math_reduce_max_1_Max = reduce_max(axes = model_2_tf_math_reduce_max_1_Max_axes_0, keep_dims = model_2_tf_math_reduce_max_1_Max_keep_dims_0, x = model_2_tf_where_2_SelectV2);
            tensor&lt;fp16, [20, 1]&gt; transpose_1_cast_to_fp32_to_fp16 = const()[name = tensor&lt;string, []&gt;("transpose_1_cast_to_fp32_to_fp16"), val = tensor&lt;fp16, [20, 1]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4291712)))];
            tensor&lt;fp16, [20]&gt; model_2_dense_12_BiasAdd_bias_0_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_dense_12_BiasAdd_bias_0_to_fp16"), val = tensor&lt;fp16, [20]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4291840)))];
            tensor&lt;fp16, [?, 20]&gt; model_2_dense_12_BiasAdd_cast = linear(bias = model_2_dense_12_BiasAdd_bias_0_to_fp16, weight = transpose_1_cast_to_fp32_to_fp16, x = model_2_tf_math_minimum_1_Minimum_cast);
            tensor&lt;bool, [?, 1]&gt; model_2_tf_math_greater_Greater = greater(x = model_2_tf_math_reduce_max_Max, y = model_2_tf_math_greater_Greater_y);
            tensor&lt;bool, [?, 1]&gt; model_2_tf_math_greater_1_Greater = greater(x = model_2_tf_math_reduce_max_1_Max, y = model_2_tf_math_greater_1_Greater_y);
            tensor&lt;string, []&gt; model_2_dense_6_Gelu_mul_1_mode_0 = const()[name = tensor&lt;string, []&gt;("model_2_dense_6_Gelu_mul_1_mode_0"), val = tensor&lt;string, []&gt;("EXACT")];
            tensor&lt;fp16, [?, 128]&gt; model_2_dense_6_Gelu_mul_1_cast = gelu(mode = model_2_dense_6_Gelu_mul_1_mode_0, x = model_2_dense_6_BiasAdd_cast);
            tensor&lt;int32, [?, 1]&gt; model_2_tf_where_1_SelectV2 = select(a = model_2_tf_math_reduce_max_Max, b = channel_quality, cond = model_2_tf_math_greater_Greater);
            tensor&lt;int32, [?, 1]&gt; model_2_tf_where_3_SelectV2 = select(a = model_2_tf_math_reduce_max_1_Max, b = channel_relevance, cond = model_2_tf_math_greater_1_Greater);
            tensor&lt;string, []&gt; model_2_dense_12_Gelu_mul_1_mode_0 = const()[name = tensor&lt;string, []&gt;("model_2_dense_12_Gelu_mul_1_mode_0"), val = tensor&lt;string, []&gt;("EXACT")];
            tensor&lt;fp16, [?, 20]&gt; model_2_dense_12_Gelu_mul_1_cast = gelu(mode = model_2_dense_12_Gelu_mul_1_mode_0, x = model_2_dense_12_BiasAdd_cast);
            tensor&lt;int32, [1]&gt; model_2_tf_compat_v1_squeeze_2_Squeeze_axes_0 = const()[name = tensor&lt;string, []&gt;("model_2_tf_compat_v1_squeeze_2_Squeeze_axes_0"), val = tensor&lt;int32, [1]&gt;([-1])];
            tensor&lt;int32, [?]&gt; model_2_tf_compat_v1_squeeze_2_Squeeze = squeeze(axes = model_2_tf_compat_v1_squeeze_2_Squeeze_axes_0, x = model_2_tf_where_1_SelectV2);
            tensor&lt;int32, [1]&gt; model_2_tf_compat_v1_squeeze_3_Squeeze_axes_0 = const()[name = tensor&lt;string, []&gt;("model_2_tf_compat_v1_squeeze_3_Squeeze_axes_0"), val = tensor&lt;int32, [1]&gt;([-1])];
            tensor&lt;int32, [?]&gt; model_2_tf_compat_v1_squeeze_3_Squeeze = squeeze(axes = model_2_tf_compat_v1_squeeze_3_Squeeze_axes_0, x = model_2_tf_where_3_SelectV2);
            tensor&lt;int32, []&gt; model_2_embedding_6_embedding_lookup_axis_0 = const()[name = tensor&lt;string, []&gt;("model_2_embedding_6_embedding_lookup_axis_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, []&gt; model_2_embedding_6_embedding_lookup_batch_dims_0 = const()[name = tensor&lt;string, []&gt;("model_2_embedding_6_embedding_lookup_batch_dims_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;fp16, [6, 128]&gt; model_2_embedding_6_embedding_lookup_12146_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_embedding_6_embedding_lookup_12146_to_fp16"), val = tensor&lt;fp16, [6, 128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4291968)))];
            tensor&lt;fp16, [?, 128]&gt; model_2_embedding_6_embedding_lookup_cast = gather(axis = model_2_embedding_6_embedding_lookup_axis_0, batch_dims = model_2_embedding_6_embedding_lookup_batch_dims_0, indices = model_2_tf_compat_v1_squeeze_2_Squeeze, x = model_2_embedding_6_embedding_lookup_12146_to_fp16);
            tensor&lt;int32, []&gt; model_2_embedding_7_embedding_lookup_axis_0 = const()[name = tensor&lt;string, []&gt;("model_2_embedding_7_embedding_lookup_axis_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, []&gt; model_2_embedding_7_embedding_lookup_batch_dims_0 = const()[name = tensor&lt;string, []&gt;("model_2_embedding_7_embedding_lookup_batch_dims_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;fp16, [9, 128]&gt; model_2_embedding_7_embedding_lookup_12152_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_embedding_7_embedding_lookup_12152_to_fp16"), val = tensor&lt;fp16, [9, 128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4293568)))];
            tensor&lt;fp16, [?, 128]&gt; model_2_embedding_7_embedding_lookup_cast = gather(axis = model_2_embedding_7_embedding_lookup_axis_0, batch_dims = model_2_embedding_7_embedding_lookup_batch_dims_0, indices = model_2_tf_compat_v1_squeeze_3_Squeeze, x = model_2_embedding_7_embedding_lookup_12152_to_fp16);
            tensor&lt;int32, [2]&gt; model_2_dense_8_Tensordot_stackx = const()[name = tensor&lt;string, []&gt;("model_2_dense_8_Tensordot_stackx"), val = tensor&lt;int32, [2]&gt;([-1, 1])];
            tensor&lt;fp16, [?, 1]&gt; model_2_dense_8_Tensordot_Reshape_cast = reshape(shape = model_2_dense_8_Tensordot_stackx, x = model_2_tf_math_minimum_Minimum_cast);
            tensor&lt;fp16, [128, 128]&gt; transpose_2_cast_to_fp32_to_fp16 = const()[name = tensor&lt;string, []&gt;("transpose_2_cast_to_fp32_to_fp16"), val = tensor&lt;fp16, [128, 128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4295936)))];
            tensor&lt;fp16, [128]&gt; model_2_dense_7_BiasAdd_bias_0_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_dense_7_BiasAdd_bias_0_to_fp16"), val = tensor&lt;fp16, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4328768)))];
            tensor&lt;fp16, [?, 128]&gt; model_2_dense_7_BiasAdd_cast = linear(bias = model_2_dense_7_BiasAdd_bias_0_to_fp16, weight = transpose_2_cast_to_fp32_to_fp16, x = model_2_dense_6_Gelu_mul_1_cast);
            tensor&lt;bool, []&gt; model_2_dense_8_Tensordot_MatMul_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("model_2_dense_8_Tensordot_MatMul_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; model_2_dense_8_Tensordot_MatMul_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("model_2_dense_8_Tensordot_MatMul_transpose_y_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [1, 20]&gt; model_2_dense_8_Tensordot_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_dense_8_Tensordot_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [1, 20]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4329088)))];
            tensor&lt;fp16, [?, 20]&gt; model_2_dense_8_Tensordot_MatMul_cast = matmul(transpose_x = model_2_dense_8_Tensordot_MatMul_transpose_x_0, transpose_y = model_2_dense_8_Tensordot_MatMul_transpose_y_0, x = model_2_dense_8_Tensordot_Reshape_cast, y = model_2_dense_8_Tensordot_ReadVariableOp_to_fp16);
            tensor&lt;int32, [3]&gt; model_2_dense_8_Tensordot_concat_1x = const()[name = tensor&lt;string, []&gt;("model_2_dense_8_Tensordot_concat_1x"), val = tensor&lt;int32, [3]&gt;([-1, 24, 20])];
            tensor&lt;fp16, [?, 24, 20]&gt; model_2_dense_8_Tensordot_cast = reshape(shape = model_2_dense_8_Tensordot_concat_1x, x = model_2_dense_8_Tensordot_MatMul_cast);
            tensor&lt;fp16, [20, 20]&gt; transpose_3_cast_to_fp32_to_fp16 = const()[name = tensor&lt;string, []&gt;("transpose_3_cast_to_fp32_to_fp16"), val = tensor&lt;fp16, [20, 20]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4329216)))];
            tensor&lt;fp16, [20]&gt; model_2_dense_13_BiasAdd_bias_0_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_dense_13_BiasAdd_bias_0_to_fp16"), val = tensor&lt;fp16, [20]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4330112)))];
            tensor&lt;fp16, [?, 20]&gt; model_2_dense_13_BiasAdd_cast = linear(bias = model_2_dense_13_BiasAdd_bias_0_to_fp16, weight = transpose_3_cast_to_fp32_to_fp16, x = model_2_dense_12_Gelu_mul_1_cast);
            tensor&lt;fp16, [?, 128]&gt; model_2_tf___operators___add_4_AddV2_cast = add(x = model_2_dense_7_BiasAdd_cast, y = model_2_embedding_4_embedding_lookup_cast);
            tensor&lt;fp16, [20]&gt; model_2_dense_8_BiasAdd_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_dense_8_BiasAdd_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [20]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4330240)))];
            tensor&lt;fp16, [?, 24, 20]&gt; model_2_dense_8_BiasAdd_cast = add(x = model_2_dense_8_Tensordot_cast, y = model_2_dense_8_BiasAdd_ReadVariableOp_to_fp16);
            tensor&lt;string, []&gt; model_2_dense_8_Gelu_mul_1_mode_0 = const()[name = tensor&lt;string, []&gt;("model_2_dense_8_Gelu_mul_1_mode_0"), val = tensor&lt;string, []&gt;("EXACT")];
            tensor&lt;fp16, [?, 24, 20]&gt; model_2_dense_8_Gelu_mul_1_cast = gelu(mode = model_2_dense_8_Gelu_mul_1_mode_0, x = model_2_dense_8_BiasAdd_cast);
            tensor&lt;string, []&gt; model_2_dense_13_Gelu_mul_1_mode_0 = const()[name = tensor&lt;string, []&gt;("model_2_dense_13_Gelu_mul_1_mode_0"), val = tensor&lt;string, []&gt;("EXACT")];
            tensor&lt;fp16, [?, 20]&gt; model_2_dense_13_Gelu_mul_1_cast = gelu(mode = model_2_dense_13_Gelu_mul_1_mode_0, x = model_2_dense_13_BiasAdd_cast);
            tensor&lt;fp16, [128, 20]&gt; transpose_4_cast_to_fp32_to_fp16 = const()[name = tensor&lt;string, []&gt;("transpose_4_cast_to_fp32_to_fp16"), val = tensor&lt;fp16, [128, 20]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4330368)))];
            tensor&lt;fp16, [128]&gt; model_2_dense_14_BiasAdd_bias_0_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_dense_14_BiasAdd_bias_0_to_fp16"), val = tensor&lt;fp16, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4335552)))];
            tensor&lt;fp16, [?, 128]&gt; model_2_dense_14_BiasAdd_cast = linear(bias = model_2_dense_14_BiasAdd_bias_0_to_fp16, weight = transpose_4_cast_to_fp32_to_fp16, x = model_2_dense_13_Gelu_mul_1_cast);
            tensor&lt;int32, [2]&gt; model_2_dense_9_Tensordot_stackx = const()[name = tensor&lt;string, []&gt;("model_2_dense_9_Tensordot_stackx"), val = tensor&lt;int32, [2]&gt;([-1, 20])];
            tensor&lt;fp16, [?, 20]&gt; model_2_dense_9_Tensordot_Reshape_cast = reshape(shape = model_2_dense_9_Tensordot_stackx, x = model_2_dense_8_Gelu_mul_1_cast);
            tensor&lt;bool, []&gt; model_2_dense_9_Tensordot_MatMul_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("model_2_dense_9_Tensordot_MatMul_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; model_2_dense_9_Tensordot_MatMul_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("model_2_dense_9_Tensordot_MatMul_transpose_y_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [20, 20]&gt; model_2_dense_9_Tensordot_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_dense_9_Tensordot_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [20, 20]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4335872)))];
            tensor&lt;fp16, [?, 20]&gt; model_2_dense_9_Tensordot_MatMul_cast = matmul(transpose_x = model_2_dense_9_Tensordot_MatMul_transpose_x_0, transpose_y = model_2_dense_9_Tensordot_MatMul_transpose_y_0, x = model_2_dense_9_Tensordot_Reshape_cast, y = model_2_dense_9_Tensordot_ReadVariableOp_to_fp16);
            tensor&lt;int32, [3]&gt; model_2_dense_9_Tensordot_concat_1x = const()[name = tensor&lt;string, []&gt;("model_2_dense_9_Tensordot_concat_1x"), val = tensor&lt;int32, [3]&gt;([-1, 24, 20])];
            tensor&lt;fp16, [?, 24, 20]&gt; model_2_dense_9_Tensordot_cast = reshape(shape = model_2_dense_9_Tensordot_concat_1x, x = model_2_dense_9_Tensordot_MatMul_cast);
            tensor&lt;fp16, [20]&gt; model_2_dense_9_BiasAdd_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_dense_9_BiasAdd_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [20]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4336768)))];
            tensor&lt;fp16, [?, 24, 20]&gt; model_2_dense_9_BiasAdd_cast = add(x = model_2_dense_9_Tensordot_cast, y = model_2_dense_9_BiasAdd_ReadVariableOp_to_fp16);
            tensor&lt;string, []&gt; model_2_dense_9_Gelu_mul_1_mode_0 = const()[name = tensor&lt;string, []&gt;("model_2_dense_9_Gelu_mul_1_mode_0"), val = tensor&lt;string, []&gt;("EXACT")];
            tensor&lt;fp16, [?, 24, 20]&gt; model_2_dense_9_Gelu_mul_1_cast = gelu(mode = model_2_dense_9_Gelu_mul_1_mode_0, x = model_2_dense_9_BiasAdd_cast);
            tensor&lt;int32, [2]&gt; model_2_dense_10_Tensordot_stackx = const()[name = tensor&lt;string, []&gt;("model_2_dense_10_Tensordot_stackx"), val = tensor&lt;int32, [2]&gt;([-1, 20])];
            tensor&lt;fp16, [?, 20]&gt; model_2_dense_10_Tensordot_Reshape_cast = reshape(shape = model_2_dense_10_Tensordot_stackx, x = model_2_dense_9_Gelu_mul_1_cast);
            tensor&lt;bool, []&gt; model_2_dense_10_Tensordot_MatMul_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("model_2_dense_10_Tensordot_MatMul_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; model_2_dense_10_Tensordot_MatMul_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("model_2_dense_10_Tensordot_MatMul_transpose_y_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [20, 20]&gt; model_2_dense_10_Tensordot_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_dense_10_Tensordot_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [20, 20]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4336896)))];
            tensor&lt;fp16, [?, 20]&gt; model_2_dense_10_Tensordot_MatMul_cast = matmul(transpose_x = model_2_dense_10_Tensordot_MatMul_transpose_x_0, transpose_y = model_2_dense_10_Tensordot_MatMul_transpose_y_0, x = model_2_dense_10_Tensordot_Reshape_cast, y = model_2_dense_10_Tensordot_ReadVariableOp_to_fp16);
            tensor&lt;int32, [3]&gt; model_2_dense_10_Tensordot_concat_1x = const()[name = tensor&lt;string, []&gt;("model_2_dense_10_Tensordot_concat_1x"), val = tensor&lt;int32, [3]&gt;([-1, 24, 20])];
            tensor&lt;fp16, [?, 24, 20]&gt; model_2_dense_10_Tensordot_cast = reshape(shape = model_2_dense_10_Tensordot_concat_1x, x = model_2_dense_10_Tensordot_MatMul_cast);
            tensor&lt;fp16, [20]&gt; model_2_dense_10_BiasAdd_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_dense_10_BiasAdd_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [20]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4337792)))];
            tensor&lt;fp16, [?, 24, 20]&gt; model_2_dense_10_BiasAdd_cast = add(x = model_2_dense_10_Tensordot_cast, y = model_2_dense_10_BiasAdd_ReadVariableOp_to_fp16);
            tensor&lt;string, []&gt; model_2_dense_10_Gelu_mul_1_mode_0 = const()[name = tensor&lt;string, []&gt;("model_2_dense_10_Gelu_mul_1_mode_0"), val = tensor&lt;string, []&gt;("EXACT")];
            tensor&lt;fp16, [?, 24, 20]&gt; model_2_dense_10_Gelu_mul_1_cast = gelu(mode = model_2_dense_10_Gelu_mul_1_mode_0, x = model_2_dense_10_BiasAdd_cast);
            tensor&lt;int32, [2]&gt; model_2_dense_11_Tensordot_stackx = const()[name = tensor&lt;string, []&gt;("model_2_dense_11_Tensordot_stackx"), val = tensor&lt;int32, [2]&gt;([-1, 20])];
            tensor&lt;fp16, [?, 20]&gt; model_2_dense_11_Tensordot_Reshape_cast = reshape(shape = model_2_dense_11_Tensordot_stackx, x = model_2_dense_10_Gelu_mul_1_cast);
            tensor&lt;bool, []&gt; model_2_dense_11_Tensordot_MatMul_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("model_2_dense_11_Tensordot_MatMul_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; model_2_dense_11_Tensordot_MatMul_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("model_2_dense_11_Tensordot_MatMul_transpose_y_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [20, 1]&gt; model_2_dense_11_Tensordot_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_dense_11_Tensordot_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [20, 1]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4337920)))];
            tensor&lt;fp16, [?, 1]&gt; model_2_dense_11_Tensordot_MatMul_cast = matmul(transpose_x = model_2_dense_11_Tensordot_MatMul_transpose_x_0, transpose_y = model_2_dense_11_Tensordot_MatMul_transpose_y_0, x = model_2_dense_11_Tensordot_Reshape_cast, y = model_2_dense_11_Tensordot_ReadVariableOp_to_fp16);
            tensor&lt;int32, [3]&gt; model_2_dense_11_Tensordot_concat_1x = const()[name = tensor&lt;string, []&gt;("model_2_dense_11_Tensordot_concat_1x"), val = tensor&lt;int32, [3]&gt;([-1, 24, 1])];
            tensor&lt;fp16, [?, 24, 1]&gt; model_2_dense_11_Tensordot_cast = reshape(shape = model_2_dense_11_Tensordot_concat_1x, x = model_2_dense_11_Tensordot_MatMul_cast);
            tensor&lt;fp16, [1]&gt; model_2_dense_11_BiasAdd_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_dense_11_BiasAdd_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [1]&gt;([0x1.e18p-1])];
            tensor&lt;fp16, [?, 24, 1]&gt; model_2_dense_11_BiasAdd_cast = add(x = model_2_dense_11_Tensordot_cast, y = model_2_dense_11_BiasAdd_ReadVariableOp_to_fp16);
            tensor&lt;fp16, [?, 24, 128]&gt; model_2_tf_math_multiply_Mul_cast = mul(x = model_2_embedding_5_embedding_lookup_cast, y = model_2_dense_11_BiasAdd_cast);
            tensor&lt;int32, [1]&gt; model_2_tf_math_reduce_mean_2_Mean_axes_0 = const()[name = tensor&lt;string, []&gt;("model_2_tf_math_reduce_mean_2_Mean_axes_0"), val = tensor&lt;int32, [1]&gt;([-2])];
            tensor&lt;bool, []&gt; model_2_tf_math_reduce_mean_2_Mean_keep_dims_0 = const()[name = tensor&lt;string, []&gt;("model_2_tf_math_reduce_mean_2_Mean_keep_dims_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [?, 128]&gt; model_2_tf_math_reduce_mean_2_Mean_cast = reduce_mean(axes = model_2_tf_math_reduce_mean_2_Mean_axes_0, keep_dims = model_2_tf_math_reduce_mean_2_Mean_keep_dims_0, x = model_2_tf_math_multiply_Mul_cast);
            tensor&lt;fp16, [?, 128]&gt; model_2_tf___operators___add_5_AddV2_cast = add(x = model_2_tf___operators___add_4_AddV2_cast, y = model_2_tf_math_reduce_mean_2_Mean_cast);
            tensor&lt;fp16, [?, 128]&gt; model_2_tf___operators___add_6_AddV2_cast = add(x = model_2_tf___operators___add_5_AddV2_cast, y = model_2_dense_14_BiasAdd_cast);
            tensor&lt;fp16, [?, 128]&gt; model_2_tf___operators___add_7_AddV2_cast = add(x = model_2_tf___operators___add_6_AddV2_cast, y = model_2_embedding_6_embedding_lookup_cast);
            tensor&lt;fp16, [?, 128]&gt; model_2_tf___operators___add_8_AddV2_cast = add(x = model_2_tf___operators___add_7_AddV2_cast, y = model_2_embedding_7_embedding_lookup_cast);
            tensor&lt;fp16, [?, 128]&gt; model_2_tf___operators___add_9_AddV2_cast = add(x = model_2_tf___operators___add_8_AddV2_cast, y = model_2_embedding_8_embedding_lookup_cast);
            tensor&lt;string, []&gt; user_hist_enc_to_fp16_dtype_0 = const()[name = tensor&lt;string, []&gt;("user_hist_enc_to_fp16_dtype_0"), val = tensor&lt;string, []&gt;("fp16")];
            tensor&lt;fp16, [?, 128]&gt; cast_27 = cast(dtype = user_hist_enc_to_fp16_dtype_0, x = user_hist_enc);
            tensor&lt;fp16, [?, 128]&gt; model_2_tf_math_multiply_4_Mul_cast = mul(x = cast_27, y = model_2_tf___operators___add_9_AddV2_cast);
            tensor&lt;fp16, [128, 128]&gt; transpose_5_cast_to_fp32_to_fp16 = const()[name = tensor&lt;string, []&gt;("transpose_5_cast_to_fp32_to_fp16"), val = tensor&lt;fp16, [128, 128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4338048)))];
            tensor&lt;fp16, [128]&gt; model_2_dense_15_BiasAdd_bias_0_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_dense_15_BiasAdd_bias_0_to_fp16"), val = tensor&lt;fp16, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4370880)))];
            tensor&lt;fp16, [?, 128]&gt; model_2_dense_15_BiasAdd_cast = linear(bias = model_2_dense_15_BiasAdd_bias_0_to_fp16, weight = transpose_5_cast_to_fp32_to_fp16, x = model_2_tf_math_multiply_4_Mul_cast);
            tensor&lt;string, []&gt; model_2_dense_15_Gelu_mul_1_mode_0 = const()[name = tensor&lt;string, []&gt;("model_2_dense_15_Gelu_mul_1_mode_0"), val = tensor&lt;string, []&gt;("EXACT")];
            tensor&lt;fp16, [?, 128]&gt; model_2_dense_15_Gelu_mul_1_cast = gelu(mode = model_2_dense_15_Gelu_mul_1_mode_0, x = model_2_dense_15_BiasAdd_cast);
            tensor&lt;fp16, [128, 128]&gt; transpose_6_cast_to_fp32_to_fp16 = const()[name = tensor&lt;string, []&gt;("transpose_6_cast_to_fp32_to_fp16"), val = tensor&lt;fp16, [128, 128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4371200)))];
            tensor&lt;fp16, [128]&gt; model_2_dense_16_BiasAdd_bias_0_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_dense_16_BiasAdd_bias_0_to_fp16"), val = tensor&lt;fp16, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4404032)))];
            tensor&lt;fp16, [?, 128]&gt; model_2_dense_16_BiasAdd_cast = linear(bias = model_2_dense_16_BiasAdd_bias_0_to_fp16, weight = transpose_6_cast_to_fp32_to_fp16, x = model_2_dense_15_Gelu_mul_1_cast);
            tensor&lt;string, []&gt; model_2_dense_16_Gelu_mul_1_mode_0 = const()[name = tensor&lt;string, []&gt;("model_2_dense_16_Gelu_mul_1_mode_0"), val = tensor&lt;string, []&gt;("EXACT")];
            tensor&lt;fp16, [?, 128]&gt; model_2_dense_16_Gelu_mul_1_cast = gelu(mode = model_2_dense_16_Gelu_mul_1_mode_0, x = model_2_dense_16_BiasAdd_cast);
            tensor&lt;fp16, [1, 128]&gt; transpose_7_cast_to_fp32_to_fp16 = const()[name = tensor&lt;string, []&gt;("transpose_7_cast_to_fp32_to_fp16"), val = tensor&lt;fp16, [1, 128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(4404352)))];
            tensor&lt;fp16, [1]&gt; model_2_dense_17_BiasAdd_bias_0_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_dense_17_BiasAdd_bias_0_to_fp16"), val = tensor&lt;fp16, [1]&gt;([-0x1.468p-1])];
            tensor&lt;fp16, [?, 1]&gt; model_2_dense_17_BiasAdd_cast = linear(bias = model_2_dense_17_BiasAdd_bias_0_to_fp16, weight = transpose_7_cast_to_fp32_to_fp16, x = model_2_dense_16_Gelu_mul_1_cast);
            tensor&lt;fp16, [?, 1]&gt; model_2_label_Sigmoid_cast = sigmoid(x = model_2_dense_17_BiasAdd_cast);
            tensor&lt;string, []&gt; model_2_label_Sigmoid_cast_to_fp32_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_2_label_Sigmoid_cast_to_fp32_dtype_0"), val = tensor&lt;string, []&gt;("fp32")];
            tensor&lt;fp32, [?, 1]&gt; Identity = cast(dtype = model_2_label_Sigmoid_cast_to_fp32_dtype_0, x = model_2_label_Sigmoid_cast);
        } -&gt; (Identity);
}</code></pre>
                    </div>
                </div>
            
                <div class="file-section" id="file-21">
                    <div class="file-header">
                        <h2>ðŸ“„ model.mil 18</h2>
                        <div class="file-meta">Size: 12.69 KB</div>
                    </div>
                    <div class="file-content">
                        <pre><code>program(1.0)
[buildInfo = dict&lt;tensor&lt;string, []&gt;, tensor&lt;string, []&gt;&gt;({{"coremlc-component-MIL", "3404.16.1"}, {"coremlc-version", "3404.24.1"}})]
{
    func main&lt;ios16&gt;(tensor&lt;fp32, [?, 128]&gt; input_1, tensor&lt;fp16, [?, 1, 768]&gt; input_2, tensor&lt;int32, [?, 1]&gt; input_3) [FlexibleShapeInformation = tuple&lt;tuple&lt;tensor&lt;string, []&gt;, dict&lt;tensor&lt;string, []&gt;, tensor&lt;int32, [?]&gt;&gt;&gt;, tuple&lt;tensor&lt;string, []&gt;, dict&lt;tensor&lt;string, []&gt;, list&lt;tensor&lt;int32, [2]&gt;, ?&gt;&gt;&gt;&gt;((("DefaultShapes", {{"input_1", [1, 128]}, {"input_2", [1, 1, 768]}, {"input_3", [1, 1]}}), ("RangeDims", {{"input_1", [[1, -1], [128, 128]]}, {"input_2", [[1, -1], [1, 1], [768, 768]]}, {"input_3", [[1, -1], [1, 1]]}})))] {
            tensor&lt;int32, []&gt; model_private_cp_mul_y = const()[name = tensor&lt;string, []&gt;("model_private_cp_mul_y"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, []&gt; model_private_cp_add_y = const()[name = tensor&lt;string, []&gt;("model_private_cp_add_y"), val = tensor&lt;int32, []&gt;(1)];
            tensor&lt;int32, [1]&gt; model_private_cp_model_2_Squeeze_axes_0 = const()[name = tensor&lt;string, []&gt;("model_private_cp_model_2_Squeeze_axes_0"), val = tensor&lt;int32, [1]&gt;([-1])];
            tensor&lt;int32, [?]&gt; model_private_cp_model_2_Squeeze = squeeze(axes = model_private_cp_model_2_Squeeze_axes_0, x = input_3);
            tensor&lt;int32, [?, 1]&gt; model_private_cp_mul = mul(x = input_3, y = model_private_cp_mul_y);
            tensor&lt;int32, []&gt; model_private_cp_model_2_embedding_3_embedding_lookup_axis_0 = const()[name = tensor&lt;string, []&gt;("model_private_cp_model_2_embedding_3_embedding_lookup_axis_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, []&gt; model_private_cp_model_2_embedding_3_embedding_lookup_batch_dims_0 = const()[name = tensor&lt;string, []&gt;("model_private_cp_model_2_embedding_3_embedding_lookup_batch_dims_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;fp16, [35, 128]&gt; model_private_cp_model_2_embedding_3_embedding_lookup_8570_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_private_cp_model_2_embedding_3_embedding_lookup_8570_to_fp16"), val = tensor&lt;fp16, [35, 128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(64)))];
            tensor&lt;fp16, [?, 128]&gt; model_private_cp_model_2_embedding_3_embedding_lookup_cast = gather(axis = model_private_cp_model_2_embedding_3_embedding_lookup_axis_0, batch_dims = model_private_cp_model_2_embedding_3_embedding_lookup_batch_dims_0, indices = model_private_cp_model_2_Squeeze, x = model_private_cp_model_2_embedding_3_embedding_lookup_8570_to_fp16);
            tensor&lt;int32, [?, 1]&gt; model_private_cp_add = add(x = model_private_cp_mul, y = model_private_cp_add_y);
            tensor&lt;int32, [1]&gt; model_private_cp_model_2_tf_compat_v1_squeeze_1_Squeeze_axes_0 = const()[name = tensor&lt;string, []&gt;("model_private_cp_model_2_tf_compat_v1_squeeze_1_Squeeze_axes_0"), val = tensor&lt;int32, [1]&gt;([-2])];
            tensor&lt;fp16, [?, 768]&gt; model_private_cp_model_2_tf_compat_v1_squeeze_1_Squeeze = squeeze(axes = model_private_cp_model_2_tf_compat_v1_squeeze_1_Squeeze_axes_0, x = input_2);
            tensor&lt;int32, [1]&gt; model_private_cp_model_2_tf_compat_v1_squeeze_2_Squeeze_axes_0 = const()[name = tensor&lt;string, []&gt;("model_private_cp_model_2_tf_compat_v1_squeeze_2_Squeeze_axes_0"), val = tensor&lt;int32, [1]&gt;([-1])];
            tensor&lt;int32, [?]&gt; model_private_cp_model_2_tf_compat_v1_squeeze_2_Squeeze = squeeze(axes = model_private_cp_model_2_tf_compat_v1_squeeze_2_Squeeze_axes_0, x = model_private_cp_add);
            tensor&lt;int32, []&gt; model_private_cp_model_2_embedding_4_embedding_lookup_axis_0 = const()[name = tensor&lt;string, []&gt;("model_private_cp_model_2_embedding_4_embedding_lookup_axis_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, []&gt; model_private_cp_model_2_embedding_4_embedding_lookup_batch_dims_0 = const()[name = tensor&lt;string, []&gt;("model_private_cp_model_2_embedding_4_embedding_lookup_batch_dims_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;fp16, [6, 128]&gt; model_private_cp_model_2_embedding_4_embedding_lookup_8577_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_private_cp_model_2_embedding_4_embedding_lookup_8577_to_fp16"), val = tensor&lt;fp16, [6, 128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(9088)))];
            tensor&lt;fp16, [?, 128]&gt; model_private_cp_model_2_embedding_4_embedding_lookup_cast = gather(axis = model_private_cp_model_2_embedding_4_embedding_lookup_axis_0, batch_dims = model_private_cp_model_2_embedding_4_embedding_lookup_batch_dims_0, indices = model_private_cp_model_2_tf_compat_v1_squeeze_2_Squeeze, x = model_private_cp_model_2_embedding_4_embedding_lookup_8577_to_fp16);
            tensor&lt;int32, []&gt; model_private_cp_model_2_embedding_5_embedding_lookup_axis_0 = const()[name = tensor&lt;string, []&gt;("model_private_cp_model_2_embedding_5_embedding_lookup_axis_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, []&gt; model_private_cp_model_2_embedding_5_embedding_lookup_batch_dims_0 = const()[name = tensor&lt;string, []&gt;("model_private_cp_model_2_embedding_5_embedding_lookup_batch_dims_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;fp16, [9, 128]&gt; model_private_cp_model_2_embedding_5_embedding_lookup_8583_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_private_cp_model_2_embedding_5_embedding_lookup_8583_to_fp16"), val = tensor&lt;fp16, [9, 128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(10688)))];
            tensor&lt;fp16, [?, 128]&gt; model_private_cp_model_2_embedding_5_embedding_lookup_cast = gather(axis = model_private_cp_model_2_embedding_5_embedding_lookup_axis_0, batch_dims = model_private_cp_model_2_embedding_5_embedding_lookup_batch_dims_0, indices = model_private_cp_model_2_tf_compat_v1_squeeze_2_Squeeze, x = model_private_cp_model_2_embedding_5_embedding_lookup_8583_to_fp16);
            tensor&lt;fp16, [128, 768]&gt; transpose_0_cast_to_fp32_to_fp16 = const()[name = tensor&lt;string, []&gt;("transpose_0_cast_to_fp32_to_fp16"), val = tensor&lt;fp16, [128, 768]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(13056)))];
            tensor&lt;fp16, [128]&gt; model_private_cp_model_2_dense_6_BiasAdd_bias_0_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_private_cp_model_2_dense_6_BiasAdd_bias_0_to_fp16"), val = tensor&lt;fp16, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(209728)))];
            tensor&lt;fp16, [?, 128]&gt; model_private_cp_model_2_dense_6_BiasAdd_cast = linear(bias = model_private_cp_model_2_dense_6_BiasAdd_bias_0_to_fp16, weight = transpose_0_cast_to_fp32_to_fp16, x = model_private_cp_model_2_tf_compat_v1_squeeze_1_Squeeze);
            tensor&lt;string, []&gt; model_private_cp_model_2_dense_6_Gelu_mul_1_mode_0 = const()[name = tensor&lt;string, []&gt;("model_private_cp_model_2_dense_6_Gelu_mul_1_mode_0"), val = tensor&lt;string, []&gt;("EXACT")];
            tensor&lt;fp16, [?, 128]&gt; model_private_cp_model_2_dense_6_Gelu_mul_1_cast = gelu(mode = model_private_cp_model_2_dense_6_Gelu_mul_1_mode_0, x = model_private_cp_model_2_dense_6_BiasAdd_cast);
            tensor&lt;fp16, [128, 128]&gt; transpose_1_cast_to_fp32_to_fp16 = const()[name = tensor&lt;string, []&gt;("transpose_1_cast_to_fp32_to_fp16"), val = tensor&lt;fp16, [128, 128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(210048)))];
            tensor&lt;fp16, [128]&gt; model_private_cp_model_2_dense_7_BiasAdd_bias_0_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_private_cp_model_2_dense_7_BiasAdd_bias_0_to_fp16"), val = tensor&lt;fp16, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(242880)))];
            tensor&lt;fp16, [?, 128]&gt; model_private_cp_model_2_dense_7_BiasAdd_cast = linear(bias = model_private_cp_model_2_dense_7_BiasAdd_bias_0_to_fp16, weight = transpose_1_cast_to_fp32_to_fp16, x = model_private_cp_model_2_dense_6_Gelu_mul_1_cast);
            tensor&lt;fp16, [?, 128]&gt; model_private_cp_model_2_tf___operators___add_1_AddV2_cast = add(x = model_private_cp_model_2_dense_7_BiasAdd_cast, y = model_private_cp_model_2_embedding_3_embedding_lookup_cast);
            tensor&lt;fp16, [?, 128]&gt; model_private_cp_model_2_tf___operators___add_2_AddV2_cast = add(x = model_private_cp_model_2_tf___operators___add_1_AddV2_cast, y = model_private_cp_model_2_embedding_4_embedding_lookup_cast);
            tensor&lt;fp16, [?, 128]&gt; model_private_cp_model_2_tf___operators___add_3_AddV2_cast = add(x = model_private_cp_model_2_tf___operators___add_2_AddV2_cast, y = model_private_cp_model_2_embedding_5_embedding_lookup_cast);
            tensor&lt;string, []&gt; input_1_to_fp16_dtype_0 = const()[name = tensor&lt;string, []&gt;("input_1_to_fp16_dtype_0"), val = tensor&lt;string, []&gt;("fp16")];
            tensor&lt;fp16, [?, 128]&gt; cast_13 = cast(dtype = input_1_to_fp16_dtype_0, x = input_1);
            tensor&lt;fp16, [?, 128]&gt; model_private_cp_model_2_tf_math_multiply_1_Mul_cast = mul(x = cast_13, y = model_private_cp_model_2_tf___operators___add_3_AddV2_cast);
            tensor&lt;fp16, [128, 128]&gt; transpose_2_cast_to_fp32_to_fp16 = const()[name = tensor&lt;string, []&gt;("transpose_2_cast_to_fp32_to_fp16"), val = tensor&lt;fp16, [128, 128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(243200)))];
            tensor&lt;fp16, [128]&gt; model_private_cp_model_2_dense_8_BiasAdd_bias_0_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_private_cp_model_2_dense_8_BiasAdd_bias_0_to_fp16"), val = tensor&lt;fp16, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(276032)))];
            tensor&lt;fp16, [?, 128]&gt; model_private_cp_model_2_dense_8_BiasAdd_cast = linear(bias = model_private_cp_model_2_dense_8_BiasAdd_bias_0_to_fp16, weight = transpose_2_cast_to_fp32_to_fp16, x = model_private_cp_model_2_tf_math_multiply_1_Mul_cast);
            tensor&lt;string, []&gt; model_private_cp_model_2_dense_8_Gelu_mul_1_mode_0 = const()[name = tensor&lt;string, []&gt;("model_private_cp_model_2_dense_8_Gelu_mul_1_mode_0"), val = tensor&lt;string, []&gt;("EXACT")];
            tensor&lt;fp16, [?, 128]&gt; model_private_cp_model_2_dense_8_Gelu_mul_1_cast = gelu(mode = model_private_cp_model_2_dense_8_Gelu_mul_1_mode_0, x = model_private_cp_model_2_dense_8_BiasAdd_cast);
            tensor&lt;fp16, [128, 128]&gt; transpose_3_cast_to_fp32_to_fp16 = const()[name = tensor&lt;string, []&gt;("transpose_3_cast_to_fp32_to_fp16"), val = tensor&lt;fp16, [128, 128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(276352)))];
            tensor&lt;fp16, [128]&gt; model_private_cp_model_2_dense_9_BiasAdd_bias_0_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_private_cp_model_2_dense_9_BiasAdd_bias_0_to_fp16"), val = tensor&lt;fp16, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(309184)))];
            tensor&lt;fp16, [?, 128]&gt; model_private_cp_model_2_dense_9_BiasAdd_cast = linear(bias = model_private_cp_model_2_dense_9_BiasAdd_bias_0_to_fp16, weight = transpose_3_cast_to_fp32_to_fp16, x = model_private_cp_model_2_dense_8_Gelu_mul_1_cast);
            tensor&lt;string, []&gt; model_private_cp_model_2_dense_9_Gelu_mul_1_mode_0 = const()[name = tensor&lt;string, []&gt;("model_private_cp_model_2_dense_9_Gelu_mul_1_mode_0"), val = tensor&lt;string, []&gt;("EXACT")];
            tensor&lt;fp16, [?, 128]&gt; model_private_cp_model_2_dense_9_Gelu_mul_1_cast = gelu(mode = model_private_cp_model_2_dense_9_Gelu_mul_1_mode_0, x = model_private_cp_model_2_dense_9_BiasAdd_cast);
            tensor&lt;fp16, [1, 128]&gt; transpose_4_cast_to_fp32_to_fp16 = const()[name = tensor&lt;string, []&gt;("transpose_4_cast_to_fp32_to_fp16"), val = tensor&lt;fp16, [1, 128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(309504)))];
            tensor&lt;fp16, [1]&gt; model_private_cp_model_2_dense_10_BiasAdd_bias_0_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_private_cp_model_2_dense_10_BiasAdd_bias_0_to_fp16"), val = tensor&lt;fp16, [1]&gt;([-0x1.ae4p-5])];
            tensor&lt;fp16, [?, 1]&gt; model_private_cp_model_2_dense_10_BiasAdd_cast = linear(bias = model_private_cp_model_2_dense_10_BiasAdd_bias_0_to_fp16, weight = transpose_4_cast_to_fp32_to_fp16, x = model_private_cp_model_2_dense_9_Gelu_mul_1_cast);
            tensor&lt;fp16, [?, 1]&gt; model_private_cp_model_2_label_Sigmoid_cast = sigmoid(x = model_private_cp_model_2_dense_10_BiasAdd_cast);
            tensor&lt;string, []&gt; model_private_cp_model_2_label_Sigmoid_cast_to_fp32_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_private_cp_model_2_label_Sigmoid_cast_to_fp32_dtype_0"), val = tensor&lt;string, []&gt;("fp32")];
            tensor&lt;fp32, [?, 1]&gt; Identity = cast(dtype = model_private_cp_model_2_label_Sigmoid_cast_to_fp32_dtype_0, x = model_private_cp_model_2_label_Sigmoid_cast);
        } -&gt; (Identity);
}</code></pre>
                    </div>
                </div>
            
                <div class="file-section" id="file-22">
                    <div class="file-header">
                        <h2>ðŸ“„ model.mil 8</h2>
                        <div class="file-meta">Size: 61.64 KB</div>
                    </div>
                    <div class="file-content">
                        <pre><code>program(1.0)
[buildInfo = dict&lt;tensor&lt;string, []&gt;, tensor&lt;string, []&gt;&gt;({{"coremlc-component-MIL", "3404.16.1"}, {"coremlc-version", "3404.24.1"}})]
{
    func main&lt;ios16&gt;(tensor&lt;fp16, [?, 64, 1, 768]&gt; hist_article_encoding, tensor&lt;int32, [?, 64]&gt; hist_channel_id) [FlexibleShapeInformation = tuple&lt;tuple&lt;tensor&lt;string, []&gt;, dict&lt;tensor&lt;string, []&gt;, tensor&lt;int32, [?]&gt;&gt;&gt;, tuple&lt;tensor&lt;string, []&gt;, dict&lt;tensor&lt;string, []&gt;, list&lt;tensor&lt;int32, [2]&gt;, ?&gt;&gt;&gt;&gt;((("DefaultShapes", {{"hist_article_encoding", [1, 64, 1, 768]}, {"hist_channel_id", [1, 64]}}), ("RangeDims", {{"hist_article_encoding", [[1, -1], [64, 64], [1, 1], [768, 768]]}, {"hist_channel_id", [[1, -1], [64, 64]]}})))] {
            tensor&lt;int32, [1]&gt; model_1_tf_compat_v1_squeeze_Squeeze_axes_0 = const()[name = tensor&lt;string, []&gt;("model_1_tf_compat_v1_squeeze_Squeeze_axes_0"), val = tensor&lt;int32, [1]&gt;([-2])];
            tensor&lt;fp16, [?, 64, 768]&gt; model_1_tf_compat_v1_squeeze_Squeeze_cast = squeeze(axes = model_1_tf_compat_v1_squeeze_Squeeze_axes_0, x = hist_article_encoding);
            tensor&lt;int32, []&gt; model_1_embedding_1_embedding_lookup_axis_0 = const()[name = tensor&lt;string, []&gt;("model_1_embedding_1_embedding_lookup_axis_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;int32, []&gt; model_1_embedding_1_embedding_lookup_batch_dims_0 = const()[name = tensor&lt;string, []&gt;("model_1_embedding_1_embedding_lookup_batch_dims_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;fp16, [35, 128]&gt; model_1_embedding_1_embedding_lookup_5400_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_embedding_1_embedding_lookup_5400_to_fp16"), val = tensor&lt;fp16, [35, 128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(64)))];
            tensor&lt;fp16, [?, 64, 128]&gt; model_1_embedding_1_embedding_lookup_cast = gather(axis = model_1_embedding_1_embedding_lookup_axis_0, batch_dims = model_1_embedding_1_embedding_lookup_batch_dims_0, indices = hist_channel_id, x = model_1_embedding_1_embedding_lookup_5400_to_fp16);
            tensor&lt;int32, [2]&gt; model_1_dense_Tensordot_stackx = const()[name = tensor&lt;string, []&gt;("model_1_dense_Tensordot_stackx"), val = tensor&lt;int32, [2]&gt;([-1, 768])];
            tensor&lt;fp16, [?, 768]&gt; model_1_dense_Tensordot_Reshape_cast = reshape(shape = model_1_dense_Tensordot_stackx, x = model_1_tf_compat_v1_squeeze_Squeeze_cast);
            tensor&lt;bool, []&gt; model_1_dense_Tensordot_MatMul_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("model_1_dense_Tensordot_MatMul_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; model_1_dense_Tensordot_MatMul_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("model_1_dense_Tensordot_MatMul_transpose_y_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [768, 128]&gt; model_1_dense_Tensordot_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_dense_Tensordot_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [768, 128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(9088)))];
            tensor&lt;fp16, [?, 128]&gt; model_1_dense_Tensordot_MatMul_cast = matmul(transpose_x = model_1_dense_Tensordot_MatMul_transpose_x_0, transpose_y = model_1_dense_Tensordot_MatMul_transpose_y_0, x = model_1_dense_Tensordot_Reshape_cast, y = model_1_dense_Tensordot_ReadVariableOp_to_fp16);
            tensor&lt;int32, [3]&gt; model_1_dense_Tensordot_concat_1x = const()[name = tensor&lt;string, []&gt;("model_1_dense_Tensordot_concat_1x"), val = tensor&lt;int32, [3]&gt;([-1, 64, 128])];
            tensor&lt;fp16, [?, 64, 128]&gt; model_1_dense_Tensordot_cast = reshape(shape = model_1_dense_Tensordot_concat_1x, x = model_1_dense_Tensordot_MatMul_cast);
            tensor&lt;fp16, [128]&gt; model_1_dense_BiasAdd_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_dense_BiasAdd_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(205760)))];
            tensor&lt;fp16, [?, 64, 128]&gt; model_1_dense_BiasAdd_cast = add(x = model_1_dense_Tensordot_cast, y = model_1_dense_BiasAdd_ReadVariableOp_to_fp16);
            tensor&lt;string, []&gt; model_1_dense_Gelu_mul_1_mode_0 = const()[name = tensor&lt;string, []&gt;("model_1_dense_Gelu_mul_1_mode_0"), val = tensor&lt;string, []&gt;("EXACT")];
            tensor&lt;fp16, [?, 64, 128]&gt; model_1_dense_Gelu_mul_1_cast = gelu(mode = model_1_dense_Gelu_mul_1_mode_0, x = model_1_dense_BiasAdd_cast);
            tensor&lt;int32, [2]&gt; model_1_dense_1_Tensordot_stackx = const()[name = tensor&lt;string, []&gt;("model_1_dense_1_Tensordot_stackx"), val = tensor&lt;int32, [2]&gt;([-1, 128])];
            tensor&lt;fp16, [?, 128]&gt; model_1_dense_1_Tensordot_Reshape_cast = reshape(shape = model_1_dense_1_Tensordot_stackx, x = model_1_dense_Gelu_mul_1_cast);
            tensor&lt;bool, []&gt; model_1_dense_1_Tensordot_MatMul_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("model_1_dense_1_Tensordot_MatMul_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; model_1_dense_1_Tensordot_MatMul_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("model_1_dense_1_Tensordot_MatMul_transpose_y_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [128, 128]&gt; model_1_dense_1_Tensordot_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_dense_1_Tensordot_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [128, 128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(206080)))];
            tensor&lt;fp16, [?, 128]&gt; model_1_dense_1_Tensordot_MatMul_cast = matmul(transpose_x = model_1_dense_1_Tensordot_MatMul_transpose_x_0, transpose_y = model_1_dense_1_Tensordot_MatMul_transpose_y_0, x = model_1_dense_1_Tensordot_Reshape_cast, y = model_1_dense_1_Tensordot_ReadVariableOp_to_fp16);
            tensor&lt;int32, [3]&gt; model_1_dense_1_Tensordot_concat_1x = const()[name = tensor&lt;string, []&gt;("model_1_dense_1_Tensordot_concat_1x"), val = tensor&lt;int32, [3]&gt;([-1, 64, 128])];
            tensor&lt;fp16, [?, 64, 128]&gt; model_1_dense_1_Tensordot_cast = reshape(shape = model_1_dense_1_Tensordot_concat_1x, x = model_1_dense_1_Tensordot_MatMul_cast);
            tensor&lt;fp16, [128]&gt; model_1_dense_1_BiasAdd_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_dense_1_BiasAdd_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(238912)))];
            tensor&lt;fp16, [?, 64, 128]&gt; model_1_dense_1_BiasAdd_cast = add(x = model_1_dense_1_Tensordot_cast, y = model_1_dense_1_BiasAdd_ReadVariableOp_to_fp16);
            tensor&lt;fp16, [?, 64, 128]&gt; model_1_tf___operators___add_AddV2_cast = add(x = model_1_dense_1_BiasAdd_cast, y = model_1_embedding_1_embedding_lookup_cast);
            tensor&lt;fp16, [64, 128]&gt; model_1_p_emb_StatefulPartitionedCall_StatefulPartitionedCall_embedding_2_embedding_lookup_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_p_emb_StatefulPartitionedCall_StatefulPartitionedCall_embedding_2_embedding_lookup_to_fp16"), val = tensor&lt;fp16, [64, 128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(239232)))];
            tensor&lt;fp16, [?, 64, 128]&gt; model_1_p_emb_StatefulPartitionedCall_StatefulPartitionedCall_add_cast = add(x = model_1_p_emb_StatefulPartitionedCall_StatefulPartitionedCall_embedding_2_embedding_lookup_to_fp16, y = model_1_tf___operators___add_AddV2_cast);
            tensor&lt;int32, [1]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_batchnorm_add_1_axes_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_batchnorm_add_1_axes_0"), val = tensor&lt;int32, [1]&gt;([2])];
            tensor&lt;fp16, [128]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_batchnorm_mul_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_batchnorm_mul_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(255680)))];
            tensor&lt;fp16, [128]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_batchnorm_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_batchnorm_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(256000)))];
            tensor&lt;fp16, []&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_batchnorm_add_y_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_batchnorm_add_y_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1.1p-20)];
            tensor&lt;fp16, [?, 64, 128]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_batchnorm_add_1_cast = layer_norm(axes = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_batchnorm_add_1_axes_0, beta = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_batchnorm_ReadVariableOp_to_fp16, epsilon = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_batchnorm_add_y_to_fp16, gamma = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_batchnorm_mul_ReadVariableOp_to_fp16, x = model_1_p_emb_StatefulPartitionedCall_StatefulPartitionedCall_add_cast);
            tensor&lt;int32, [2]&gt; concat_3x = const()[name = tensor&lt;string, []&gt;("concat_3x"), val = tensor&lt;int32, [2]&gt;([-1, 128])];
            tensor&lt;fp16, [?, 128]&gt; reshape_0_cast = reshape(shape = concat_3x, x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_batchnorm_add_1_cast);
            tensor&lt;bool, []&gt; matmul_0_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("matmul_0_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; matmul_0_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("matmul_0_transpose_y_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [128, 256]&gt; reshape_1_to_fp16 = const()[name = tensor&lt;string, []&gt;("reshape_1_to_fp16"), val = tensor&lt;fp16, [128, 256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(256320)))];
            tensor&lt;fp16, [?, 256]&gt; matmul_0_cast = matmul(transpose_x = matmul_0_transpose_x_0, transpose_y = matmul_0_transpose_y_0, x = reshape_0_cast, y = reshape_1_to_fp16);
            tensor&lt;int32, [4]&gt; concat_7x = const()[name = tensor&lt;string, []&gt;("concat_7x"), val = tensor&lt;int32, [4]&gt;([-1, 64, 4, 64])];
            tensor&lt;fp16, [?, 64, 4, 64]&gt; reshape_2_cast = reshape(shape = concat_7x, x = matmul_0_cast);
            tensor&lt;bool, []&gt; matmul_1_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("matmul_1_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; matmul_1_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("matmul_1_transpose_y_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [128, 256]&gt; reshape_4_to_fp16 = const()[name = tensor&lt;string, []&gt;("reshape_4_to_fp16"), val = tensor&lt;fp16, [128, 256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(321920)))];
            tensor&lt;fp16, [?, 256]&gt; matmul_1_cast = matmul(transpose_x = matmul_1_transpose_x_0, transpose_y = matmul_1_transpose_y_0, x = reshape_0_cast, y = reshape_4_to_fp16);
            tensor&lt;int32, [4]&gt; concat_15x = const()[name = tensor&lt;string, []&gt;("concat_15x"), val = tensor&lt;int32, [4]&gt;([-1, 64, 4, 64])];
            tensor&lt;fp16, [?, 64, 4, 64]&gt; reshape_5_cast = reshape(shape = concat_15x, x = matmul_1_cast);
            tensor&lt;bool, []&gt; matmul_2_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("matmul_2_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; matmul_2_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("matmul_2_transpose_y_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [128, 256]&gt; reshape_7_to_fp16 = const()[name = tensor&lt;string, []&gt;("reshape_7_to_fp16"), val = tensor&lt;fp16, [128, 256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(387520)))];
            tensor&lt;fp16, [?, 256]&gt; matmul_2_cast = matmul(transpose_x = matmul_2_transpose_x_0, transpose_y = matmul_2_transpose_y_0, x = reshape_0_cast, y = reshape_7_to_fp16);
            tensor&lt;int32, [4]&gt; concat_23x = const()[name = tensor&lt;string, []&gt;("concat_23x"), val = tensor&lt;int32, [4]&gt;([-1, 64, 4, 64])];
            tensor&lt;fp16, [?, 64, 4, 64]&gt; reshape_8_cast = reshape(shape = concat_23x, x = matmul_2_cast);
            tensor&lt;fp16, [4, 64]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_key_add_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_key_add_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [4, 64]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(453120)))];
            tensor&lt;fp16, [?, 64, 4, 64]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_key_add_cast = add(x = reshape_2_cast, y = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_key_add_ReadVariableOp_to_fp16);
            tensor&lt;fp16, [4, 64]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_query_add_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_query_add_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [4, 64]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(453696)))];
            tensor&lt;fp16, [?, 64, 4, 64]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_query_add_cast = add(x = reshape_5_cast, y = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_query_add_ReadVariableOp_to_fp16);
            tensor&lt;fp16, [4, 64]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_value_add_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_value_add_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [4, 64]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(454272)))];
            tensor&lt;fp16, [?, 64, 4, 64]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_value_add_cast = add(x = reshape_8_cast, y = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_value_add_ReadVariableOp_to_fp16);
            tensor&lt;fp16, []&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_Mul_y_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_Mul_y_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1p-3)];
            tensor&lt;fp16, [?, 64, 4, 64]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_Mul_cast = mul(x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_query_add_cast, y = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_Mul_y_to_fp16);
            tensor&lt;bool, []&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_einsum_Einsum_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_einsum_Einsum_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_einsum_Einsum_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_einsum_Einsum_transpose_y_0"), val = tensor&lt;bool, []&gt;(true)];
            tensor&lt;int32, [4]&gt; transpose_26_perm_0 = const()[name = tensor&lt;string, []&gt;("transpose_26_perm_0"), val = tensor&lt;int32, [4]&gt;([0, 2, 1, 3])];
            tensor&lt;int32, [4]&gt; transpose_27_perm_0 = const()[name = tensor&lt;string, []&gt;("transpose_27_perm_0"), val = tensor&lt;int32, [4]&gt;([0, 2, 1, 3])];
            tensor&lt;fp16, [?, 4, 64, 64]&gt; transpose_36 = transpose(perm = transpose_27_perm_0, x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_key_add_cast);
            tensor&lt;fp16, [?, 4, 64, 64]&gt; transpose_37 = transpose(perm = transpose_26_perm_0, x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_Mul_cast);
            tensor&lt;fp16, [?, 4, 64, 64]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_einsum_Einsum_cast = matmul(transpose_x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_einsum_Einsum_transpose_x_0, transpose_y = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_einsum_Einsum_transpose_y_0, x = transpose_37, y = transpose_36);
            tensor&lt;int32, []&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_softmax_Softmax_axis_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_softmax_Softmax_axis_0"), val = tensor&lt;int32, []&gt;(-1)];
            tensor&lt;fp16, [?, 4, 64, 64]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_softmax_Softmax_cast = softmax(axis = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_softmax_Softmax_axis_0, x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_einsum_Einsum_cast);
            tensor&lt;int32, [4]&gt; transpose_8_perm_0 = const()[name = tensor&lt;string, []&gt;("transpose_8_perm_0"), val = tensor&lt;int32, [4]&gt;([0, 2, 1, 3])];
            tensor&lt;bool, []&gt; matmul_3_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("matmul_3_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; matmul_3_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("matmul_3_transpose_y_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [?, 4, 64, 64]&gt; transpose_35 = transpose(perm = transpose_8_perm_0, x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_value_add_cast);
            tensor&lt;fp16, [?, 4, 64, 64]&gt; matmul_3_cast = matmul(transpose_x = matmul_3_transpose_x_0, transpose_y = matmul_3_transpose_y_0, x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_softmax_Softmax_cast, y = transpose_35);
            tensor&lt;int32, [4]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_einsum_1_Einsum_perm_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_einsum_1_Einsum_perm_0"), val = tensor&lt;int32, [4]&gt;([0, 2, 1, 3])];
            tensor&lt;int32, [2]&gt; concat_27x = const()[name = tensor&lt;string, []&gt;("concat_27x"), val = tensor&lt;int32, [2]&gt;([-1, 256])];
            tensor&lt;fp16, [?, 64, 4, 64]&gt; transpose_34 = transpose(perm = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_einsum_1_Einsum_perm_0, x = matmul_3_cast);
            tensor&lt;fp16, [?, 256]&gt; reshape_9_cast = reshape(shape = concat_27x, x = transpose_34);
            tensor&lt;bool, []&gt; matmul_4_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("matmul_4_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; matmul_4_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("matmul_4_transpose_y_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [256, 128]&gt; reshape_10_to_fp16 = const()[name = tensor&lt;string, []&gt;("reshape_10_to_fp16"), val = tensor&lt;fp16, [256, 128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(454848)))];
            tensor&lt;fp16, [?, 128]&gt; matmul_4_cast = matmul(transpose_x = matmul_4_transpose_x_0, transpose_y = matmul_4_transpose_y_0, x = reshape_9_cast, y = reshape_10_to_fp16);
            tensor&lt;int32, [3]&gt; concat_31x = const()[name = tensor&lt;string, []&gt;("concat_31x"), val = tensor&lt;int32, [3]&gt;([-1, 64, 128])];
            tensor&lt;fp16, [?, 64, 128]&gt; reshape_11_cast = reshape(shape = concat_31x, x = matmul_4_cast);
            tensor&lt;fp16, [128]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_attention_output_add_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_attention_output_add_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(520448)))];
            tensor&lt;fp16, [?, 64, 128]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_attention_output_add_cast = add(x = reshape_11_cast, y = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_attention_output_add_ReadVariableOp_to_fp16);
            tensor&lt;fp16, [?, 64, 128]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_add_cast = add(x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_batchnorm_add_1_cast, y = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_attention_output_add_cast);
            tensor&lt;int32, [1]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_1_batchnorm_add_1_axes_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_1_batchnorm_add_1_axes_0"), val = tensor&lt;int32, [1]&gt;([2])];
            tensor&lt;fp16, [128]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_1_batchnorm_mul_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_1_batchnorm_mul_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(520768)))];
            tensor&lt;fp16, [128]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_1_batchnorm_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_1_batchnorm_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(521088)))];
            tensor&lt;fp16, []&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_1_batchnorm_add_y_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_1_batchnorm_add_y_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1.1p-20)];
            tensor&lt;fp16, [?, 64, 128]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_1_batchnorm_add_1_cast = layer_norm(axes = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_1_batchnorm_add_1_axes_0, beta = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_1_batchnorm_ReadVariableOp_to_fp16, epsilon = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_1_batchnorm_add_y_to_fp16, gamma = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_1_batchnorm_mul_ReadVariableOp_to_fp16, x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_add_cast);
            tensor&lt;int32, [2]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Tensordot_stackx = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Tensordot_stackx"), val = tensor&lt;int32, [2]&gt;([-1, 128])];
            tensor&lt;fp16, [?, 128]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Tensordot_Reshape_cast = reshape(shape = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Tensordot_stackx, x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_1_batchnorm_add_1_cast);
            tensor&lt;bool, []&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Tensordot_MatMul_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Tensordot_MatMul_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Tensordot_MatMul_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Tensordot_MatMul_transpose_y_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [128, 256]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Tensordot_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Tensordot_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [128, 256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(521408)))];
            tensor&lt;fp16, [?, 256]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Tensordot_MatMul_cast = matmul(transpose_x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Tensordot_MatMul_transpose_x_0, transpose_y = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Tensordot_MatMul_transpose_y_0, x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Tensordot_Reshape_cast, y = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Tensordot_ReadVariableOp_to_fp16);
            tensor&lt;int32, [3]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Tensordot_concat_1x = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Tensordot_concat_1x"), val = tensor&lt;int32, [3]&gt;([-1, 64, 256])];
            tensor&lt;fp16, [?, 64, 256]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Tensordot_cast = reshape(shape = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Tensordot_concat_1x, x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Tensordot_MatMul_cast);
            tensor&lt;fp16, [256]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_BiasAdd_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_BiasAdd_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(587008)))];
            tensor&lt;fp16, [?, 64, 256]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_BiasAdd_cast = add(x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Tensordot_cast, y = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_BiasAdd_ReadVariableOp_to_fp16);
            tensor&lt;string, []&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Gelu_mul_1_mode_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Gelu_mul_1_mode_0"), val = tensor&lt;string, []&gt;("EXACT")];
            tensor&lt;fp16, [?, 64, 256]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Gelu_mul_1_cast = gelu(mode = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Gelu_mul_1_mode_0, x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_BiasAdd_cast);
            tensor&lt;int32, [2]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_Tensordot_stackx = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_Tensordot_stackx"), val = tensor&lt;int32, [2]&gt;([-1, 256])];
            tensor&lt;fp16, [?, 256]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_Tensordot_Reshape_cast = reshape(shape = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_Tensordot_stackx, x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_2_Gelu_mul_1_cast);
            tensor&lt;bool, []&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_Tensordot_MatMul_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_Tensordot_MatMul_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_Tensordot_MatMul_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_Tensordot_MatMul_transpose_y_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [256, 128]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_Tensordot_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_Tensordot_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [256, 128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(587584)))];
            tensor&lt;fp16, [?, 128]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_Tensordot_MatMul_cast = matmul(transpose_x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_Tensordot_MatMul_transpose_x_0, transpose_y = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_Tensordot_MatMul_transpose_y_0, x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_Tensordot_Reshape_cast, y = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_Tensordot_ReadVariableOp_to_fp16);
            tensor&lt;int32, [3]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_Tensordot_concat_1x = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_Tensordot_concat_1x"), val = tensor&lt;int32, [3]&gt;([-1, 64, 128])];
            tensor&lt;fp16, [?, 64, 128]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_Tensordot_cast = reshape(shape = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_Tensordot_concat_1x, x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_Tensordot_MatMul_cast);
            tensor&lt;fp16, [128]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_BiasAdd_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_BiasAdd_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(653184)))];
            tensor&lt;fp16, [?, 64, 128]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_BiasAdd_cast = add(x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_Tensordot_cast, y = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_BiasAdd_ReadVariableOp_to_fp16);
            tensor&lt;fp16, [?, 64, 128]&gt; model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_add_1_cast = add(x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_1_batchnorm_add_1_cast, y = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_sequential_dense_3_BiasAdd_cast);
            tensor&lt;int32, [1]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_2_batchnorm_add_1_axes_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_2_batchnorm_add_1_axes_0"), val = tensor&lt;int32, [1]&gt;([2])];
            tensor&lt;fp16, [128]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_2_batchnorm_mul_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_2_batchnorm_mul_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(653504)))];
            tensor&lt;fp16, [128]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_2_batchnorm_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_2_batchnorm_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(653824)))];
            tensor&lt;fp16, []&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_2_batchnorm_add_y_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_2_batchnorm_add_y_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1.1p-20)];
            tensor&lt;fp16, [?, 64, 128]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_2_batchnorm_add_1_cast = layer_norm(axes = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_2_batchnorm_add_1_axes_0, beta = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_2_batchnorm_ReadVariableOp_to_fp16, epsilon = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_2_batchnorm_add_y_to_fp16, gamma = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_2_batchnorm_mul_ReadVariableOp_to_fp16, x = model_1_transformer_block_StatefulPartitionedCall_StatefulPartitionedCall_add_1_cast);
            tensor&lt;int32, [2]&gt; concat_35x = const()[name = tensor&lt;string, []&gt;("concat_35x"), val = tensor&lt;int32, [2]&gt;([-1, 128])];
            tensor&lt;fp16, [?, 128]&gt; reshape_12_cast = reshape(shape = concat_35x, x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_2_batchnorm_add_1_cast);
            tensor&lt;bool, []&gt; matmul_5_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("matmul_5_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; matmul_5_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("matmul_5_transpose_y_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [128, 256]&gt; reshape_13_to_fp16 = const()[name = tensor&lt;string, []&gt;("reshape_13_to_fp16"), val = tensor&lt;fp16, [128, 256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(654144)))];
            tensor&lt;fp16, [?, 256]&gt; matmul_5_cast = matmul(transpose_x = matmul_5_transpose_x_0, transpose_y = matmul_5_transpose_y_0, x = reshape_12_cast, y = reshape_13_to_fp16);
            tensor&lt;int32, [4]&gt; concat_39x = const()[name = tensor&lt;string, []&gt;("concat_39x"), val = tensor&lt;int32, [4]&gt;([-1, 64, 4, 64])];
            tensor&lt;fp16, [?, 64, 4, 64]&gt; reshape_14_cast = reshape(shape = concat_39x, x = matmul_5_cast);
            tensor&lt;bool, []&gt; matmul_6_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("matmul_6_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; matmul_6_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("matmul_6_transpose_y_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [128, 256]&gt; reshape_16_to_fp16 = const()[name = tensor&lt;string, []&gt;("reshape_16_to_fp16"), val = tensor&lt;fp16, [128, 256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(719744)))];
            tensor&lt;fp16, [?, 256]&gt; matmul_6_cast = matmul(transpose_x = matmul_6_transpose_x_0, transpose_y = matmul_6_transpose_y_0, x = reshape_12_cast, y = reshape_16_to_fp16);
            tensor&lt;int32, [4]&gt; concat_47x = const()[name = tensor&lt;string, []&gt;("concat_47x"), val = tensor&lt;int32, [4]&gt;([-1, 64, 4, 64])];
            tensor&lt;fp16, [?, 64, 4, 64]&gt; reshape_17_cast = reshape(shape = concat_47x, x = matmul_6_cast);
            tensor&lt;bool, []&gt; matmul_7_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("matmul_7_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; matmul_7_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("matmul_7_transpose_y_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [128, 256]&gt; reshape_19_to_fp16 = const()[name = tensor&lt;string, []&gt;("reshape_19_to_fp16"), val = tensor&lt;fp16, [128, 256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(785344)))];
            tensor&lt;fp16, [?, 256]&gt; matmul_7_cast = matmul(transpose_x = matmul_7_transpose_x_0, transpose_y = matmul_7_transpose_y_0, x = reshape_12_cast, y = reshape_19_to_fp16);
            tensor&lt;int32, [4]&gt; concat_55x = const()[name = tensor&lt;string, []&gt;("concat_55x"), val = tensor&lt;int32, [4]&gt;([-1, 64, 4, 64])];
            tensor&lt;fp16, [?, 64, 4, 64]&gt; reshape_20_cast = reshape(shape = concat_55x, x = matmul_7_cast);
            tensor&lt;fp16, [4, 64]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_key_add_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_key_add_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [4, 64]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(850944)))];
            tensor&lt;fp16, [?, 64, 4, 64]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_key_add_cast = add(x = reshape_14_cast, y = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_key_add_ReadVariableOp_to_fp16);
            tensor&lt;fp16, [4, 64]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_query_add_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_query_add_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [4, 64]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(851520)))];
            tensor&lt;fp16, [?, 64, 4, 64]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_query_add_cast = add(x = reshape_17_cast, y = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_query_add_ReadVariableOp_to_fp16);
            tensor&lt;fp16, [4, 64]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_value_add_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_value_add_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [4, 64]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(852096)))];
            tensor&lt;fp16, [?, 64, 4, 64]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_value_add_cast = add(x = reshape_20_cast, y = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_value_add_ReadVariableOp_to_fp16);
            tensor&lt;fp16, []&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_Mul_y_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_Mul_y_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1p-3)];
            tensor&lt;fp16, [?, 64, 4, 64]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_Mul_cast = mul(x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_query_add_cast, y = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_Mul_y_to_fp16);
            tensor&lt;bool, []&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_einsum_Einsum_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_einsum_Einsum_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_einsum_Einsum_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_einsum_Einsum_transpose_y_0"), val = tensor&lt;bool, []&gt;(true)];
            tensor&lt;int32, [4]&gt; transpose_28_perm_0 = const()[name = tensor&lt;string, []&gt;("transpose_28_perm_0"), val = tensor&lt;int32, [4]&gt;([0, 2, 1, 3])];
            tensor&lt;int32, [4]&gt; transpose_29_perm_0 = const()[name = tensor&lt;string, []&gt;("transpose_29_perm_0"), val = tensor&lt;int32, [4]&gt;([0, 2, 1, 3])];
            tensor&lt;fp16, [?, 4, 64, 64]&gt; transpose_32 = transpose(perm = transpose_29_perm_0, x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_key_add_cast);
            tensor&lt;fp16, [?, 4, 64, 64]&gt; transpose_33 = transpose(perm = transpose_28_perm_0, x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_Mul_cast);
            tensor&lt;fp16, [?, 4, 64, 64]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_einsum_Einsum_cast = matmul(transpose_x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_einsum_Einsum_transpose_x_0, transpose_y = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_einsum_Einsum_transpose_y_0, x = transpose_33, y = transpose_32);
            tensor&lt;int32, []&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_softmax_Softmax_axis_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_softmax_Softmax_axis_0"), val = tensor&lt;int32, []&gt;(-1)];
            tensor&lt;fp16, [?, 4, 64, 64]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_softmax_Softmax_cast = softmax(axis = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_softmax_Softmax_axis_0, x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_einsum_Einsum_cast);
            tensor&lt;int32, [4]&gt; transpose_19_perm_0 = const()[name = tensor&lt;string, []&gt;("transpose_19_perm_0"), val = tensor&lt;int32, [4]&gt;([0, 2, 1, 3])];
            tensor&lt;bool, []&gt; matmul_8_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("matmul_8_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; matmul_8_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("matmul_8_transpose_y_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [?, 4, 64, 64]&gt; transpose_31 = transpose(perm = transpose_19_perm_0, x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_value_add_cast);
            tensor&lt;fp16, [?, 4, 64, 64]&gt; matmul_8_cast = matmul(transpose_x = matmul_8_transpose_x_0, transpose_y = matmul_8_transpose_y_0, x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_softmax_Softmax_cast, y = transpose_31);
            tensor&lt;int32, [4]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_einsum_1_Einsum_perm_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_einsum_1_Einsum_perm_0"), val = tensor&lt;int32, [4]&gt;([0, 2, 1, 3])];
            tensor&lt;int32, [2]&gt; concat_59x = const()[name = tensor&lt;string, []&gt;("concat_59x"), val = tensor&lt;int32, [2]&gt;([-1, 256])];
            tensor&lt;fp16, [?, 64, 4, 64]&gt; transpose_30 = transpose(perm = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_einsum_1_Einsum_perm_0, x = matmul_8_cast);
            tensor&lt;fp16, [?, 256]&gt; reshape_21_cast = reshape(shape = concat_59x, x = transpose_30);
            tensor&lt;bool, []&gt; matmul_9_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("matmul_9_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; matmul_9_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("matmul_9_transpose_y_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [256, 128]&gt; reshape_22_to_fp16 = const()[name = tensor&lt;string, []&gt;("reshape_22_to_fp16"), val = tensor&lt;fp16, [256, 128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(852672)))];
            tensor&lt;fp16, [?, 128]&gt; matmul_9_cast = matmul(transpose_x = matmul_9_transpose_x_0, transpose_y = matmul_9_transpose_y_0, x = reshape_21_cast, y = reshape_22_to_fp16);
            tensor&lt;int32, [3]&gt; concat_63x = const()[name = tensor&lt;string, []&gt;("concat_63x"), val = tensor&lt;int32, [3]&gt;([-1, 64, 128])];
            tensor&lt;fp16, [?, 64, 128]&gt; reshape_23_cast = reshape(shape = concat_63x, x = matmul_9_cast);
            tensor&lt;fp16, [128]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_attention_output_add_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_attention_output_add_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(918272)))];
            tensor&lt;fp16, [?, 64, 128]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_attention_output_add_cast = add(x = reshape_23_cast, y = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_attention_output_add_ReadVariableOp_to_fp16);
            tensor&lt;fp16, [?, 64, 128]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_add_cast = add(x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_2_batchnorm_add_1_cast, y = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_multi_head_attention_1_attention_output_add_cast);
            tensor&lt;int32, [1]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_3_batchnorm_add_1_axes_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_3_batchnorm_add_1_axes_0"), val = tensor&lt;int32, [1]&gt;([2])];
            tensor&lt;fp16, [128]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_3_batchnorm_mul_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_3_batchnorm_mul_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(918592)))];
            tensor&lt;fp16, [128]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_3_batchnorm_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_3_batchnorm_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(918912)))];
            tensor&lt;fp16, []&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_3_batchnorm_add_y_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_3_batchnorm_add_y_to_fp16"), val = tensor&lt;fp16, []&gt;(0x1.1p-20)];
            tensor&lt;fp16, [?, 64, 128]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_3_batchnorm_add_1_cast = layer_norm(axes = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_3_batchnorm_add_1_axes_0, beta = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_3_batchnorm_ReadVariableOp_to_fp16, epsilon = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_3_batchnorm_add_y_to_fp16, gamma = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_3_batchnorm_mul_ReadVariableOp_to_fp16, x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_add_cast);
            tensor&lt;int32, [2]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Tensordot_stackx = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Tensordot_stackx"), val = tensor&lt;int32, [2]&gt;([-1, 128])];
            tensor&lt;fp16, [?, 128]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Tensordot_Reshape_cast = reshape(shape = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Tensordot_stackx, x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_3_batchnorm_add_1_cast);
            tensor&lt;bool, []&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Tensordot_MatMul_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Tensordot_MatMul_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Tensordot_MatMul_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Tensordot_MatMul_transpose_y_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [128, 256]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Tensordot_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Tensordot_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [128, 256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(919232)))];
            tensor&lt;fp16, [?, 256]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Tensordot_MatMul_cast = matmul(transpose_x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Tensordot_MatMul_transpose_x_0, transpose_y = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Tensordot_MatMul_transpose_y_0, x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Tensordot_Reshape_cast, y = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Tensordot_ReadVariableOp_to_fp16);
            tensor&lt;int32, [3]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Tensordot_concat_1x = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Tensordot_concat_1x"), val = tensor&lt;int32, [3]&gt;([-1, 64, 256])];
            tensor&lt;fp16, [?, 64, 256]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Tensordot_cast = reshape(shape = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Tensordot_concat_1x, x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Tensordot_MatMul_cast);
            tensor&lt;fp16, [256]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_BiasAdd_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_BiasAdd_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [256]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(984832)))];
            tensor&lt;fp16, [?, 64, 256]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_BiasAdd_cast = add(x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Tensordot_cast, y = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_BiasAdd_ReadVariableOp_to_fp16);
            tensor&lt;string, []&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Gelu_mul_1_mode_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Gelu_mul_1_mode_0"), val = tensor&lt;string, []&gt;("EXACT")];
            tensor&lt;fp16, [?, 64, 256]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Gelu_mul_1_cast = gelu(mode = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Gelu_mul_1_mode_0, x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_BiasAdd_cast);
            tensor&lt;int32, [2]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_Tensordot_stackx = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_Tensordot_stackx"), val = tensor&lt;int32, [2]&gt;([-1, 256])];
            tensor&lt;fp16, [?, 256]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_Tensordot_Reshape_cast = reshape(shape = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_Tensordot_stackx, x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_4_Gelu_mul_1_cast);
            tensor&lt;bool, []&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_Tensordot_MatMul_transpose_x_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_Tensordot_MatMul_transpose_x_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_Tensordot_MatMul_transpose_y_0 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_Tensordot_MatMul_transpose_y_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [256, 128]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_Tensordot_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_Tensordot_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [256, 128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(985408)))];
            tensor&lt;fp16, [?, 128]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_Tensordot_MatMul_cast = matmul(transpose_x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_Tensordot_MatMul_transpose_x_0, transpose_y = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_Tensordot_MatMul_transpose_y_0, x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_Tensordot_Reshape_cast, y = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_Tensordot_ReadVariableOp_to_fp16);
            tensor&lt;int32, [3]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_Tensordot_concat_1x = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_Tensordot_concat_1x"), val = tensor&lt;int32, [3]&gt;([-1, 64, 128])];
            tensor&lt;fp16, [?, 64, 128]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_Tensordot_cast = reshape(shape = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_Tensordot_concat_1x, x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_Tensordot_MatMul_cast);
            tensor&lt;fp16, [128]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_BiasAdd_ReadVariableOp_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_BiasAdd_ReadVariableOp_to_fp16"), val = tensor&lt;fp16, [128]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(1051008)))];
            tensor&lt;fp16, [?, 64, 128]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_BiasAdd_cast = add(x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_Tensordot_cast, y = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_BiasAdd_ReadVariableOp_to_fp16);
            tensor&lt;fp16, [?, 64, 128]&gt; model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_add_1_cast = add(x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_layer_normalization_3_batchnorm_add_1_cast, y = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_sequential_1_dense_5_BiasAdd_cast);
            tensor&lt;int32, [1]&gt; model_1_tf_math_reduce_mean_Mean_axes_0 = const()[name = tensor&lt;string, []&gt;("model_1_tf_math_reduce_mean_Mean_axes_0"), val = tensor&lt;int32, [1]&gt;([-2])];
            tensor&lt;bool, []&gt; model_1_tf_math_reduce_mean_Mean_keep_dims_0 = const()[name = tensor&lt;string, []&gt;("model_1_tf_math_reduce_mean_Mean_keep_dims_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;fp16, [?, 128]&gt; model_1_tf_math_reduce_mean_Mean_cast = reduce_mean(axes = model_1_tf_math_reduce_mean_Mean_axes_0, keep_dims = model_1_tf_math_reduce_mean_Mean_keep_dims_0, x = model_1_transformer_block_1_StatefulPartitionedCall_StatefulPartitionedCall_add_1_cast);
            tensor&lt;string, []&gt; model_1_tf_math_reduce_mean_Mean_cast_to_fp32_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_1_tf_math_reduce_mean_Mean_cast_to_fp32_dtype_0"), val = tensor&lt;string, []&gt;("fp32")];
            tensor&lt;fp32, [?, 128]&gt; Identity = cast(dtype = model_1_tf_math_reduce_mean_Mean_cast_to_fp32_dtype_0, x = model_1_tf_math_reduce_mean_Mean_cast);
        } -&gt; (Identity);
}</code></pre>
                    </div>
                </div>
            
                <div class="file-section" id="file-23">
                    <div class="file-header">
                        <h2>ðŸ“„ model.mil 5</h2>
                        <div class="file-meta">Size: 7.74 KB</div>
                    </div>
                    <div class="file-content">
                        <pre><code>program(1.0)
[buildInfo = dict&lt;tensor&lt;string, []&gt;, tensor&lt;string, []&gt;&gt;({{"coremlc-component-MIL", "3400.8.1"}, {"coremlc-version", "3400.7.1"}})]
{
    func main&lt;ios16&gt;(tensor&lt;fp32, [?, 64]&gt; context_encoding) [FlexibleShapeInformation = tuple&lt;tuple&lt;tensor&lt;string, []&gt;, dict&lt;tensor&lt;string, []&gt;, tensor&lt;int32, [?]&gt;&gt;&gt;, tuple&lt;tensor&lt;string, []&gt;, dict&lt;tensor&lt;string, []&gt;, list&lt;tensor&lt;int32, [2]&gt;, ?&gt;&gt;&gt;&gt;((("DefaultShapes", {{"context_encoding", [1, 64]}}), ("RangeDims", {{"context_encoding", [[1, -1], [64, 64]]}})))] {
            tensor&lt;string, []&gt; context_encoding_to_fp16_dtype_0 = const()[name = tensor&lt;string, []&gt;("context_encoding_to_fp16_dtype_0"), val = tensor&lt;string, []&gt;("fp16")];
            tensor&lt;fp16, [786, 64]&gt; transpose_0_cast_to_fp32_to_fp16 = const()[name = tensor&lt;string, []&gt;("transpose_0_cast_to_fp32_to_fp16"), val = tensor&lt;fp16, [786, 64]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(64)))];
            tensor&lt;fp16, [786]&gt; model_2_dense_1_BiasAdd_bias_0_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_2_dense_1_BiasAdd_bias_0_to_fp16"), val = tensor&lt;fp16, [786]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(100736)))];
            tensor&lt;fp16, [?, 64]&gt; cast_3 = cast(dtype = context_encoding_to_fp16_dtype_0, x = context_encoding);
            tensor&lt;fp16, [?, 786]&gt; model_2_dense_1_BiasAdd_cast = linear(bias = model_2_dense_1_BiasAdd_bias_0_to_fp16, weight = transpose_0_cast_to_fp32_to_fp16, x = cast_3);
            tensor&lt;int32, []&gt; model_2_lambda_1_argsort_TopKV2_k_0 = const()[name = tensor&lt;string, []&gt;("model_2_lambda_1_argsort_TopKV2_k_0"), val = tensor&lt;int32, []&gt;(786)];
            tensor&lt;int32, []&gt; model_2_lambda_1_argsort_TopKV2_axis_0 = const()[name = tensor&lt;string, []&gt;("model_2_lambda_1_argsort_TopKV2_axis_0"), val = tensor&lt;int32, []&gt;(-1)];
            tensor&lt;bool, []&gt; model_2_lambda_1_argsort_TopKV2_sort_0 = const()[name = tensor&lt;string, []&gt;("model_2_lambda_1_argsort_TopKV2_sort_0"), val = tensor&lt;bool, []&gt;(true)];
            tensor&lt;bool, []&gt; model_2_lambda_1_argsort_TopKV2_ascending_0 = const()[name = tensor&lt;string, []&gt;("model_2_lambda_1_argsort_TopKV2_ascending_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; model_2_lambda_1_argsort_TopKV2_return_indices_0 = const()[name = tensor&lt;string, []&gt;("model_2_lambda_1_argsort_TopKV2_return_indices_0"), val = tensor&lt;bool, []&gt;(true)];
            tensor&lt;fp16, [?, 786]&gt; model_2_lambda_1_argsort_TopKV2_cast_0, tensor&lt;int32, [?, 786]&gt; model_2_lambda_1_argsort_TopKV2_cast_1 = topk(ascending = model_2_lambda_1_argsort_TopKV2_ascending_0, axis = model_2_lambda_1_argsort_TopKV2_axis_0, k = model_2_lambda_1_argsort_TopKV2_k_0, return_indices = model_2_lambda_1_argsort_TopKV2_return_indices_0, sort = model_2_lambda_1_argsort_TopKV2_sort_0, x = model_2_dense_1_BiasAdd_cast);
            tensor&lt;int32, [2]&gt; model_2_tf___operators___getitem_2_strided_slice_begin_0 = const()[name = tensor&lt;string, []&gt;("model_2_tf___operators___getitem_2_strided_slice_begin_0"), val = tensor&lt;int32, [2]&gt;([0, 0])];
            tensor&lt;int32, [2]&gt; model_2_tf___operators___getitem_2_strided_slice_end_0 = const()[name = tensor&lt;string, []&gt;("model_2_tf___operators___getitem_2_strided_slice_end_0"), val = tensor&lt;int32, [2]&gt;([0, 40])];
            tensor&lt;int32, [2]&gt; model_2_tf___operators___getitem_2_strided_slice_stride_0 = const()[name = tensor&lt;string, []&gt;("model_2_tf___operators___getitem_2_strided_slice_stride_0"), val = tensor&lt;int32, [2]&gt;([1, 1])];
            tensor&lt;bool, [2]&gt; model_2_tf___operators___getitem_2_strided_slice_begin_mask_0 = const()[name = tensor&lt;string, []&gt;("model_2_tf___operators___getitem_2_strided_slice_begin_mask_0"), val = tensor&lt;bool, [2]&gt;([true, true])];
            tensor&lt;bool, [2]&gt; model_2_tf___operators___getitem_2_strided_slice_end_mask_0 = const()[name = tensor&lt;string, []&gt;("model_2_tf___operators___getitem_2_strided_slice_end_mask_0"), val = tensor&lt;bool, [2]&gt;([true, false])];
            tensor&lt;bool, [2]&gt; model_2_tf___operators___getitem_2_strided_slice_squeeze_mask_0 = const()[name = tensor&lt;string, []&gt;("model_2_tf___operators___getitem_2_strided_slice_squeeze_mask_0"), val = tensor&lt;bool, [2]&gt;([false, false])];
            tensor&lt;int32, [?, 40]&gt; Identity = slice_by_index(begin = model_2_tf___operators___getitem_2_strided_slice_begin_0, begin_mask = model_2_tf___operators___getitem_2_strided_slice_begin_mask_0, end = model_2_tf___operators___getitem_2_strided_slice_end_0, end_mask = model_2_tf___operators___getitem_2_strided_slice_end_mask_0, squeeze_mask = model_2_tf___operators___getitem_2_strided_slice_squeeze_mask_0, stride = model_2_tf___operators___getitem_2_strided_slice_stride_0, x = model_2_lambda_1_argsort_TopKV2_cast_1);
            tensor&lt;int32, []&gt; model_2_tf_compat_v1_gather_1_GatherV2_axis_0 = const()[name = tensor&lt;string, []&gt;("model_2_tf_compat_v1_gather_1_GatherV2_axis_0"), val = tensor&lt;int32, []&gt;(1)];
            tensor&lt;int32, []&gt; model_2_tf_compat_v1_gather_1_GatherV2_batch_dims_0 = const()[name = tensor&lt;string, []&gt;("model_2_tf_compat_v1_gather_1_GatherV2_batch_dims_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;fp16, [?, ?, 786]&gt; model_2_tf_compat_v1_gather_1_GatherV2_cast = gather(axis = model_2_tf_compat_v1_gather_1_GatherV2_axis_0, batch_dims = model_2_tf_compat_v1_gather_1_GatherV2_batch_dims_0, indices = model_2_lambda_1_argsort_TopKV2_cast_1, x = model_2_dense_1_BiasAdd_cast);
            tensor&lt;int32, [3]&gt; model_2_tf___operators___getitem_3_strided_slice_begin_0 = const()[name = tensor&lt;string, []&gt;("model_2_tf___operators___getitem_3_strided_slice_begin_0"), val = tensor&lt;int32, [3]&gt;([0, 0, 0])];
            tensor&lt;int32, [3]&gt; model_2_tf___operators___getitem_3_strided_slice_end_0 = const()[name = tensor&lt;string, []&gt;("model_2_tf___operators___getitem_3_strided_slice_end_0"), val = tensor&lt;int32, [3]&gt;([0, 1, 40])];
            tensor&lt;int32, [3]&gt; model_2_tf___operators___getitem_3_strided_slice_stride_0 = const()[name = tensor&lt;string, []&gt;("model_2_tf___operators___getitem_3_strided_slice_stride_0"), val = tensor&lt;int32, [3]&gt;([1, 1, 1])];
            tensor&lt;bool, [3]&gt; model_2_tf___operators___getitem_3_strided_slice_begin_mask_0 = const()[name = tensor&lt;string, []&gt;("model_2_tf___operators___getitem_3_strided_slice_begin_mask_0"), val = tensor&lt;bool, [3]&gt;([true, false, true])];
            tensor&lt;bool, [3]&gt; model_2_tf___operators___getitem_3_strided_slice_end_mask_0 = const()[name = tensor&lt;string, []&gt;("model_2_tf___operators___getitem_3_strided_slice_end_mask_0"), val = tensor&lt;bool, [3]&gt;([true, false, false])];
            tensor&lt;bool, [3]&gt; model_2_tf___operators___getitem_3_strided_slice_squeeze_mask_0 = const()[name = tensor&lt;string, []&gt;("model_2_tf___operators___getitem_3_strided_slice_squeeze_mask_0"), val = tensor&lt;bool, [3]&gt;([false, true, false])];
            tensor&lt;fp16, [?, 40]&gt; model_2_tf___operators___getitem_3_strided_slice_cast = slice_by_index(begin = model_2_tf___operators___getitem_3_strided_slice_begin_0, begin_mask = model_2_tf___operators___getitem_3_strided_slice_begin_mask_0, end = model_2_tf___operators___getitem_3_strided_slice_end_0, end_mask = model_2_tf___operators___getitem_3_strided_slice_end_mask_0, squeeze_mask = model_2_tf___operators___getitem_3_strided_slice_squeeze_mask_0, stride = model_2_tf___operators___getitem_3_strided_slice_stride_0, x = model_2_tf_compat_v1_gather_1_GatherV2_cast);
            tensor&lt;string, []&gt; model_2_tf___operators___getitem_3_strided_slice_cast_to_fp32_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_2_tf___operators___getitem_3_strided_slice_cast_to_fp32_dtype_0"), val = tensor&lt;string, []&gt;("fp32")];
            tensor&lt;fp32, [?, 40]&gt; Identity_1 = cast(dtype = model_2_tf___operators___getitem_3_strided_slice_cast_to_fp32_dtype_0, x = model_2_tf___operators___getitem_3_strided_slice_cast);
        } -&gt; (Identity, Identity_1);
}</code></pre>
                    </div>
                </div>
            
                <div class="file-section" id="file-24">
                    <div class="file-header">
                        <h2>ðŸ“„ model.mil 2</h2>
                        <div class="file-meta">Size: 7.64 KB</div>
                    </div>
                    <div class="file-content">
                        <pre><code>program(1.0)
[buildInfo = dict&lt;tensor&lt;string, []&gt;, tensor&lt;string, []&gt;&gt;({{"coremlc-component-MIL", "3400.8.1"}, {"coremlc-version", "3400.7.1"}})]
{
    func main&lt;ios16&gt;(tensor&lt;fp32, [?, 64]&gt; context_encoding) [FlexibleShapeInformation = tuple&lt;tuple&lt;tensor&lt;string, []&gt;, dict&lt;tensor&lt;string, []&gt;, tensor&lt;int32, [?]&gt;&gt;&gt;, tuple&lt;tensor&lt;string, []&gt;, dict&lt;tensor&lt;string, []&gt;, list&lt;tensor&lt;int32, [2]&gt;, ?&gt;&gt;&gt;&gt;((("DefaultShapes", {{"context_encoding", [1, 64]}}), ("RangeDims", {{"context_encoding", [[1, -1], [64, 64]]}})))] {
            tensor&lt;string, []&gt; context_encoding_to_fp16_dtype_0 = const()[name = tensor&lt;string, []&gt;("context_encoding_to_fp16_dtype_0"), val = tensor&lt;string, []&gt;("fp16")];
            tensor&lt;fp16, [109, 64]&gt; transpose_0_cast_to_fp32_to_fp16 = const()[name = tensor&lt;string, []&gt;("transpose_0_cast_to_fp32_to_fp16"), val = tensor&lt;fp16, [109, 64]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(64)))];
            tensor&lt;fp16, [109]&gt; model_1_dense_BiasAdd_bias_0_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_1_dense_BiasAdd_bias_0_to_fp16"), val = tensor&lt;fp16, [109]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(14080)))];
            tensor&lt;fp16, [?, 64]&gt; cast_3 = cast(dtype = context_encoding_to_fp16_dtype_0, x = context_encoding);
            tensor&lt;fp16, [?, 109]&gt; model_1_dense_BiasAdd_cast = linear(bias = model_1_dense_BiasAdd_bias_0_to_fp16, weight = transpose_0_cast_to_fp32_to_fp16, x = cast_3);
            tensor&lt;int32, []&gt; model_1_lambda_argsort_TopKV2_k_0 = const()[name = tensor&lt;string, []&gt;("model_1_lambda_argsort_TopKV2_k_0"), val = tensor&lt;int32, []&gt;(109)];
            tensor&lt;int32, []&gt; model_1_lambda_argsort_TopKV2_axis_0 = const()[name = tensor&lt;string, []&gt;("model_1_lambda_argsort_TopKV2_axis_0"), val = tensor&lt;int32, []&gt;(-1)];
            tensor&lt;bool, []&gt; model_1_lambda_argsort_TopKV2_sort_0 = const()[name = tensor&lt;string, []&gt;("model_1_lambda_argsort_TopKV2_sort_0"), val = tensor&lt;bool, []&gt;(true)];
            tensor&lt;bool, []&gt; model_1_lambda_argsort_TopKV2_ascending_0 = const()[name = tensor&lt;string, []&gt;("model_1_lambda_argsort_TopKV2_ascending_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; model_1_lambda_argsort_TopKV2_return_indices_0 = const()[name = tensor&lt;string, []&gt;("model_1_lambda_argsort_TopKV2_return_indices_0"), val = tensor&lt;bool, []&gt;(true)];
            tensor&lt;fp16, [?, 109]&gt; model_1_lambda_argsort_TopKV2_cast_0, tensor&lt;int32, [?, 109]&gt; model_1_lambda_argsort_TopKV2_cast_1 = topk(ascending = model_1_lambda_argsort_TopKV2_ascending_0, axis = model_1_lambda_argsort_TopKV2_axis_0, k = model_1_lambda_argsort_TopKV2_k_0, return_indices = model_1_lambda_argsort_TopKV2_return_indices_0, sort = model_1_lambda_argsort_TopKV2_sort_0, x = model_1_dense_BiasAdd_cast);
            tensor&lt;int32, [2]&gt; model_1_tf___operators___getitem_strided_slice_begin_0 = const()[name = tensor&lt;string, []&gt;("model_1_tf___operators___getitem_strided_slice_begin_0"), val = tensor&lt;int32, [2]&gt;([0, 0])];
            tensor&lt;int32, [2]&gt; model_1_tf___operators___getitem_strided_slice_end_0 = const()[name = tensor&lt;string, []&gt;("model_1_tf___operators___getitem_strided_slice_end_0"), val = tensor&lt;int32, [2]&gt;([0, 100])];
            tensor&lt;int32, [2]&gt; model_1_tf___operators___getitem_strided_slice_stride_0 = const()[name = tensor&lt;string, []&gt;("model_1_tf___operators___getitem_strided_slice_stride_0"), val = tensor&lt;int32, [2]&gt;([1, 1])];
            tensor&lt;bool, [2]&gt; model_1_tf___operators___getitem_strided_slice_begin_mask_0 = const()[name = tensor&lt;string, []&gt;("model_1_tf___operators___getitem_strided_slice_begin_mask_0"), val = tensor&lt;bool, [2]&gt;([true, true])];
            tensor&lt;bool, [2]&gt; model_1_tf___operators___getitem_strided_slice_end_mask_0 = const()[name = tensor&lt;string, []&gt;("model_1_tf___operators___getitem_strided_slice_end_mask_0"), val = tensor&lt;bool, [2]&gt;([true, false])];
            tensor&lt;bool, [2]&gt; model_1_tf___operators___getitem_strided_slice_squeeze_mask_0 = const()[name = tensor&lt;string, []&gt;("model_1_tf___operators___getitem_strided_slice_squeeze_mask_0"), val = tensor&lt;bool, [2]&gt;([false, false])];
            tensor&lt;int32, [?, 100]&gt; Identity = slice_by_index(begin = model_1_tf___operators___getitem_strided_slice_begin_0, begin_mask = model_1_tf___operators___getitem_strided_slice_begin_mask_0, end = model_1_tf___operators___getitem_strided_slice_end_0, end_mask = model_1_tf___operators___getitem_strided_slice_end_mask_0, squeeze_mask = model_1_tf___operators___getitem_strided_slice_squeeze_mask_0, stride = model_1_tf___operators___getitem_strided_slice_stride_0, x = model_1_lambda_argsort_TopKV2_cast_1);
            tensor&lt;int32, []&gt; model_1_tf_compat_v1_gather_GatherV2_axis_0 = const()[name = tensor&lt;string, []&gt;("model_1_tf_compat_v1_gather_GatherV2_axis_0"), val = tensor&lt;int32, []&gt;(1)];
            tensor&lt;int32, []&gt; model_1_tf_compat_v1_gather_GatherV2_batch_dims_0 = const()[name = tensor&lt;string, []&gt;("model_1_tf_compat_v1_gather_GatherV2_batch_dims_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;fp16, [?, ?, 109]&gt; model_1_tf_compat_v1_gather_GatherV2_cast = gather(axis = model_1_tf_compat_v1_gather_GatherV2_axis_0, batch_dims = model_1_tf_compat_v1_gather_GatherV2_batch_dims_0, indices = model_1_lambda_argsort_TopKV2_cast_1, x = model_1_dense_BiasAdd_cast);
            tensor&lt;int32, [3]&gt; model_1_tf___operators___getitem_1_strided_slice_begin_0 = const()[name = tensor&lt;string, []&gt;("model_1_tf___operators___getitem_1_strided_slice_begin_0"), val = tensor&lt;int32, [3]&gt;([0, 0, 0])];
            tensor&lt;int32, [3]&gt; model_1_tf___operators___getitem_1_strided_slice_end_0 = const()[name = tensor&lt;string, []&gt;("model_1_tf___operators___getitem_1_strided_slice_end_0"), val = tensor&lt;int32, [3]&gt;([0, 1, 100])];
            tensor&lt;int32, [3]&gt; model_1_tf___operators___getitem_1_strided_slice_stride_0 = const()[name = tensor&lt;string, []&gt;("model_1_tf___operators___getitem_1_strided_slice_stride_0"), val = tensor&lt;int32, [3]&gt;([1, 1, 1])];
            tensor&lt;bool, [3]&gt; model_1_tf___operators___getitem_1_strided_slice_begin_mask_0 = const()[name = tensor&lt;string, []&gt;("model_1_tf___operators___getitem_1_strided_slice_begin_mask_0"), val = tensor&lt;bool, [3]&gt;([true, false, true])];
            tensor&lt;bool, [3]&gt; model_1_tf___operators___getitem_1_strided_slice_end_mask_0 = const()[name = tensor&lt;string, []&gt;("model_1_tf___operators___getitem_1_strided_slice_end_mask_0"), val = tensor&lt;bool, [3]&gt;([true, false, false])];
            tensor&lt;bool, [3]&gt; model_1_tf___operators___getitem_1_strided_slice_squeeze_mask_0 = const()[name = tensor&lt;string, []&gt;("model_1_tf___operators___getitem_1_strided_slice_squeeze_mask_0"), val = tensor&lt;bool, [3]&gt;([false, true, false])];
            tensor&lt;fp16, [?, 100]&gt; model_1_tf___operators___getitem_1_strided_slice_cast = slice_by_index(begin = model_1_tf___operators___getitem_1_strided_slice_begin_0, begin_mask = model_1_tf___operators___getitem_1_strided_slice_begin_mask_0, end = model_1_tf___operators___getitem_1_strided_slice_end_0, end_mask = model_1_tf___operators___getitem_1_strided_slice_end_mask_0, squeeze_mask = model_1_tf___operators___getitem_1_strided_slice_squeeze_mask_0, stride = model_1_tf___operators___getitem_1_strided_slice_stride_0, x = model_1_tf_compat_v1_gather_GatherV2_cast);
            tensor&lt;string, []&gt; model_1_tf___operators___getitem_1_strided_slice_cast_to_fp32_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_1_tf___operators___getitem_1_strided_slice_cast_to_fp32_dtype_0"), val = tensor&lt;string, []&gt;("fp32")];
            tensor&lt;fp32, [?, 100]&gt; Identity_1 = cast(dtype = model_1_tf___operators___getitem_1_strided_slice_cast_to_fp32_dtype_0, x = model_1_tf___operators___getitem_1_strided_slice_cast);
        } -&gt; (Identity, Identity_1);
}</code></pre>
                    </div>
                </div>
            
                <div class="file-section" id="file-25">
                    <div class="file-header">
                        <h2>ðŸ“„ model.mil 3</h2>
                        <div class="file-meta">Size: 7.74 KB</div>
                    </div>
                    <div class="file-content">
                        <pre><code>program(1.0)
[buildInfo = dict&lt;tensor&lt;string, []&gt;, tensor&lt;string, []&gt;&gt;({{"coremlc-component-MIL", "3400.8.1"}, {"coremlc-version", "3400.7.1"}})]
{
    func main&lt;ios16&gt;(tensor&lt;fp32, [?, 64]&gt; context_encoding) [FlexibleShapeInformation = tuple&lt;tuple&lt;tensor&lt;string, []&gt;, dict&lt;tensor&lt;string, []&gt;, tensor&lt;int32, [?]&gt;&gt;&gt;, tuple&lt;tensor&lt;string, []&gt;, dict&lt;tensor&lt;string, []&gt;, list&lt;tensor&lt;int32, [2]&gt;, ?&gt;&gt;&gt;&gt;((("DefaultShapes", {{"context_encoding", [1, 64]}}), ("RangeDims", {{"context_encoding", [[1, -1], [64, 64]]}})))] {
            tensor&lt;string, []&gt; context_encoding_to_fp16_dtype_0 = const()[name = tensor&lt;string, []&gt;("context_encoding_to_fp16_dtype_0"), val = tensor&lt;string, []&gt;("fp16")];
            tensor&lt;fp16, [170, 64]&gt; transpose_0_cast_to_fp32_to_fp16 = const()[name = tensor&lt;string, []&gt;("transpose_0_cast_to_fp32_to_fp16"), val = tensor&lt;fp16, [170, 64]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(64)))];
            tensor&lt;fp16, [170]&gt; model_4_dense_3_BiasAdd_bias_0_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_4_dense_3_BiasAdd_bias_0_to_fp16"), val = tensor&lt;fp16, [170]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(21888)))];
            tensor&lt;fp16, [?, 64]&gt; cast_3 = cast(dtype = context_encoding_to_fp16_dtype_0, x = context_encoding);
            tensor&lt;fp16, [?, 170]&gt; model_4_dense_3_BiasAdd_cast = linear(bias = model_4_dense_3_BiasAdd_bias_0_to_fp16, weight = transpose_0_cast_to_fp32_to_fp16, x = cast_3);
            tensor&lt;int32, []&gt; model_4_lambda_3_argsort_TopKV2_k_0 = const()[name = tensor&lt;string, []&gt;("model_4_lambda_3_argsort_TopKV2_k_0"), val = tensor&lt;int32, []&gt;(170)];
            tensor&lt;int32, []&gt; model_4_lambda_3_argsort_TopKV2_axis_0 = const()[name = tensor&lt;string, []&gt;("model_4_lambda_3_argsort_TopKV2_axis_0"), val = tensor&lt;int32, []&gt;(-1)];
            tensor&lt;bool, []&gt; model_4_lambda_3_argsort_TopKV2_sort_0 = const()[name = tensor&lt;string, []&gt;("model_4_lambda_3_argsort_TopKV2_sort_0"), val = tensor&lt;bool, []&gt;(true)];
            tensor&lt;bool, []&gt; model_4_lambda_3_argsort_TopKV2_ascending_0 = const()[name = tensor&lt;string, []&gt;("model_4_lambda_3_argsort_TopKV2_ascending_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; model_4_lambda_3_argsort_TopKV2_return_indices_0 = const()[name = tensor&lt;string, []&gt;("model_4_lambda_3_argsort_TopKV2_return_indices_0"), val = tensor&lt;bool, []&gt;(true)];
            tensor&lt;fp16, [?, 170]&gt; model_4_lambda_3_argsort_TopKV2_cast_0, tensor&lt;int32, [?, 170]&gt; model_4_lambda_3_argsort_TopKV2_cast_1 = topk(ascending = model_4_lambda_3_argsort_TopKV2_ascending_0, axis = model_4_lambda_3_argsort_TopKV2_axis_0, k = model_4_lambda_3_argsort_TopKV2_k_0, return_indices = model_4_lambda_3_argsort_TopKV2_return_indices_0, sort = model_4_lambda_3_argsort_TopKV2_sort_0, x = model_4_dense_3_BiasAdd_cast);
            tensor&lt;int32, [2]&gt; model_4_tf___operators___getitem_6_strided_slice_begin_0 = const()[name = tensor&lt;string, []&gt;("model_4_tf___operators___getitem_6_strided_slice_begin_0"), val = tensor&lt;int32, [2]&gt;([0, 0])];
            tensor&lt;int32, [2]&gt; model_4_tf___operators___getitem_6_strided_slice_end_0 = const()[name = tensor&lt;string, []&gt;("model_4_tf___operators___getitem_6_strided_slice_end_0"), val = tensor&lt;int32, [2]&gt;([0, 10])];
            tensor&lt;int32, [2]&gt; model_4_tf___operators___getitem_6_strided_slice_stride_0 = const()[name = tensor&lt;string, []&gt;("model_4_tf___operators___getitem_6_strided_slice_stride_0"), val = tensor&lt;int32, [2]&gt;([1, 1])];
            tensor&lt;bool, [2]&gt; model_4_tf___operators___getitem_6_strided_slice_begin_mask_0 = const()[name = tensor&lt;string, []&gt;("model_4_tf___operators___getitem_6_strided_slice_begin_mask_0"), val = tensor&lt;bool, [2]&gt;([true, true])];
            tensor&lt;bool, [2]&gt; model_4_tf___operators___getitem_6_strided_slice_end_mask_0 = const()[name = tensor&lt;string, []&gt;("model_4_tf___operators___getitem_6_strided_slice_end_mask_0"), val = tensor&lt;bool, [2]&gt;([true, false])];
            tensor&lt;bool, [2]&gt; model_4_tf___operators___getitem_6_strided_slice_squeeze_mask_0 = const()[name = tensor&lt;string, []&gt;("model_4_tf___operators___getitem_6_strided_slice_squeeze_mask_0"), val = tensor&lt;bool, [2]&gt;([false, false])];
            tensor&lt;int32, [?, 10]&gt; Identity = slice_by_index(begin = model_4_tf___operators___getitem_6_strided_slice_begin_0, begin_mask = model_4_tf___operators___getitem_6_strided_slice_begin_mask_0, end = model_4_tf___operators___getitem_6_strided_slice_end_0, end_mask = model_4_tf___operators___getitem_6_strided_slice_end_mask_0, squeeze_mask = model_4_tf___operators___getitem_6_strided_slice_squeeze_mask_0, stride = model_4_tf___operators___getitem_6_strided_slice_stride_0, x = model_4_lambda_3_argsort_TopKV2_cast_1);
            tensor&lt;int32, []&gt; model_4_tf_compat_v1_gather_3_GatherV2_axis_0 = const()[name = tensor&lt;string, []&gt;("model_4_tf_compat_v1_gather_3_GatherV2_axis_0"), val = tensor&lt;int32, []&gt;(1)];
            tensor&lt;int32, []&gt; model_4_tf_compat_v1_gather_3_GatherV2_batch_dims_0 = const()[name = tensor&lt;string, []&gt;("model_4_tf_compat_v1_gather_3_GatherV2_batch_dims_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;fp16, [?, ?, 170]&gt; model_4_tf_compat_v1_gather_3_GatherV2_cast = gather(axis = model_4_tf_compat_v1_gather_3_GatherV2_axis_0, batch_dims = model_4_tf_compat_v1_gather_3_GatherV2_batch_dims_0, indices = model_4_lambda_3_argsort_TopKV2_cast_1, x = model_4_dense_3_BiasAdd_cast);
            tensor&lt;int32, [3]&gt; model_4_tf___operators___getitem_7_strided_slice_begin_0 = const()[name = tensor&lt;string, []&gt;("model_4_tf___operators___getitem_7_strided_slice_begin_0"), val = tensor&lt;int32, [3]&gt;([0, 0, 0])];
            tensor&lt;int32, [3]&gt; model_4_tf___operators___getitem_7_strided_slice_end_0 = const()[name = tensor&lt;string, []&gt;("model_4_tf___operators___getitem_7_strided_slice_end_0"), val = tensor&lt;int32, [3]&gt;([0, 1, 10])];
            tensor&lt;int32, [3]&gt; model_4_tf___operators___getitem_7_strided_slice_stride_0 = const()[name = tensor&lt;string, []&gt;("model_4_tf___operators___getitem_7_strided_slice_stride_0"), val = tensor&lt;int32, [3]&gt;([1, 1, 1])];
            tensor&lt;bool, [3]&gt; model_4_tf___operators___getitem_7_strided_slice_begin_mask_0 = const()[name = tensor&lt;string, []&gt;("model_4_tf___operators___getitem_7_strided_slice_begin_mask_0"), val = tensor&lt;bool, [3]&gt;([true, false, true])];
            tensor&lt;bool, [3]&gt; model_4_tf___operators___getitem_7_strided_slice_end_mask_0 = const()[name = tensor&lt;string, []&gt;("model_4_tf___operators___getitem_7_strided_slice_end_mask_0"), val = tensor&lt;bool, [3]&gt;([true, false, false])];
            tensor&lt;bool, [3]&gt; model_4_tf___operators___getitem_7_strided_slice_squeeze_mask_0 = const()[name = tensor&lt;string, []&gt;("model_4_tf___operators___getitem_7_strided_slice_squeeze_mask_0"), val = tensor&lt;bool, [3]&gt;([false, true, false])];
            tensor&lt;fp16, [?, 10]&gt; model_4_tf___operators___getitem_7_strided_slice_cast = slice_by_index(begin = model_4_tf___operators___getitem_7_strided_slice_begin_0, begin_mask = model_4_tf___operators___getitem_7_strided_slice_begin_mask_0, end = model_4_tf___operators___getitem_7_strided_slice_end_0, end_mask = model_4_tf___operators___getitem_7_strided_slice_end_mask_0, squeeze_mask = model_4_tf___operators___getitem_7_strided_slice_squeeze_mask_0, stride = model_4_tf___operators___getitem_7_strided_slice_stride_0, x = model_4_tf_compat_v1_gather_3_GatherV2_cast);
            tensor&lt;string, []&gt; model_4_tf___operators___getitem_7_strided_slice_cast_to_fp32_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_4_tf___operators___getitem_7_strided_slice_cast_to_fp32_dtype_0"), val = tensor&lt;string, []&gt;("fp32")];
            tensor&lt;fp32, [?, 10]&gt; Identity_1 = cast(dtype = model_4_tf___operators___getitem_7_strided_slice_cast_to_fp32_dtype_0, x = model_4_tf___operators___getitem_7_strided_slice_cast);
        } -&gt; (Identity, Identity_1);
}</code></pre>
                    </div>
                </div>
            
                <div class="file-section" id="file-26">
                    <div class="file-header">
                        <h2>ðŸ“„ model.mil 7</h2>
                        <div class="file-meta">Size: 7.74 KB</div>
                    </div>
                    <div class="file-content">
                        <pre><code>program(1.0)
[buildInfo = dict&lt;tensor&lt;string, []&gt;, tensor&lt;string, []&gt;&gt;({{"coremlc-component-MIL", "3400.8.1"}, {"coremlc-version", "3400.7.1"}})]
{
    func main&lt;ios16&gt;(tensor&lt;fp32, [?, 64]&gt; context_encoding) [FlexibleShapeInformation = tuple&lt;tuple&lt;tensor&lt;string, []&gt;, dict&lt;tensor&lt;string, []&gt;, tensor&lt;int32, [?]&gt;&gt;&gt;, tuple&lt;tensor&lt;string, []&gt;, dict&lt;tensor&lt;string, []&gt;, list&lt;tensor&lt;int32, [2]&gt;, ?&gt;&gt;&gt;&gt;((("DefaultShapes", {{"context_encoding", [1, 64]}}), ("RangeDims", {{"context_encoding", [[1, -1], [64, 64]]}})))] {
            tensor&lt;string, []&gt; context_encoding_to_fp16_dtype_0 = const()[name = tensor&lt;string, []&gt;("context_encoding_to_fp16_dtype_0"), val = tensor&lt;string, []&gt;("fp16")];
            tensor&lt;fp16, [303, 64]&gt; transpose_0_cast_to_fp32_to_fp16 = const()[name = tensor&lt;string, []&gt;("transpose_0_cast_to_fp32_to_fp16"), val = tensor&lt;fp16, [303, 64]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(64)))];
            tensor&lt;fp16, [303]&gt; model_3_dense_2_BiasAdd_bias_0_to_fp16 = const()[name = tensor&lt;string, []&gt;("model_3_dense_2_BiasAdd_bias_0_to_fp16"), val = tensor&lt;fp16, [303]&gt;(BLOBFILE(path = tensor&lt;string, []&gt;("@model_path/weights/weight.bin"), offset = tensor&lt;uint64, []&gt;(38912)))];
            tensor&lt;fp16, [?, 64]&gt; cast_3 = cast(dtype = context_encoding_to_fp16_dtype_0, x = context_encoding);
            tensor&lt;fp16, [?, 303]&gt; model_3_dense_2_BiasAdd_cast = linear(bias = model_3_dense_2_BiasAdd_bias_0_to_fp16, weight = transpose_0_cast_to_fp32_to_fp16, x = cast_3);
            tensor&lt;int32, []&gt; model_3_lambda_2_argsort_TopKV2_k_0 = const()[name = tensor&lt;string, []&gt;("model_3_lambda_2_argsort_TopKV2_k_0"), val = tensor&lt;int32, []&gt;(303)];
            tensor&lt;int32, []&gt; model_3_lambda_2_argsort_TopKV2_axis_0 = const()[name = tensor&lt;string, []&gt;("model_3_lambda_2_argsort_TopKV2_axis_0"), val = tensor&lt;int32, []&gt;(-1)];
            tensor&lt;bool, []&gt; model_3_lambda_2_argsort_TopKV2_sort_0 = const()[name = tensor&lt;string, []&gt;("model_3_lambda_2_argsort_TopKV2_sort_0"), val = tensor&lt;bool, []&gt;(true)];
            tensor&lt;bool, []&gt; model_3_lambda_2_argsort_TopKV2_ascending_0 = const()[name = tensor&lt;string, []&gt;("model_3_lambda_2_argsort_TopKV2_ascending_0"), val = tensor&lt;bool, []&gt;(false)];
            tensor&lt;bool, []&gt; model_3_lambda_2_argsort_TopKV2_return_indices_0 = const()[name = tensor&lt;string, []&gt;("model_3_lambda_2_argsort_TopKV2_return_indices_0"), val = tensor&lt;bool, []&gt;(true)];
            tensor&lt;fp16, [?, 303]&gt; model_3_lambda_2_argsort_TopKV2_cast_0, tensor&lt;int32, [?, 303]&gt; model_3_lambda_2_argsort_TopKV2_cast_1 = topk(ascending = model_3_lambda_2_argsort_TopKV2_ascending_0, axis = model_3_lambda_2_argsort_TopKV2_axis_0, k = model_3_lambda_2_argsort_TopKV2_k_0, return_indices = model_3_lambda_2_argsort_TopKV2_return_indices_0, sort = model_3_lambda_2_argsort_TopKV2_sort_0, x = model_3_dense_2_BiasAdd_cast);
            tensor&lt;int32, [2]&gt; model_3_tf___operators___getitem_4_strided_slice_begin_0 = const()[name = tensor&lt;string, []&gt;("model_3_tf___operators___getitem_4_strided_slice_begin_0"), val = tensor&lt;int32, [2]&gt;([0, 0])];
            tensor&lt;int32, [2]&gt; model_3_tf___operators___getitem_4_strided_slice_end_0 = const()[name = tensor&lt;string, []&gt;("model_3_tf___operators___getitem_4_strided_slice_end_0"), val = tensor&lt;int32, [2]&gt;([0, 40])];
            tensor&lt;int32, [2]&gt; model_3_tf___operators___getitem_4_strided_slice_stride_0 = const()[name = tensor&lt;string, []&gt;("model_3_tf___operators___getitem_4_strided_slice_stride_0"), val = tensor&lt;int32, [2]&gt;([1, 1])];
            tensor&lt;bool, [2]&gt; model_3_tf___operators___getitem_4_strided_slice_begin_mask_0 = const()[name = tensor&lt;string, []&gt;("model_3_tf___operators___getitem_4_strided_slice_begin_mask_0"), val = tensor&lt;bool, [2]&gt;([true, true])];
            tensor&lt;bool, [2]&gt; model_3_tf___operators___getitem_4_strided_slice_end_mask_0 = const()[name = tensor&lt;string, []&gt;("model_3_tf___operators___getitem_4_strided_slice_end_mask_0"), val = tensor&lt;bool, [2]&gt;([true, false])];
            tensor&lt;bool, [2]&gt; model_3_tf___operators___getitem_4_strided_slice_squeeze_mask_0 = const()[name = tensor&lt;string, []&gt;("model_3_tf___operators___getitem_4_strided_slice_squeeze_mask_0"), val = tensor&lt;bool, [2]&gt;([false, false])];
            tensor&lt;int32, [?, 40]&gt; Identity = slice_by_index(begin = model_3_tf___operators___getitem_4_strided_slice_begin_0, begin_mask = model_3_tf___operators___getitem_4_strided_slice_begin_mask_0, end = model_3_tf___operators___getitem_4_strided_slice_end_0, end_mask = model_3_tf___operators___getitem_4_strided_slice_end_mask_0, squeeze_mask = model_3_tf___operators___getitem_4_strided_slice_squeeze_mask_0, stride = model_3_tf___operators___getitem_4_strided_slice_stride_0, x = model_3_lambda_2_argsort_TopKV2_cast_1);
            tensor&lt;int32, []&gt; model_3_tf_compat_v1_gather_2_GatherV2_axis_0 = const()[name = tensor&lt;string, []&gt;("model_3_tf_compat_v1_gather_2_GatherV2_axis_0"), val = tensor&lt;int32, []&gt;(1)];
            tensor&lt;int32, []&gt; model_3_tf_compat_v1_gather_2_GatherV2_batch_dims_0 = const()[name = tensor&lt;string, []&gt;("model_3_tf_compat_v1_gather_2_GatherV2_batch_dims_0"), val = tensor&lt;int32, []&gt;(0)];
            tensor&lt;fp16, [?, ?, 303]&gt; model_3_tf_compat_v1_gather_2_GatherV2_cast = gather(axis = model_3_tf_compat_v1_gather_2_GatherV2_axis_0, batch_dims = model_3_tf_compat_v1_gather_2_GatherV2_batch_dims_0, indices = model_3_lambda_2_argsort_TopKV2_cast_1, x = model_3_dense_2_BiasAdd_cast);
            tensor&lt;int32, [3]&gt; model_3_tf___operators___getitem_5_strided_slice_begin_0 = const()[name = tensor&lt;string, []&gt;("model_3_tf___operators___getitem_5_strided_slice_begin_0"), val = tensor&lt;int32, [3]&gt;([0, 0, 0])];
            tensor&lt;int32, [3]&gt; model_3_tf___operators___getitem_5_strided_slice_end_0 = const()[name = tensor&lt;string, []&gt;("model_3_tf___operators___getitem_5_strided_slice_end_0"), val = tensor&lt;int32, [3]&gt;([0, 1, 40])];
            tensor&lt;int32, [3]&gt; model_3_tf___operators___getitem_5_strided_slice_stride_0 = const()[name = tensor&lt;string, []&gt;("model_3_tf___operators___getitem_5_strided_slice_stride_0"), val = tensor&lt;int32, [3]&gt;([1, 1, 1])];
            tensor&lt;bool, [3]&gt; model_3_tf___operators___getitem_5_strided_slice_begin_mask_0 = const()[name = tensor&lt;string, []&gt;("model_3_tf___operators___getitem_5_strided_slice_begin_mask_0"), val = tensor&lt;bool, [3]&gt;([true, false, true])];
            tensor&lt;bool, [3]&gt; model_3_tf___operators___getitem_5_strided_slice_end_mask_0 = const()[name = tensor&lt;string, []&gt;("model_3_tf___operators___getitem_5_strided_slice_end_mask_0"), val = tensor&lt;bool, [3]&gt;([true, false, false])];
            tensor&lt;bool, [3]&gt; model_3_tf___operators___getitem_5_strided_slice_squeeze_mask_0 = const()[name = tensor&lt;string, []&gt;("model_3_tf___operators___getitem_5_strided_slice_squeeze_mask_0"), val = tensor&lt;bool, [3]&gt;([false, true, false])];
            tensor&lt;fp16, [?, 40]&gt; model_3_tf___operators___getitem_5_strided_slice_cast = slice_by_index(begin = model_3_tf___operators___getitem_5_strided_slice_begin_0, begin_mask = model_3_tf___operators___getitem_5_strided_slice_begin_mask_0, end = model_3_tf___operators___getitem_5_strided_slice_end_0, end_mask = model_3_tf___operators___getitem_5_strided_slice_end_mask_0, squeeze_mask = model_3_tf___operators___getitem_5_strided_slice_squeeze_mask_0, stride = model_3_tf___operators___getitem_5_strided_slice_stride_0, x = model_3_tf_compat_v1_gather_2_GatherV2_cast);
            tensor&lt;string, []&gt; model_3_tf___operators___getitem_5_strided_slice_cast_to_fp32_dtype_0 = const()[name = tensor&lt;string, []&gt;("model_3_tf___operators___getitem_5_strided_slice_cast_to_fp32_dtype_0"), val = tensor&lt;string, []&gt;("fp32")];
            tensor&lt;fp32, [?, 40]&gt; Identity_1 = cast(dtype = model_3_tf___operators___getitem_5_strided_slice_cast_to_fp32_dtype_0, x = model_3_tf___operators___getitem_5_strided_slice_cast);
        } -&gt; (Identity, Identity_1);
}</code></pre>
                    </div>
                </div>
            
        
        <button class="back-to-top" onclick="window.scrollTo({top: 0, behavior: 'smooth'})">â†‘</button>
    </div>
</body>
</html>
